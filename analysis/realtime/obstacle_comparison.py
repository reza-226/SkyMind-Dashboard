"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ù…ÙˆØ§Ù†Ø¹
Ù…Ø³ÛŒØ±: analysis/realtime/obstacle_comparison.py
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List
import json

class ObstacleComparison:
    """Ú©Ù„Ø§Ø³ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± Ø³Ø·ÙˆØ­ Ù…Ø®ØªÙ„Ù Ù…ÙˆØ§Ù†Ø¹"""
    
    def __init__(self):
        self.results = {
            'simple': {},
            'medium': {},
            'complex': {}
        }
        
        self.metrics = [
            'avg_delay',
            'avg_energy',
            'success_rate',
            'collision_rate',
            'path_length',
            'computation_time'
        ]
        
        self.algorithms = ['MADDPG', 'DQN', 'BLS', 'GA', 'ECORI']
        self.layers = ['Ground', 'Local', 'Edge', 'Cloud']
    
    def add_result(self, 
                   complexity: str, 
                   algorithm: str, 
                   layer: str, 
                   metrics: Dict):
        """
        Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†ØªÛŒØ¬Ù‡ ÛŒÚ© Ø¢Ø²Ù…Ø§ÛŒØ´
        
        Args:
            complexity: 'simple', 'medium', 'complex'
            algorithm: Ù†Ø§Ù… Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…
            layer: Ù†Ø§Ù… Ù„Ø§ÛŒÙ‡
            metrics: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
        """
        key = f"{algorithm}_{layer}"
        self.results[complexity][key] = metrics
    
    def generate_intra_complexity_comparison(self, complexity: str):
        """
        ğŸ“ˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ø§Ø®Ù„ÛŒ: Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ Ø¯Ø± ÛŒÚ© Ø³Ø·Ø­ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ
        """
        data = []
        
        for algo in self.algorithms:
            for layer in self.layers:
                key = f"{algo}_{layer}"
                if key in self.results[complexity]:
                    metrics = self.results[complexity][key]
                    data.append({
                        'Algorithm': algo,
                        'Layer': layer,
                        **metrics
                    })
        
        df = pd.DataFrame(data)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'Intra-Complexity Comparison: {complexity.upper()}',
                    fontsize=16, fontweight='bold')
        
        metrics_to_plot = [
            ('avg_delay', 'Average Delay (ms)'),
            ('avg_energy', 'Average Energy (J)'),
            ('success_rate', 'Success Rate (%)'),
            ('collision_rate', 'Collision Rate (%)'),
            ('path_length', 'Average Path Length (m)'),
            ('computation_time', 'Computation Time (s)')
        ]
        
        for idx, (metric, title) in enumerate(metrics_to_plot):
            ax = axes[idx // 3, idx % 3]
            
            # Bar plot
            pivot = df.pivot(index='Algorithm', columns='Layer', values=metric)
            pivot.plot(kind='bar', ax=ax, width=0.8)
            
            ax.set_title(title, fontsize=12, fontweight='bold')
            ax.set_xlabel('Algorithm', fontsize=10)
            ax.set_ylabel(title, fontsize=10)
            ax.legend(title='Layer', fontsize=9)
            ax.grid(axis='y', alpha=0.3)
            
            # Ú†Ø±Ø®Ø´ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§
            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
        
        plt.tight_layout()
        plt.savefig(f'results/intra_comparison_{complexity}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return df
    
    def generate_inter_layer_comparison(self, complexity: str, algorithm: str):
        """
        ğŸ“Š Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨ÛŒÙ†â€ŒÙ„Ø§ÛŒÙ‡â€ŒØ§ÛŒ: Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…
        """
        data = []
        
        for layer in self.layers:
            key = f"{algorithm}_{layer}"
            if key in self.results[complexity]:
                metrics = self.results[complexity][key]
                data.append({
                    'Layer': layer,
                    **metrics
                })
        
        df = pd.DataFrame(data)
        
        # Ù†Ù…ÙˆØ¯Ø§Ø± Ø±Ø§Ø¯Ø§Ø±
        fig = plt.figure(figsize=(10, 10))
        ax = fig.add_subplot(111, projection='polar')
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
        normalized_metrics = ['avg_delay_norm', 'avg_energy_norm', 
                            'success_rate', 'collision_rate_inv']
        
        angles = np.linspace(0, 2 * np.pi, len(normalized_metrics), endpoint=False).tolist()
        angles += angles[:1]
        
        for _, row in df.iterrows():
            values = [
                1 - row['avg_delay'] / df['avg_delay'].max(),
                1 - row['avg_energy'] / df['avg_energy'].max(),
                row['success_rate'],
                1 - row['collision_rate']
            ]
            values += values[:1]
            
            ax.plot(angles, values, 'o-', linewidth=2, label=row['Layer'])
            ax.fill(angles, values, alpha=0.15)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(['Delayâ†“', 'Energyâ†“', 'Successâ†‘', 'Safetyâ†‘'])
        ax.set_ylim(0, 1)
        ax.set_title(f'Layer Comparison: {algorithm} ({complexity})',
                    size=14, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(f'results/inter_layer_{algorithm}_{complexity}.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        return df
    
    def generate_cross_complexity_comparison(self, algorithm: str, layer: str):
        """
        ğŸ”„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…ØªÙ‚Ø§Ø·Ø¹: ØªØ£Ø«ÛŒØ± Ø§ÙØ²Ø§ÛŒØ´ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ
        """
        data = []
        
        for complexity in ['simple', 'medium', 'complex']:
            key = f"{algorithm}_{layer}"
            if key in self.results[complexity]:
                metrics = self.results[complexity][key]
                data.append({
                    'Complexity': complexity,
                    **metrics
                })
        
        df = pd.DataFrame(data)
        
        # Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ø·ÛŒ
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle(f'Cross-Complexity Analysis: {algorithm} on {layer}',
                    fontsize=14, fontweight='bold')
        
        metrics_plot = [
            ('avg_delay', 'Delay (ms)', axes[0, 0]),
            ('avg_energy', 'Energy (J)', axes[0, 1]),
            ('success_rate', 'Success Rate (%)', axes[1, 0]),
            ('collision_rate', 'Collision Rate (%)', axes[1, 1])
        ]
        
        for metric, ylabel, ax in metrics_plot:
            ax.plot(df['Complexity'], df[metric], 
                   marker='o', linewidth=2, markersize=8)
            ax.set_xlabel('Complexity Level', fontsize=11)
            ax.set_ylabel(ylabel, fontsize=11)
            ax.set_title(ylabel, fontsize=12, fontweight='bold')
            ax.grid(True, alpha=0.3)
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±ÙˆÛŒ Ù†Ù‚Ø§Ø·
            for x, y in zip(df['Complexity'], df[metric]):
                ax.annotate(f'{y:.2f}', xy=(x, y), 
                          textcoords='offset points', xytext=(0, 10),
                          ha='center', fontsize=9)
        
        plt.tight_layout()
        plt.savefig(f'results/cross_complexity_{algorithm}_{layer}.png',
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        return df
    
    def generate_heatmap_comparison(self):
        """
        ğŸŒ¡ï¸ Ù†Ù…ÙˆØ¯Ø§Ø± Ø­Ø±Ø§Ø±ØªÛŒ: Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ã— Ø³Ø·Ø­ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ
        """
        # Ù…ØªØ±ÛŒÚ©: Average Delay
        data = []
        
        for complexity in ['simple', 'medium', 'complex']:
            row = []
            for algo in self.algorithms:
                # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø±ÙˆÛŒ Ù‡Ù…Ù‡ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§
                values = []
                for layer in self.layers:
                    key = f"{algo}_{layer}"
                    if key in self.results[complexity]:
                        values.append(self.results[complexity][key]['avg_delay'])
                
                row.append(np.mean(values) if values else np.nan)
            
            data.append(row)
        
        df = pd.DataFrame(data, 
                         index=['Simple', 'Medium', 'Complex'],
                         columns=self.algorithms)
        
        # Ø±Ø³Ù… heatmap
        plt.figure(figsize=(10, 6))
        sns.heatmap(df, annot=True, fmt='.2f', cmap='YlOrRd', 
                   cbar_kws={'label': 'Average Delay (ms)'})
        plt.title('Algorithm Performance Across Complexity Levels\n(Lower is Better)',
                 fontsize=14, fontweight='bold')
        plt.xlabel('Algorithm', fontsize=12)
        plt.ylabel('Complexity Level', fontsize=12)
        plt.tight_layout()
        plt.savefig('results/heatmap_complexity_algo.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_summary_table(self):
        """
        ğŸ“‹ Ø¬Ø¯ÙˆÙ„ Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
        """
        rows = []
        
        for complexity in ['simple', 'medium', 'complex']:
            for algo in self.algorithms:
                for layer in self.layers:
                    key = f"{algo}_{layer}"
                    if key in self.results[complexity]:
                        metrics = self.results[complexity][key]
                        rows.append({
                            'Complexity': complexity.capitalize(),
                            'Algorithm': algo,
                            'Layer': layer,
                            'Delay (ms)': f"{metrics['avg_delay']:.2f}",
                            'Energy (J)': f"{metrics['avg_energy']:.2f}",
                            'Success (%)': f"{metrics['success_rate']:.1f}",
                            'Collision (%)': f"{metrics['collision_rate']:.1f}"
                        })
        
        df = pd.DataFrame(rows)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ CSV
        df.to_csv('results/obstacle_comparison_summary.csv', index=False)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ LaTeX
        latex_table = df.to_latex(index=False, 
                                  caption='Performance comparison across obstacle complexities',
                                  label='tab:obstacle_comparison')
        
        with open('results/obstacle_comparison_table.tex', 'w') as f:
            f.write(latex_table)
        
        return df
    
    def save_results(self, filename: str = 'obstacle_comparison_results.json'):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ ÙØ§ÛŒÙ„ JSON"""
        with open(f'results/{filename}', 'w') as f:
            json.dump(self.results, f, indent=2)
    
    def load_results(self, filename: str = 'obstacle_comparison_results.json'):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ Ø§Ø² ÙØ§ÛŒÙ„ JSON"""
        with open(f'results/{filename}', 'r') as f:
            self.results = json.load(f)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ§ª Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

if __name__ == "__main__":
    # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡
    comparison = ObstacleComparison()
    
    # ÙØ±Ø¶: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ (Ø¯Ø± ÙˆØ§Ù‚Ø¹ÛŒØª Ø§Ø² Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒØ¢ÛŒØ¯)
    np.random.seed(42)
    
    for complexity in ['simple', 'medium', 'complex']:
        # Ø¶Ø±ÛŒØ¨ Ø³Ø®ØªÛŒ
        difficulty_factor = {'simple': 1.0, 'medium': 1.5, 'complex': 2.0}[complexity]
        
        for algo in ['MADDPG', 'DQN', 'BLS', 'GA']:
            # ÙØ±Ø¶: MADDPG Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
            algo_factor = {'MADDPG': 0.8, 'DQN': 1.0, 'BLS': 1.3, 'GA': 1.5}[algo]
            
            for layer in ['Ground', 'Local', 'Edge', 'Cloud']:
                # ÙØ±Ø¶: Edge Ø¨Ù‡ÛŒÙ†Ù‡ Ø§Ø³Øª
                layer_factor = {'Ground': 1.2, 'Local': 1.1, 'Edge': 0.9, 'Cloud': 1.0}[layer]
                
                metrics = {
                    'avg_delay': np.random.uniform(50, 150) * difficulty_factor * algo_factor * layer_factor,
                    'avg_energy': np.random.uniform(10, 50) * difficulty_factor * algo_factor,
                    'success_rate': max(60, 100 - np.random.uniform(5, 20) * difficulty_factor * algo_factor),
                    'collision_rate': min(30, np.random.uniform(1, 10) * difficulty_factor / algo_factor),
                    'path_length': np.random.uniform(200, 500) * difficulty_factor,
                    'computation_time': np.random.uniform(0.5, 3) * difficulty_factor * algo_factor
                }
                
                comparison.add_result(complexity, algo, layer, metrics)
    
    # ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§
    print("ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ...")
    
    # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ø§Ø®Ù„ÛŒ
    for complexity in ['simple', 'medium', 'complex']:
        df = comparison.generate_intra_complexity_comparison(complexity)
        print(f"âœ… Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ø§Ø®Ù„ÛŒ {complexity} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨ÛŒÙ†â€ŒÙ„Ø§ÛŒÙ‡â€ŒØ§ÛŒ
    for algo in ['MADDPG', 'DQN']:
        for complexity in ['simple', 'complex']:
            df = comparison.generate_inter_layer_comparison(complexity, algo)
            print(f"âœ… Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ {algo} Ø¯Ø± {complexity} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…ØªÙ‚Ø§Ø·Ø¹
    for algo in ['MADDPG', 'BLS']:
        for layer in ['Edge', 'Cloud']:
            df = comparison.generate_cross_complexity_comparison(algo, layer)
            print(f"âœ… ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ‚Ø§Ø·Ø¹ {algo} Ø±ÙˆÛŒ {layer} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    # Heatmap
    comparison.generate_heatmap_comparison()
    print("âœ… Heatmap ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯")
    
    # Ø¬Ø¯ÙˆÙ„ Ø®Ù„Ø§ØµÙ‡
    summary_df = comparison.generate_summary_table()
    print("âœ… Ø¬Ø¯ÙˆÙ„ Ø®Ù„Ø§ØµÙ‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    comparison.save_results()
    print("âœ… Ù†ØªØ§ÛŒØ¬ JSON Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    print("\n" + "â”" * 60)
    print("ğŸ‰ ØªÙ…Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù†Ø¯!")
    print("ğŸ“ Ù…Ø³ÛŒØ±: results/")
    print("â”" * 60)
