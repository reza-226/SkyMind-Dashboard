"""
Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú†Ù†Ø¯ Ù…Ø¯Ù„ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø±
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List
from .model_evaluator import ModelEvaluator


class ModelComparison:
    """Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú†Ù†Ø¯ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡"""
    
    def __init__(self, model_paths: List[str]):
        """
        Args:
            model_paths: Ù„ÛŒØ³Øª Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        """
        self.model_paths = [Path(p) for p in model_paths]
        self.evaluators = []
        self.model_names = []
        
        for path in self.model_paths:
            try:
                evaluator = ModelEvaluator(str(path))
                self.evaluators.append(evaluator)
                # Ù†Ø§Ù… Ù…Ø¯Ù„ Ø§Ø² Ù†Ø§Ù… Ù¾ÙˆØ´Ù‡
                self.model_names.append(path.parent.name)
            except Exception as e:
                print(f"âš ï¸  Could not load model from {path}: {e}")
    
    def compare(self, num_episodes: int = 50) -> Dict:
        """
        Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        
        Args:
            num_episodes: ØªØ¹Ø¯Ø§Ø¯ Ø§Ù¾ÛŒØ²ÙˆØ¯Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„
        
        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø­Ø§ÙˆÛŒ Ù†ØªØ§ÛŒØ¬ Ù…Ù‚Ø§ÛŒØ³Ù‡
        """
        if not self.evaluators:
            return {'error': 'No valid models loaded'}
        
        print(f"\n{'='*70}")
        print(f"ğŸ” Comparing {len(self.evaluators)} Models")
        print(f"{'='*70}\n")
        
        all_results = {}
        
        for i, (evaluator, name) in enumerate(zip(self.evaluators, self.model_names)):
            print(f"ğŸ“Š Evaluating Model {i+1}/{len(self.evaluators)}: {name}")
            
            results = evaluator.evaluate(num_episodes=num_episodes, detailed=False)
            all_results[name] = results
            
            print(f"   Mean Reward: {results['statistics']['mean_reward']:.2f}\n")
        
        # ØªØ­Ù„ÛŒÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ
        comparison = self._analyze_comparison(all_results)
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
        self._print_comparison(comparison)
        
        return {
            'models': all_results,
            'comparison': comparison
        }
    
    def _analyze_comparison(self, results: Dict) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        means = {name: res['statistics']['mean_reward'] 
                for name, res in results.items()}
        stds = {name: res['statistics']['std_reward'] 
               for name, res in results.items()}
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ù‡ØªØ±ÛŒÙ† Ùˆ Ø¨Ø¯ØªØ±ÛŒÙ†
        best_model = max(means.items(), key=lambda x: x[1])
        worst_model = min(means.items(), key=lambda x: x[1])
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        ranked = sorted(means.items(), key=lambda x: x[1], reverse=True)
        
        return {
            'best_model': {
                'name': best_model[0],
                'mean_reward': float(best_model[1]),
                'std_reward': float(stds[best_model[0]])
            },
            'worst_model': {
                'name': worst_model[0],
                'mean_reward': float(worst_model[1]),
                'std_reward': float(stds[worst_model[0]])
            },
            'ranking': [
                {
                    'rank': i + 1,
                    'name': name,
                    'mean_reward': float(reward),
                    'std_reward': float(stds[name])
                }
                for i, (name, reward) in enumerate(ranked)
            ],
            'performance_gap': float(best_model[1] - worst_model[1])
        }
    
    def _print_comparison(self, comparison: Dict):
        """Ú†Ø§Ù¾ Ù†ØªØ§ÛŒØ¬ Ù…Ù‚Ø§ÛŒØ³Ù‡"""
        print(f"\n{'='*70}")
        print(f"ğŸ† Comparison Results")
        print(f"{'='*70}\n")
        
        print(f"ğŸ“Š Ranking:")
        for rank_info in comparison['ranking']:
            print(f"   {rank_info['rank']}. {rank_info['name']}")
            print(f"      Mean: {rank_info['mean_reward']:.2f} Â± {rank_info['std_reward']:.2f}")
        
        print(f"\nâœ¨ Best Model: {comparison['best_model']['name']}")
        print(f"   Reward: {comparison['best_model']['mean_reward']:.2f}")
        
        print(f"\nâš ï¸  Performance Gap: {comparison['performance_gap']:.2f}\n")
