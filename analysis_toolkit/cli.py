"""
Ø±Ø§Ø¨Ø· Ø®Ø· ÙØ±Ù…Ø§Ù† (CLI) Ø¨Ø±Ø§ÛŒ Analysis Toolkit
"""

import argparse
import json
from pathlib import Path
from typing import Optional
import sys

from .analyzers.training_analyzer import TrainingAnalyzer
from .analyzers.model_evaluator import ModelEvaluator
from .analyzers.action_analyzer import ActionAnalyzer
from .analyzers.comparison import ComparisonAnalyzer
from .reporters.html_reporter import HTMLReporter
from .reporters.markdown_reporter import MarkdownReporter


class AnalysisCLI:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ø±Ø§Ø¨Ø· Ø®Ø· ÙØ±Ù…Ø§Ù† Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„"""
    
    def __init__(self):
        self.parser = self._create_parser()
        self.results = {}
    
    def _create_parser(self):
        """Ø§ÛŒØ¬Ø§Ø¯ parser Ø¨Ø±Ø§ÛŒ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø· ÙØ±Ù…Ø§Ù†"""
        parser = argparse.ArgumentParser(
            description='UAV-MEC Training Analysis Toolkit',
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡:
  
  # ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„
  python -m analysis_toolkit --model results/experiment_1/best_model.pth --full-analysis
  
  # ÙÙ‚Ø· Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
  python -m analysis_toolkit --model results/experiment_1/best_model.pth --evaluate --episodes 50
  
  # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ ØªØµØ§Ø¯ÙÛŒ
  python -m analysis_toolkit --model results/experiment_1/best_model.pth --compare-random --episodes 30
  
  # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ HTML
  python -m analysis_toolkit --model results/experiment_1/best_model.pth --full-analysis --html
            """
        )
        
        # Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
        parser.add_argument('--model', type=str, required=True,
                          help='Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ (best_model.pth)')
        
        parser.add_argument('--full-analysis', action='store_true',
                          help='Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ (Ù‡Ù…Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§)')
        
        # Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
        parser.add_argument('--evaluate', action='store_true',
                          help='Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø¯Ø± Ù…Ø­ÛŒØ·')
        
        parser.add_argument('--compare-random', action='store_true',
                          help='Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ ØªØµØ§Ø¯ÙÛŒ')
        
        parser.add_argument('--analyze-training', action='store_true',
                          help='ØªØ­Ù„ÛŒÙ„ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø¢Ù…ÙˆØ²Ø´')
        
        parser.add_argument('--analyze-actions', action='store_true',
                          help='ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ Ø§Ú©Ø´Ù†â€ŒÙ‡Ø§')
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        parser.add_argument('--episodes', type=int, default=50,
                          help='ØªØ¹Ø¯Ø§Ø¯ Ø§Ù¾ÛŒØ²ÙˆØ¯Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 50)')
        
        parser.add_argument('--detailed', action='store_true',
                          help='Ù†Ù…Ø§ÛŒØ´ Ø¬Ø²Ø¦ÛŒØ§Øª Ú©Ø§Ù…Ù„ Ù‡Ø± Ø§Ù¾ÛŒØ²ÙˆØ¯')
        
        # Ø®Ø±ÙˆØ¬ÛŒ
        parser.add_argument('--output-dir', type=str, default='analysis_results',
                          help='Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ù†ØªØ§ÛŒØ¬')
        
        parser.add_argument('--html', action='store_true',
                          help='ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ HTML')
        
        parser.add_argument('--markdown', action='store_true',
                          help='ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Markdown')
        
        return parser
    
    def run(self, args=None):
        """Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§"""
        args = self.parser.parse_args(args)
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„
        model_path = Path(args.model)
        if not model_path.exists():
            print(f"âŒ Error: Model file not found: {model_path}")
            sys.exit(1)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\n{'='*70}")
        print(f"ğŸš€ UAV-MEC Analysis Toolkit")
        print(f"{'='*70}\n")
        print(f"ğŸ“‚ Model: {model_path}")
        print(f"ğŸ“Š Output: {output_dir}\n")
        
        # ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù‡Ù…Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ Ø¯Ø± Ø­Ø§Ù„Øª full-analysis
        if args.full_analysis:
            args.evaluate = True
            args.compare_random = True
            args.analyze_training = True
            args.analyze_actions = True
        
        # 1. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
        if args.evaluate:
            print("ğŸ” Step 1/4: Evaluating model...")
            evaluator = ModelEvaluator(model_path)
            eval_results = evaluator.evaluate(
                num_episodes=args.episodes,
                detailed=args.detailed
            )
            self.results['evaluation'] = eval_results
            self._print_evaluation_summary(eval_results)
        
        # 2. Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ ØªØµØ§Ø¯ÙÛŒ
        if args.compare_random:
            print("\nğŸ“Š Step 2/4: Comparing with random strategy...")
            comparator = ComparisonAnalyzer(model_path)
            comparison_results = comparator.compare(
                num_episodes=args.episodes
            )
            self.results['comparison'] = comparison_results
            self._print_comparison_summary(comparison_results)
        
        # 3. ØªØ­Ù„ÛŒÙ„ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø¢Ù…ÙˆØ²Ø´
        if args.analyze_training:
            print("\nğŸ“ˆ Step 3/4: Analyzing training history...")
            training_analyzer = TrainingAnalyzer(model_path.parent)
            training_results = training_analyzer.analyze()
            self.results['training'] = training_results
            self._print_training_summary(training_results)
        
        # 4. ØªØ­Ù„ÛŒÙ„ Ø§Ú©Ø´Ù†â€ŒÙ‡Ø§
        if args.analyze_actions:
            print("\nğŸ¯ Step 4/4: Analyzing action distributions...")
            action_analyzer = ActionAnalyzer(model_path)
            action_results = action_analyzer.analyze(num_episodes=args.episodes)
            self.results['actions'] = action_results
            self._print_action_summary(action_results)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        results_file = output_dir / 'analysis_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
        print(f"\nğŸ’¾ Results saved to: {results_file}")
        
        # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§
        if args.html:
            print("\nğŸ“ Generating HTML report...")
            html_reporter = HTMLReporter(output_dir)
            html_file = html_reporter.generate(self.results)
            print(f"âœ… HTML report: {html_file}")
        
        if args.markdown:
            print("\nğŸ“ Generating Markdown report...")
            md_reporter = MarkdownReporter(output_dir)
            md_file = md_reporter.generate(self.results)
            print(f"âœ… Markdown report: {md_file}")
        
        print(f"\n{'='*70}")
        print(f"âœ… Analysis completed successfully!")
        print(f"{'='*70}\n")
    
    def _print_evaluation_summary(self, results):
        """Ú†Ø§Ù¾ Ø®Ù„Ø§ØµÙ‡ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ"""
        stats = results['statistics']
        print(f"\n  ğŸ“Š Evaluation Results:")
        print(f"     Episodes: {results['num_episodes']}")
        print(f"     Mean Reward: {stats['mean_reward']:.2f}")
        print(f"     Std Reward: {stats['std_reward']:.2f}")
        print(f"     Min Reward: {stats['min_reward']:.2f}")
        print(f"     Max Reward: {stats['max_reward']:.2f}")
    
    def _print_comparison_summary(self, results):
        """Ú†Ø§Ù¾ Ø®Ù„Ø§ØµÙ‡ Ù…Ù‚Ø§ÛŒØ³Ù‡"""
        print(f"\n  ğŸ“Š Comparison Results:")
        print(f"     Trained Model: {results['trained_model']['mean']:.2f} Â± {results['trained_model']['std']:.2f}")
        print(f"     Random Policy: {results['random_policy']['mean']:.2f} Â± {results['random_policy']['std']:.2f}")
        print(f"     Improvement: {results['improvement']:.2f}%")
    
    def _print_training_summary(self, results):
        """Ú†Ø§Ù¾ Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…ÙˆØ²Ø´"""
        if 'error' in results:
            print(f"\n  âš ï¸  Training analysis not available: {results['error']}")
            return
        
        print(f"\n  ğŸ“Š Training Summary:")
        print(f"     Total Episodes: {results['total_episodes']}")
        print(f"     Best Reward: {results['best_reward']:.2f}")
        print(f"     Final Reward: {results['final_reward']:.2f}")
    
    def _print_action_summary(self, results):
        """Ú†Ø§Ù¾ Ø®Ù„Ø§ØµÙ‡ Ø§Ú©Ø´Ù†â€ŒÙ‡Ø§"""
        print(f"\n  ğŸ“Š Action Distribution:")
        offload_dist = results['offload_distribution']
        for location, count in offload_dist.items():
            print(f"     {location}: {count} times")


def run_analysis():
    """Ù†Ù‚Ø·Ù‡ ÙˆØ±ÙˆØ¯ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„"""
    cli = AnalysisCLI()
    cli.run()


if __name__ == '__main__':
    run_analysis()
