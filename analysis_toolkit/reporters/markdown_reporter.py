"""
ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Markdown
"""

from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime


class MarkdownReporter:
    """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Markdown"""
    
    def __init__(self, output_dir: str = 'reports'):
        """
        Args:
            output_dir: Ù¾ÙˆØ´Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_report(self,
                       training_analysis: Optional[Dict] = None,
                       evaluation_results: Optional[Dict] = None,
                       action_analysis: Optional[Dict] = None,
                       comparison_results: Optional[Dict] = None,
                       plot_paths: Optional[Dict] = None,
                       save_name: str = "analysis_report"):
        """
        ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Markdown Ú©Ø§Ù…Ù„
        
        Args:
            training_analysis: Ù†ØªØ§ÛŒØ¬ ØªØ­Ù„ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´
            evaluation_results: Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
            action_analysis: ØªØ­Ù„ÛŒÙ„ Ø§Ú©Ø´Ù†â€ŒÙ‡Ø§
            comparison_results: Ù†ØªØ§ÛŒØ¬ Ù…Ù‚Ø§ÛŒØ³Ù‡
            plot_paths: Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§
            save_name: Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ú¯Ø²Ø§Ø±Ø´
        """
        md_content = self._generate_markdown(
            training_analysis, evaluation_results, action_analysis,
            comparison_results, plot_paths
        )
        
        report_path = self.output_dir / f"{save_name}.md"
        report_path.write_text(md_content, encoding='utf-8')
        
        print(f"âœ… Markdown report generated: {report_path}")
        return report_path
    
    def _generate_markdown(self, training_analysis, evaluation_results,
                          action_analysis, comparison_results, plot_paths):
        """ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ÛŒ Markdown"""
        
        md = f"""# ğŸš MADDPG UAV Offloading Analysis Report

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

"""
        
        md += self._generate_summary_section(training_analysis, evaluation_results)
        md += self._generate_training_section(training_analysis, plot_paths)
        md += self._generate_evaluation_section(evaluation_results, plot_paths)
        md += self._generate_action_section(action_analysis, plot_paths)
        md += self._generate_comparison_section(comparison_results, plot_paths)
        
        md += "\n---\n\n*Report generated by Analysis Toolkit v1.0*\n"
        
        return md
    
    def _generate_summary_section(self, training_analysis, evaluation_results):
        """Ø¨Ø®Ø´ Ø®Ù„Ø§ØµÙ‡"""
        if not training_analysis and not evaluation_results:
            return ""
        
        md = "## ğŸ“Š Executive Summary\n\n"
        
        if training_analysis:
            trend = training_analysis.get('trend_analysis', {})
            md += "### Training Overview\n\n"
            md += f"- **Total Episodes:** {training_analysis.get('total_episodes', 'N/A')}\n"
            md += f"- **Final Reward:** {trend.get('final_reward', 0):.2f}\n"
            md += f"- **Best Reward:** {trend.get('best_reward', 0):.2f}\n"
            md += f"- **Improvement:** {trend.get('improvement_percent', 0):.1f}%\n\n"
        
        if evaluation_results:
            stats = evaluation_results.get('statistics', {})
            md += "### Model Performance\n\n"
            md += f"- **Mean Reward:** {stats.get('mean_reward', 0):.2f} Â± {stats.get('std_reward', 0):.2f}\n"
            md += f"- **Min Reward:** {stats.get('min_reward', 0):.2f}\n"
            md += f"- **Max Reward:** {stats.get('max_reward', 0):.2f}\n\n"
        
        return md
    
    def _generate_training_section(self, training_analysis, plot_paths):
        """Ø¨Ø®Ø´ Ø¢Ù…ÙˆØ²Ø´"""
        if not training_analysis:
            return ""
        
        md = "## ğŸ“ˆ Training Analysis\n\n"
        
        # Convergence
        conv = training_analysis.get('convergence', {})
        if conv.get('converged'):
            md += f"âœ… **Status:** Converged at episode {conv.get('convergence_episode', 'N/A')}\n\n"
        else:
            md += "â³ **Status:** Training in progress\n\n"
        
        # Phases
        phases = training_analysis.get('phases', {})
        if phases:
            md += "### Training Phases\n\n"
            md += "| Phase | Mean Reward | Std Deviation |\n"
            md += "|-------|-------------|---------------|\n"
            for phase_name, phase_data in phases.items():
                md += f"| {phase_name.replace('_', ' ').title()} | {phase_data.get('mean', 0):.2f} | {phase_data.get('std', 0):.2f} |\n"
            md += "\n"
        
        # Plot
        if plot_paths and 'training_curve' in plot_paths:
            md += f"![Training Curve]({plot_paths['training_curve']})\n\n"
        
        return md
    
    def _generate_evaluation_section(self, evaluation_results, plot_paths):
        """Ø¨Ø®Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ"""
        if not evaluation_results:
            return ""
        
        md = "## ğŸ¯ Model Evaluation\n\n"
        
        stats = evaluation_results.get('statistics', {})
        md += "### Performance Statistics\n\n"
        md += f"- **Mean Reward:** {stats.get('mean_reward', 0):.2f}\n"
        md += f"- **Std Deviation:** {stats.get('std_reward', 0):.2f}\n"
        md += f"- **Min Reward:** {stats.get('min_reward', 0):.2f}\n"
        md += f"- **Max Reward:** {stats.get('max_reward', 0):.2f}\n\n"
        
        if plot_paths and 'reward_distribution' in plot_paths:
            md += f"![Reward Distribution]({plot_paths['reward_distribution']})\n\n"
        
        return md
    
    def _generate_action_section(self, action_analysis, plot_paths):
        """Ø¨Ø®Ø´ Ø§Ú©Ø´Ù†â€ŒÙ‡Ø§"""
        if not action_analysis:
            return ""
        
        md = "## ğŸ® Action Analysis\n\n"
        
        # Offload
        offload = action_analysis.get('offload_distribution', {})
        if offload:
            md += "### Offload Distribution\n\n"
            md += "| Location | Percentage | Count |\n"
            md += "|----------|-----------|-------|\n"
            for loc, pct in offload.get('percentages', {}).items():
                count = offload.get('counts', {}).get(loc, 0)
                md += f"| {loc} | {pct:.1f}% | {count} |\n"
            md += "\n"
        
        # CPU
        cpu = action_analysis.get('cpu_allocation', {})
        if cpu:
            md += "### CPU Allocation\n\n"
            md += f"- **Mean:** {cpu.get('mean', 0):.3f}\n"
            md += f"- **Std:** {cpu.get('std', 0):.3f}\n"
            md += f"- **Range:** [{cpu.get('min', 0):.3f}, {cpu.get('max', 0):.3f}]\n\n"
        
        # Movement
        movement = action_analysis.get('movement_pattern', {})
        if movement:
            md += "### Movement Pattern\n\n"
            md += f"- **Mean Distance:** {movement.get('total_distance', {}).get('mean', 0):.2f}\n"
            md += f"- **Total Distance:** {movement.get('total_distance', {}).get('sum', 0):.2f}\n\n"
        
        # Plots
        if plot_paths:
            if 'offload_distribution' in plot_paths:
                md += f"![Offload Distribution]({plot_paths['offload_distribution']})\n\n"
            if 'resource_allocation' in plot_paths:
                md += f"![Resource Allocation]({plot_paths['resource_allocation']})\n\n"
            if 'movement_pattern' in plot_paths:
                md += f"![Movement Pattern]({plot_paths['movement_pattern']})\n\n"
        
        return md
    
    def _generate_comparison_section(self, comparison_results, plot_paths):
        """Ø¨Ø®Ø´ Ù…Ù‚Ø§ÛŒØ³Ù‡"""
        if not comparison_results:
            return ""
        
        md = "## âš–ï¸ Model Comparison\n\n"
        
        # Ranking
        ranking = comparison_results.get('ranking', [])
        if ranking:
            md += "### Performance Ranking\n\n"
            md += "| Rank | Model | Mean Reward | Std Deviation |\n"
            md += "|------|-------|-------------|---------------|\n"
            for rank, model_info in enumerate(ranking, 1):
                md += f"| {rank} | {model_info['model']} | {model_info['mean_reward']:.2f} | {model_info['std_reward']:.2f} |\n"
            md += "\n"
        
        # Best/Worst
        best = comparison_results.get('best_model', {})
        worst = comparison_results.get('worst_model', {})
        
        if best:
            md += f"**ğŸ† Best Model:** {best.get('model', 'N/A')} (Reward: {best.get('mean_reward', 0):.2f})\n\n"
        if worst:
            md += f"**ğŸ“‰ Worst Model:** {worst.get('model', 'N/A')} (Reward: {worst.get('mean_reward', 0):.2f})\n\n"
        
        # Plot
        if plot_paths and 'comparison' in plot_paths:
            md += f"![Model Comparison]({plot_paths['comparison']})\n\n"
        
        return md
