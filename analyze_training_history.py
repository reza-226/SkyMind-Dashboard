# ÙØ§ÛŒÙ„: analyze_training_history.py

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
import seaborn as sns

class TrainingHistoryAnalyzer:
    """ØªØ­Ù„ÛŒÙ„â€ŒÚ¯Ø± Ø¬Ø§Ù…Ø¹ Training History"""
    
    def __init__(self, history_path="models/maddpg/training_history.json"):
        self.history_path = Path(history_path)
        self.data = None
        self.df = None
        
    def load_data(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Training History"""
        print(f"ğŸ“‚ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ: {self.history_path}")
        
        with open(self.history_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ Episodes: {len(self.data)}")
        
        # Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡
        if self.data:
            first_key = list(self.data.keys())[0]
            print(f"\nğŸ“‹ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡ (Episode {first_key}):")
            print(json.dumps(self.data[first_key], indent=2, ensure_ascii=False))
    
    def create_dataframe(self):
        """ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ DataFrame"""
        records = []
        
        for episode_key, metrics in self.data.items():
            record = {
                'episode': int(episode_key),
                'stage': metrics.get('stage', 'unknown'),
                'avg_reward': metrics.get('avg_reward', 0),
                'agent_0_reward': metrics.get('rewards', {}).get('agent_0', 0),
                'agent_1_reward': metrics.get('rewards', {}).get('agent_1', 0),
                'actor_loss': metrics.get('actor_loss', 0),
                'critic_loss': metrics.get('critic_loss', 0)
            }
            records.append(record)
        
        self.df = pd.DataFrame(records).sort_values('episode')
        print(f"\nâœ… DataFrame Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: {self.df.shape}")
        print(f"\nğŸ“Š Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯:")
        print(self.df.columns.tolist())
        print(f"\nğŸ“ˆ Ø¢Ù…Ø§Ø± Ø§ÙˆÙ„ÛŒÙ‡:")
        print(self.df.describe())
        
        return self.df
    
    def analyze_stages(self):
        """ØªØ­Ù„ÛŒÙ„ Stages Ù…Ø®ØªÙ„Ù"""
        print("\n" + "="*80)
        print("ğŸ¯ ØªØ­Ù„ÛŒÙ„ Curriculum Stages")
        print("="*80)
        
        stage_stats = self.df.groupby('stage').agg({
            'episode': ['count', 'min', 'max'],
            'avg_reward': ['mean', 'std', 'min', 'max'],
            'actor_loss': 'mean',
            'critic_loss': 'mean'
        }).round(4)
        
        print("\nğŸ“Š Ø¢Ù…Ø§Ø± Ù‡Ø± Stage:")
        print(stage_stats)
        
        # ÛŒØ§ÙØªÙ† Ù†Ù‚Ø·Ù‡ ØªØºÛŒÛŒØ± Stage
        stage_changes = self.df[self.df['stage'] != self.df['stage'].shift()].copy()
        if len(stage_changes) > 1:
            print(f"\nğŸ”„ ØªØºÛŒÛŒØ±Ø§Øª Stage:")
            for idx, row in stage_changes.iterrows():
                print(f"  Episode {int(row['episode'])}: {row['stage']}")
    
    def analyze_crisis(self):
        """ØªØ­Ù„ÛŒÙ„ Ø¨Ø­Ø±Ø§Ù† Over-Specialization"""
        print("\n" + "="*80)
        print("ğŸ” ØªØ­Ù„ÛŒÙ„ Ø¨Ø­Ø±Ø§Ù† Over-Specialization")
        print("="*80)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Rolling Statistics
        window = 50
        self.df['reward_ma'] = self.df['avg_reward'].rolling(window).mean()
        self.df['reward_std'] = self.df['avg_reward'].rolling(window).std()
        
        # ÛŒØ§ÙØªÙ† Episodes Ø¨Ø§ Reward Ù¾Ø§ÛŒÛŒÙ†
        if len(self.df) > window:
            last_100 = self.df.tail(100)
            recent_avg = last_100['avg_reward'].mean()
            overall_avg = self.df['avg_reward'].mean()
            
            print(f"\nğŸ“‰ Ù…Ù‚Ø§ÛŒØ³Ù‡ Reward:")
            print(f"  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©Ù„ÛŒ: {overall_avg:.2f}")
            print(f"  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† 100 Ø§Ù¾ÛŒØ²ÙˆØ¯ Ø§Ø®ÛŒØ±: {recent_avg:.2f}")
            print(f"  â€¢ ØªÙØ§ÙˆØª: {recent_avg - overall_avg:.2f} ({(recent_avg/overall_avg - 1)*100:.1f}%)")
            
            if recent_avg < overall_avg * 0.8:
                print("\nğŸš¨ Ù‡Ø´Ø¯Ø§Ø±: Ú©Ø§Ù‡Ø´ Ø´Ø¯ÛŒØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± Episodes Ø§Ø®ÛŒØ±!")
            elif recent_avg > overall_avg * 1.2:
                print("\nâœ… Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø³Øª!")
        
        # ÛŒØ§ÙØªÙ† Episodes Ø¨Ø§ Loss Ø¨Ø§Ù„Ø§
        if self.df['actor_loss'].max() > 0:
            high_loss = self.df[self.df['actor_loss'] > self.df['actor_loss'].quantile(0.95)]
            if not high_loss.empty:
                print(f"\nâš ï¸ {len(high_loss)} Episode Ø¨Ø§ Actor Loss Ø¨Ø§Ù„Ø§ (>95th percentile):")
                print(high_loss[['episode', 'stage', 'avg_reward', 'actor_loss']].head(10))
    
    def plot_comprehensive_analysis(self):
        """Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¬Ø§Ù…Ø¹"""
        
        fig = plt.figure(figsize=(20, 12))
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙÙˆÙ†Øª ÙØ§Ø±Ø³ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        plt.rcParams['font.family'] = 'DejaVu Sans'
        
        # 1. Rewards Over Time
        ax1 = fig.add_subplot(gs[0, :2])
        ax1.plot(self.df['episode'], self.df['avg_reward'], 
                'o', alpha=0.3, markersize=2, label='Episode Reward', color='blue')
        ax1.plot(self.df['episode'], self.df['reward_ma'], 
                'r-', linewidth=2, label='Moving Avg (50)')
        ax1.fill_between(self.df['episode'], 
                         self.df['reward_ma'] - self.df['reward_std'],
                         self.df['reward_ma'] + self.df['reward_std'],
                         alpha=0.2, color='red', label='Std Dev')
        
        # Ù…Ø´Ø®Øµ Ú©Ø±Ø¯Ù† Stage Changes
        stage_changes = self.df[self.df['stage'] != self.df['stage'].shift()]
        for _, row in stage_changes.iterrows():
            ax1.axvline(x=row['episode'], color='green', 
                       linestyle='--', alpha=0.5, linewidth=1.5)
            ax1.text(row['episode'], ax1.get_ylim()[1], f"{row['stage']}", 
                    rotation=90, va='top', fontsize=9, color='green')
        
        ax1.set_xlabel('Episode', fontsize=12)
        ax1.set_ylabel('Average Reward', fontsize=12)
        ax1.set_title('Training Rewards Over Time (with Stage Transitions)', 
                     fontsize=14, fontweight='bold')
        ax1.legend(loc='lower right')
        ax1.grid(True, alpha=0.3)
        
        # 2. Reward by Stage (Box Plot)
        ax2 = fig.add_subplot(gs[0, 2])
        stages = sorted(self.df['stage'].unique())
        stage_data = [self.df[self.df['stage'] == stage]['avg_reward'] for stage in stages]
        bp = ax2.boxplot(stage_data, labels=stages, patch_artist=True)
        for patch in bp['boxes']:
            patch.set_facecolor('lightblue')
        ax2.set_xlabel('Stage', fontsize=12)
        ax2.set_ylabel('Reward', fontsize=12)
        ax2.set_title('Reward Distribution by Stage', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3, axis='y')
        
        # 3. Actor Loss
        ax3 = fig.add_subplot(gs[1, :2])
        valid_loss = self.df[self.df['actor_loss'] > 0]
        if not valid_loss.empty:
            ax3.plot(valid_loss['episode'], valid_loss['actor_loss'], 
                    'o-', alpha=0.6, markersize=2, color='orange')
            ax3.set_yscale('log')
        ax3.set_xlabel('Episode', fontsize=12)
        ax3.set_ylabel('Actor Loss (log scale)', fontsize=12)
        ax3.set_title('Actor Loss Over Time', fontsize=14, fontweight='bold')
        ax3.grid(True, alpha=0.3)
        
        # 4. Critic Loss
        ax4 = fig.add_subplot(gs[1, 2])
        valid_critic = self.df[self.df['critic_loss'] > 0]
        if not valid_critic.empty:
            ax4.plot(valid_critic['episode'], valid_critic['critic_loss'], 
                    'o-', alpha=0.6, markersize=2, color='purple')
            ax4.set_yscale('log')
        ax4.set_xlabel('Episode', fontsize=12)
        ax4.set_ylabel('Critic Loss (log scale)', fontsize=12)
        ax4.set_title('Critic Loss Over Time', fontsize=14, fontweight='bold')
        ax4.grid(True, alpha=0.3)
        
        # 5. Agent Rewards Comparison
        ax5 = fig.add_subplot(gs[2, 0])
        ax5.plot(self.df['episode'], self.df['agent_0_reward'], 
                alpha=0.5, label='Agent 0', color='blue')
        ax5.plot(self.df['episode'], self.df['agent_1_reward'], 
                alpha=0.5, label='Agent 1', color='green')
        ax5.plot(self.df['episode'], 
                self.df['agent_0_reward'].rolling(50).mean(), 
                linewidth=2, color='blue')
        ax5.plot(self.df['episode'], 
                self.df['agent_1_reward'].rolling(50).mean(), 
                linewidth=2, color='green')
        ax5.set_xlabel('Episode', fontsize=12)
        ax5.set_ylabel('Reward', fontsize=12)
        ax5.set_title('Individual Agent Rewards', fontsize=14, fontweight='bold')
        ax5.legend()
        ax5.grid(True, alpha=0.3)
        
        # 6. Reward Improvement Rate
        ax6 = fig.add_subplot(gs[2, 1])
        window = 100
        if len(self.df) > window:
            improvement = self.df['avg_reward'].rolling(window).apply(
                lambda x: (x.iloc[-1] - x.iloc[0]) / window if len(x) == window else 0
            )
            ax6.plot(self.df['episode'], improvement, 'g-', linewidth=2)
            ax6.axhline(y=0, color='red', linestyle='--', alpha=0.5)
        ax6.set_xlabel('Episode', fontsize=12)
        ax6.set_ylabel('Improvement Rate', fontsize=12)
        ax6.set_title(f'Reward Improvement Rate ({window}-ep window)', 
                     fontsize=14, fontweight='bold')
        ax6.grid(True, alpha=0.3)
        
        # 7. Recent Performance (Last 200 episodes)
        ax7 = fig.add_subplot(gs[2, 2])
        last_n = min(200, len(self.df))
        recent = self.df.tail(last_n)
        ax7.hist(recent['avg_reward'], bins=30, 
                color='teal', alpha=0.7, edgecolor='black')
        ax7.axvline(x=recent['avg_reward'].mean(), 
                   color='red', linestyle='--', linewidth=2, 
                   label=f"Mean: {recent['avg_reward'].mean():.2f}")
        ax7.set_xlabel('Reward', fontsize=12)
        ax7.set_ylabel('Frequency', fontsize=12)
        ax7.set_title(f'Recent Performance (Last {last_n} Episodes)', 
                     fontsize=14, fontweight='bold')
        ax7.legend()
        ax7.grid(True, alpha=0.3, axis='y')
        
        plt.suptitle('MADDPG Training History - Comprehensive Analysis', 
                    fontsize=18, fontweight='bold', y=0.995)
        
        output_path = 'training_history_analysis.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"\nâœ… Ù†Ù…ÙˆØ¯Ø§Ø± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_path}")
        plt.close()
    
    def generate_report(self):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ"""
        
        print("\n" + "="*80)
        print("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Training History")
        print("="*80)
        
        print(f"\nğŸ“ˆ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ:")
        print(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ Episodes: {len(self.df)}")
        print(f"  â€¢ Episode Ø§ÙˆÙ„: {self.df['episode'].min()}")
        print(f"  â€¢ Episode Ø¢Ø®Ø±: {self.df['episode'].max()}")
        
        print(f"\nğŸ¯ Rewards:")
        print(f"  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†: {self.df['avg_reward'].mean():.2f}")
        print(f"  â€¢ Ø¨Ù‡ØªØ±ÛŒÙ†: {self.df['avg_reward'].max():.2f}")
        print(f"  â€¢ Ø¨Ø¯ØªØ±ÛŒÙ†: {self.df['avg_reward'].min():.2f}")
        print(f"  â€¢ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±: {self.df['avg_reward'].std():.2f}")
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ù‡ØªØ±ÛŒÙ† Episode
        best_ep = self.df.loc[self.df['avg_reward'].idxmax()]
        print(f"\nğŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Episode: {int(best_ep['episode'])} (Stage: {best_ep['stage']})")
        print(f"  â€¢ Reward: {best_ep['avg_reward']:.2f}")
        print(f"  â€¢ Agent 0: {best_ep['agent_0_reward']:.2f}")
        print(f"  â€¢ Agent 1: {best_ep['agent_1_reward']:.2f}")
        
        # Ø¢Ù…Ø§Ø± Losses
        if self.df['actor_loss'].max() > 0:
            print(f"\nğŸ“‰ Actor Loss:")
            valid_actor = self.df[self.df['actor_loss'] > 0]
            print(f"  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†: {valid_actor['actor_loss'].mean():.6f}")
            print(f"  â€¢ Ø¨ÛŒØ´ØªØ±ÛŒÙ†: {valid_actor['actor_loss'].max():.6f}")
            print(f"  â€¢ Ú©Ù…ØªØ±ÛŒÙ†: {valid_actor['actor_loss'].min():.6f}")
        
        if self.df['critic_loss'].max() > 0:
            print(f"\nğŸ“‰ Critic Loss:")
            valid_critic = self.df[self.df['critic_loss'] > 0]
            print(f"  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†: {valid_critic['critic_loss'].mean():.6f}")
            print(f"  â€¢ Ø¨ÛŒØ´ØªØ±ÛŒÙ†: {valid_critic['critic_loss'].max():.6f}")
            print(f"  â€¢ Ú©Ù…ØªØ±ÛŒÙ†: {valid_critic['critic_loss'].min():.6f}")
        
        # ØªØ­Ù„ÛŒÙ„ 100 Ø§Ù¾ÛŒØ²ÙˆØ¯ Ø§Ø®ÛŒØ±
        last_100 = self.df.tail(100)
        print(f"\nğŸ”¥ Ø¹Ù…Ù„Ú©Ø±Ø¯ 100 Ø§Ù¾ÛŒØ²ÙˆØ¯ Ø§Ø®ÛŒØ±:")
        print(f"  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Reward: {last_100['avg_reward'].mean():.2f}")
        print(f"  â€¢ Ø¨Ù‡ØªØ±ÛŒÙ†: {last_100['avg_reward'].max():.2f}")
        print(f"  â€¢ Ø¨Ø¯ØªØ±ÛŒÙ†: {last_100['avg_reward'].min():.2f}")
        
        # Ø°Ø®ÛŒØ±Ù‡ CSV
        csv_path = 'training_history_analysis.csv'
        self.df.to_csv(csv_path, index=False)
        print(f"\nâœ… CSV Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {csv_path}")
        
        # Summary JSON
        summary = {
            'total_episodes': int(len(self.df)),
            'stages': self.df['stage'].unique().tolist(),
            'best_episode': int(best_ep['episode']),
            'best_reward': float(best_ep['avg_reward']),
            'overall_avg_reward': float(self.df['avg_reward'].mean()),
            'recent_100_avg_reward': float(last_100['avg_reward'].mean())
        }
        
        with open('training_summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"âœ… Summary JSON Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: training_summary.json")

def main():
    print("ğŸš€ Training History Analyzer v2.0")
    print("="*80)
    
    analyzer = TrainingHistoryAnalyzer()
    
    try:
        analyzer.load_data()
        analyzer.create_dataframe()
        analyzer.analyze_stages()
        analyzer.analyze_crisis()
        analyzer.plot_comprehensive_analysis()
        analyzer.generate_report()
        
        print("\n" + "="*80)
        print("ğŸ‰ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
        print("="*80)
        print("\nğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ:")
        print("  â€¢ training_history_analysis.png")
        print("  â€¢ training_history_analysis.csv")
        print("  â€¢ training_summary.json")
        
    except FileNotFoundError:
        print(f"âŒ ÙØ§ÛŒÙ„ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {analyzer.history_path}")
        print("ğŸ’¡ Ù…Ø³ÛŒØ± ÙØ¹Ù„ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ù…Ø³ÛŒØ± Ú©Ø§Ù…Ù„ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
