"""
Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ablation - Variants Ù…Ø®ØªÙ„Ù MADDPG
Ù…Ø³ÛŒØ±: core/evaluation/ablation_variants.py
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from collections import deque
import random


# ============================================================================
# Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¨Ø±Ø§ÛŒ Variants Ù…Ø®ØªÙ„Ù
# ============================================================================

class SimpleMLPActor(nn.Module):
    """Actor Ø³Ø§Ø¯Ù‡ Ø¨Ø¯ÙˆÙ† GAT"""
    
    def __init__(self, state_dim, action_dim, hidden=512):
        super().__init__()
        
        self.fc1 = nn.Linear(state_dim, hidden)
        self.fc2 = nn.Linear(hidden, hidden)
        self.fc3 = nn.Linear(hidden, action_dim)
        
        self.activation = nn.ReLU()
        
    def forward(self, state):
        x = self.activation(self.fc1(state))
        x = self.activation(self.fc2(x))
        
        # âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² sigmoid Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ [0,1]
        action = torch.sigmoid(self.fc3(x))
        
        # âœ… CRITICAL: Ø§Ø¹Ù…Ø§Ù„ epsilon Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² 0.0 Ùˆ 1.0 Ø¯Ù‚ÛŒÙ‚
        epsilon = 1e-6
        action = action * (1 - 2*epsilon) + epsilon
        
        return action


class SimpleCritic(nn.Module):
    """Critic Ø³Ø§Ø¯Ù‡"""
    
    def __init__(self, state_dim, action_dim, hidden=512):
        super().__init__()
        
        self.fc1 = nn.Linear(state_dim + action_dim, hidden)
        self.fc2 = nn.Linear(hidden, hidden)
        self.q_out = nn.Linear(hidden, 1)
        
        self.activation = nn.ReLU()
        
    def forward(self, state, action):
        x = torch.cat([state, action], dim=-1)
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        q = self.q_out(x)
        return q


class SmallActor(nn.Module):
    """Actor Ú©ÙˆÚ†Ú©â€ŒØªØ± Ø¨Ø±Ø§ÛŒ SimplerArchVariant"""
    
    def __init__(self, state_dim, action_dim, hidden=256):
        super().__init__()
        
        self.fc1 = nn.Linear(state_dim, hidden)
        self.fc2 = nn.Linear(hidden, action_dim)
        
        self.activation = nn.ReLU()
        
    def forward(self, state):
        x = self.activation(self.fc1(state))
        
        # âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² sigmoid Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ [0,1]
        action = torch.sigmoid(self.fc2(x))
        
        # âœ… CRITICAL: Ø§Ø¹Ù…Ø§Ù„ epsilon
        epsilon = 1e-6
        action = action * (1 - 2*epsilon) + epsilon
        
        return action


class SmallCritic(nn.Module):
    """Critic Ú©ÙˆÚ†Ú©â€ŒØªØ±"""
    
    def __init__(self, state_dim, action_dim, hidden=256):
        super().__init__()
        
        self.fc1 = nn.Linear(state_dim + action_dim, hidden)
        self.q_out = nn.Linear(hidden, 1)
        
        self.activation = nn.ReLU()
        
    def forward(self, state, action):
        x = torch.cat([state, action], dim=-1)
        x = self.activation(self.fc1(x))
        q = self.q_out(x)
        return q


class LocalCritic(nn.Module):
    """Critic Ù…Ø­Ù„ÛŒ Ø¨Ø±Ø§ÛŒ DecentralizedVariant"""
    
    def __init__(self, state_dim, action_dim, hidden=512):
        super().__init__()
        
        self.fc1 = nn.Linear(state_dim + action_dim, hidden)
        self.fc2 = nn.Linear(hidden, hidden)
        self.q_out = nn.Linear(hidden, 1)
        
        self.activation = nn.ReLU()
        
    def forward(self, state, action):
        x = torch.cat([state, action], dim=-1)
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        q = self.q_out(x)
        return q


# ============================================================================
# Replay Buffer
# ============================================================================

class ReplayBuffer:
    """Replay Buffer Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ØªØ¬Ø±Ø¨ÛŒØ§Øª"""
    
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        return (
            np.array(states),
            np.array(actions),
            np.array(rewards),
            np.array(next_states),
            np.array(dones)
        )
    
    def __len__(self):
        return len(self.buffer)


# ============================================================================
# VARIANT 1: Full MADDPG (Baseline)
# ============================================================================

class FullMADDPGVariant:
    """Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ MADDPG Ø¨Ø§ ØªÙ…Ø§Ù… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§"""
    
    def __init__(self, obs_dim, action_dim, num_agents, **config):
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.num_agents = num_agents
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
        self.gamma = config.get("gamma", 0.95)
        self.tau = config.get("tau", 0.001)
        self.batch_size = config.get("batch_size", 64)
        self.actor_lr = config.get("actor_lr", 1e-4)
        self.critic_lr = config.get("critic_lr", 1e-3)
        
        # Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§
        self.actors = []
        self.actor_targets = []
        self.critics = []
        self.critic_targets = []
        self.actor_optimizers = []
        self.critic_optimizers = []
        
        for _ in range(num_agents):
            actor = SimpleMLPActor(obs_dim, action_dim, hidden=512).to(self.device)
            actor_target = SimpleMLPActor(obs_dim, action_dim, hidden=512).to(self.device)
            actor_target.load_state_dict(actor.state_dict())
            
            total_state_dim = obs_dim * num_agents
            total_action_dim = action_dim * num_agents
            critic = SimpleCritic(total_state_dim, total_action_dim, hidden=512).to(self.device)
            critic_target = SimpleCritic(total_state_dim, total_action_dim, hidden=512).to(self.device)
            critic_target.load_state_dict(critic.state_dict())
            
            actor_optimizer = torch.optim.Adam(actor.parameters(), lr=self.actor_lr)
            critic_optimizer = torch.optim.Adam(critic.parameters(), lr=self.critic_lr)
            
            self.actors.append(actor)
            self.actor_targets.append(actor_target)
            self.critics.append(critic)
            self.critic_targets.append(critic_target)
            self.actor_optimizers.append(actor_optimizer)
            self.critic_optimizers.append(critic_optimizer)
        
        buffer_size = config.get("buffer_size", 100000)
        self.replay_buffers = [ReplayBuffer(buffer_size) for _ in range(num_agents)]
        
        # âœ… Exploration noise Ø¨Ø§ Ú©Ø§Ù‡Ø´ ØªØ¯Ø±ÛŒØ¬ÛŒ
        self.noise_scale = 0.1  # Ø´Ø±ÙˆØ¹ Ø¨Ø§ 0.1
        self.noise_decay = 0.9995  # Ú©Ø§Ù‡Ø´ ØªØ¯Ø±ÛŒØ¬ÛŒ
        self.min_noise = 0.01  # Ø­Ø¯Ø§Ù‚Ù„ Ù†ÙˆÛŒØ²
        
    def select_action(self, agent_id, obs, add_noise=True):
        """Ø§Ù†ØªØ®Ø§Ø¨ action Ø¨Ø§ Clipping Ø¯Ù‚ÛŒÙ‚"""
        if isinstance(agent_id, str):
            agent_id = int(agent_id.split('_')[-1])
        
        obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action = self.actors[agent_id](obs_tensor)
        
        action = action.cpu().numpy()[0]
        
        if add_noise:
            # âœ… Ù†ÙˆÛŒØ² Ø¨Ø§ Ú©Ø§Ù‡Ø´ ØªØ¯Ø±ÛŒØ¬ÛŒ
            noise = np.random.normal(0, self.noise_scale, size=action.shape)
            action = action + noise
            
            # Ú©Ø§Ù‡Ø´ Ù†ÙˆÛŒØ²
            self.noise_scale = max(self.min_noise, self.noise_scale * self.noise_decay)
        
        # âœ… CRITICAL: Clipping Ø¯Ù‚ÛŒÙ‚ Ø¨Ø§ ØªÙ„Ø±Ø§Ù†Ø³
        action = np.clip(action, 0.0, 1.0)
        
        # âœ… Round Ø¨Ù‡ 6 Ø±Ù‚Ù… Ø§Ø¹Ø´Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¯Ù‚Øª
        action = np.round(action, decimals=6)
        
        # âœ… Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø¨Ø§Ø²Ù‡ [epsilon, 1-epsilon]
        epsilon = 1e-7
        action = np.where(action < epsilon, epsilon, action)
        action = np.where(action > (1.0 - epsilon), 1.0 - epsilon, action)
        
        return action.astype(np.float32)
    
    def store_transition(self, agent_id, state, action, reward, next_state, done):
        """Ø°Ø®ÛŒØ±Ù‡ ØªØ¬Ø±Ø¨Ù‡"""
        if isinstance(agent_id, str):
            agent_id = int(agent_id.split('_')[-1])
        
        self.replay_buffers[agent_id].push(state, action, reward, next_state, done)
    
    def update(self):
        """Ø¢Ù¾Ø¯ÛŒØª Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§"""
        for agent_id in range(self.num_agents):
            if len(self.replay_buffers[agent_id]) < self.batch_size:
                continue
            
            states, actions, rewards, next_states, dones = \
                self.replay_buffers[agent_id].sample(self.batch_size)
            
            states = torch.FloatTensor(states).to(self.device)
            actions = torch.FloatTensor(actions).to(self.device)
            rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)
            next_states = torch.FloatTensor(next_states).to(self.device)
            dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)
            
            all_states = states.repeat(1, self.num_agents)
            all_actions = actions.repeat(1, self.num_agents)
            all_next_states = next_states.repeat(1, self.num_agents)
            
            # ========== Update Critic ==========
            with torch.no_grad():
                next_actions = self.actor_targets[agent_id](next_states)
                all_next_actions = next_actions.repeat(1, self.num_agents)
                
                target_q = self.critic_targets[agent_id](all_next_states, all_next_actions)
                target_q = rewards + (1 - dones) * self.gamma * target_q
            
            current_q = self.critics[agent_id](all_states, all_actions)
            critic_loss = F.mse_loss(current_q, target_q)
            
            self.critic_optimizers[agent_id].zero_grad()
            critic_loss.backward()
            
            # âœ… CRITICAL: Gradient clipping
            torch.nn.utils.clip_grad_norm_(
                self.critics[agent_id].parameters(), 
                max_norm=0.5
            )
            
            self.critic_optimizers[agent_id].step()
            
            # ========== Update Actor ==========
            predicted_actions = self.actors[agent_id](states)
            all_predicted_actions = predicted_actions.repeat(1, self.num_agents)
            
            actor_loss = -self.critics[agent_id](all_states, all_predicted_actions).mean()
            
            self.actor_optimizers[agent_id].zero_grad()
            actor_loss.backward()
            
            # âœ… CRITICAL: Gradient clipping
            torch.nn.utils.clip_grad_norm_(
                self.actors[agent_id].parameters(), 
                max_norm=0.5
            )
            
            self.actor_optimizers[agent_id].step()
            
            # ========== Soft Update Target Networks ==========
            self._soft_update(self.actors[agent_id], self.actor_targets[agent_id])
            self._soft_update(self.critics[agent_id], self.critic_targets[agent_id])
    
    def _soft_update(self, source, target):
        """Soft update target network"""
        for target_param, source_param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(
                self.tau * source_param.data + (1.0 - self.tau) * target_param.data
            )
    
    def save(self, path):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„"""
        checkpoint = {
            'actors': [actor.state_dict() for actor in self.actors],
            'critics': [critic.state_dict() for critic in self.critics],
            'noise_scale': self.noise_scale
        }
        torch.save(checkpoint, path)
    
    def load(self, path):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„"""
        checkpoint = torch.load(path, map_location=self.device)
        for i in range(self.num_agents):
            self.actors[i].load_state_dict(checkpoint['actors'][i])
            self.critics[i].load_state_dict(checkpoint['critics'][i])
        
        if 'noise_scale' in checkpoint:
            self.noise_scale = checkpoint['noise_scale']


# ============================================================================
# VARIANT 2: No GAT
# ============================================================================

class NoGATVariant(FullMADDPGVariant):
    """MADDPG Ø¨Ø¯ÙˆÙ† Graph Attention Network"""
    
    def __init__(self, obs_dim, action_dim, num_agents, **config):
        super().__init__(obs_dim, action_dim, num_agents, **config)
        print("ğŸ“Œ NoGATVariant: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² MLP Ø³Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¬Ø§ÛŒ GAT")


# ============================================================================
# VARIANT 3: No Temporal Features
# ============================================================================

class NoTemporalVariant(FullMADDPGVariant):
    """MADDPG Ø¨Ø¯ÙˆÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ"""
    
    def __init__(self, obs_dim, action_dim, num_agents, **config):
        # Ú©Ø§Ù‡Ø´ 30% Ø§Ø² Ø§Ø¨Ø¹Ø§Ø¯ obs
        reduced_obs_dim = int(obs_dim * 0.7)
        super().__init__(reduced_obs_dim, action_dim, num_agents, **config)
        
        self.original_obs_dim = obs_dim
        self.reduced_obs_dim = reduced_obs_dim
        print(f"ğŸ“Œ NoTemporalVariant: Ú©Ø§Ù‡Ø´ state Ø§Ø² {obs_dim} Ø¨Ù‡ {reduced_obs_dim}")
    
    def select_action(self, agent_id, obs, add_noise=True):
        """Ø§Ù†ØªØ®Ø§Ø¨ action Ø¨Ø§ obs Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØªÙ‡"""
        reduced_obs = obs[:self.reduced_obs_dim]
        return super().select_action(agent_id, reduced_obs, add_noise)
    
    def store_transition(self, agent_id, state, action, reward, next_state, done):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø§ state Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØªÙ‡"""
        reduced_state = state[:self.reduced_obs_dim]
        reduced_next_state = next_state[:self.reduced_obs_dim]
        super().store_transition(agent_id, reduced_state, action, reward, 
                                reduced_next_state, done)
# ============================================================================
# VARIANT 4: Decentralized Learning
# ============================================================================

class DecentralizedVariant:
    """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØºÛŒØ±Ù…ØªÙ…Ø±Ú©Ø² - Ù‡Ø± agent Ù…Ø³ØªÙ‚Ù„ Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ø¯"""
    
    def __init__(self, obs_dim, action_dim, num_agents, **config):
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.num_agents = num_agents
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
        self.gamma = config.get("gamma", 0.95)
        self.tau = config.get("tau", 0.001)
        self.batch_size = config.get("batch_size", 64)
        self.actor_lr = config.get("actor_lr", 1e-4)
        self.critic_lr = config.get("critic_lr", 1e-3)
        
        # Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ù„ÛŒ
        self.actors = []
        self.actor_targets = []
        self.critics = []
        self.critic_targets = []
        self.actor_optimizers = []
        self.critic_optimizers = []
        
        for _ in range(num_agents):
            actor = SimpleMLPActor(obs_dim, action_dim, hidden=512).to(self.device)
            actor_target = SimpleMLPActor(obs_dim, action_dim, hidden=512).to(self.device)
            actor_target.load_state_dict(actor.state_dict())
            
            # âœ… Critic Ù…Ø­Ù„ÛŒ - ÙÙ‚Ø· state Ùˆ action Ø®ÙˆØ¯Ø´ Ø±Ø§ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ø¯
            critic = LocalCritic(obs_dim, action_dim, hidden=512).to(self.device)
            critic_target = LocalCritic(obs_dim, action_dim, hidden=512).to(self.device)
            critic_target.load_state_dict(critic.state_dict())
            
            actor_optimizer = torch.optim.Adam(actor.parameters(), lr=self.actor_lr)
            critic_optimizer = torch.optim.Adam(critic.parameters(), lr=self.critic_lr)
            
            self.actors.append(actor)
            self.actor_targets.append(actor_target)
            self.critics.append(critic)
            self.critic_targets.append(critic_target)
            self.actor_optimizers.append(actor_optimizer)
            self.critic_optimizers.append(critic_optimizer)
        
        buffer_size = config.get("buffer_size", 100000)
        self.replay_buffers = [ReplayBuffer(buffer_size) for _ in range(num_agents)]
        
        # âœ… Exploration noise Ø¨Ø§ Ú©Ø§Ù‡Ø´ ØªØ¯Ø±ÛŒØ¬ÛŒ
        self.noise_scale = 0.1
        self.noise_decay = 0.9995
        self.min_noise = 0.01
        
        print("ğŸ“Œ DecentralizedVariant: Critic Ù…Ø­Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± agent")
    
    def select_action(self, agent_id, obs, add_noise=True):
        """Ø§Ù†ØªØ®Ø§Ø¨ action - Ù…Ø´Ø§Ø¨Ù‡ FullMADDPG"""
        if isinstance(agent_id, str):
            agent_id = int(agent_id.split('_')[-1])
        
        obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action = self.actors[agent_id](obs_tensor)
        
        action = action.cpu().numpy()[0]
        
        if add_noise:
            noise = np.random.normal(0, self.noise_scale, size=action.shape)
            action = action + noise
            self.noise_scale = max(self.min_noise, self.noise_scale * self.noise_decay)
        
        # âœ… Clipping Ø¯Ù‚ÛŒÙ‚
        action = np.clip(action, 0.0, 1.0)
        action = np.round(action, decimals=6)
        
        epsilon = 1e-7
        action = np.where(action < epsilon, epsilon, action)
        action = np.where(action > (1.0 - epsilon), 1.0 - epsilon, action)
        
        return action.astype(np.float32)
    
    def store_transition(self, agent_id, state, action, reward, next_state, done):
        """Ø°Ø®ÛŒØ±Ù‡ ØªØ¬Ø±Ø¨Ù‡"""
        if isinstance(agent_id, str):
            agent_id = int(agent_id.split('_')[-1])
        
        self.replay_buffers[agent_id].push(state, action, reward, next_state, done)
    
    def update(self):
        """Ø¢Ù¾Ø¯ÛŒØª ØºÛŒØ±Ù…ØªÙ…Ø±Ú©Ø²"""
        for agent_id in range(self.num_agents):
            if len(self.replay_buffers[agent_id]) < self.batch_size:
                continue
            
            states, actions, rewards, next_states, dones = \
                self.replay_buffers[agent_id].sample(self.batch_size)
            
            states = torch.FloatTensor(states).to(self.device)
            actions = torch.FloatTensor(actions).to(self.device)
            rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)
            next_states = torch.FloatTensor(next_states).to(self.device)
            dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)
            
            # ========== Update Critic (Ù…Ø­Ù„ÛŒ) ==========
            with torch.no_grad():
                next_actions = self.actor_targets[agent_id](next_states)
                target_q = self.critic_targets[agent_id](next_states, next_actions)
                target_q = rewards + (1 - dones) * self.gamma * target_q
            
            current_q = self.critics[agent_id](states, actions)
            critic_loss = F.mse_loss(current_q, target_q)
            
            self.critic_optimizers[agent_id].zero_grad()
            critic_loss.backward()
            
            # âœ… Gradient clipping
            torch.nn.utils.clip_grad_norm_(
                self.critics[agent_id].parameters(), 
                max_norm=0.5
            )
            
            self.critic_optimizers[agent_id].step()
            
            # ========== Update Actor ==========
            predicted_actions = self.actors[agent_id](states)
            actor_loss = -self.critics[agent_id](states, predicted_actions).mean()
            
            self.actor_optimizers[agent_id].zero_grad()
            actor_loss.backward()
            
            # âœ… Gradient clipping
            torch.nn.utils.clip_grad_norm_(
                self.actors[agent_id].parameters(), 
                max_norm=0.5
            )
            
            self.actor_optimizers[agent_id].step()
            
            # ========== Soft Update ==========
            self._soft_update(self.actors[agent_id], self.actor_targets[agent_id])
            self._soft_update(self.critics[agent_id], self.critic_targets[agent_id])
    
    def _soft_update(self, source, target):
        """Soft update target network"""
        for target_param, source_param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(
                self.tau * source_param.data + (1.0 - self.tau) * target_param.data
            )
    
    def save(self, path):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„"""
        checkpoint = {
            'actors': [actor.state_dict() for actor in self.actors],
            'critics': [critic.state_dict() for critic in self.critics],
            'noise_scale': self.noise_scale
        }
        torch.save(checkpoint, path)
    
    def load(self, path):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„"""
        checkpoint = torch.load(path, map_location=self.device)
        for i in range(self.num_agents):
            self.actors[i].load_state_dict(checkpoint['actors'][i])
            self.critics[i].load_state_dict(checkpoint['critics'][i])
        
        if 'noise_scale' in checkpoint:
            self.noise_scale = checkpoint['noise_scale']


# ============================================================================
# VARIANT 5: Simpler Architecture
# ============================================================================

class SimplerArchVariant:
    """Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ø¨Ø§ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ú©Ù…ØªØ±"""
    
    def __init__(self, obs_dim, action_dim, num_agents, **config):
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.num_agents = num_agents
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
        self.gamma = config.get("gamma", 0.95)
        self.tau = config.get("tau", 0.001)
        self.batch_size = config.get("batch_size", 64)
        self.actor_lr = config.get("actor_lr", 1e-4)
        self.critic_lr = config.get("critic_lr", 1e-3)
        
        # Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©â€ŒØªØ±
        self.actors = []
        self.actor_targets = []
        self.critics = []
        self.critic_targets = []
        self.actor_optimizers = []
        self.critic_optimizers = []
        
        for _ in range(num_agents):
            # âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©ÙˆÚ†Ú©â€ŒØªØ± (256 Ø¨Ù‡ Ø¬Ø§ÛŒ 512)
            actor = SmallActor(obs_dim, action_dim, hidden=256).to(self.device)
            actor_target = SmallActor(obs_dim, action_dim, hidden=256).to(self.device)
            actor_target.load_state_dict(actor.state_dict())
            
            total_state_dim = obs_dim * num_agents
            total_action_dim = action_dim * num_agents
            critic = SmallCritic(total_state_dim, total_action_dim, hidden=256).to(self.device)
            critic_target = SmallCritic(total_state_dim, total_action_dim, hidden=256).to(self.device)
            critic_target.load_state_dict(critic.state_dict())
            
            actor_optimizer = torch.optim.Adam(actor.parameters(), lr=self.actor_lr)
            critic_optimizer = torch.optim.Adam(critic.parameters(), lr=self.critic_lr)
            
            self.actors.append(actor)
            self.actor_targets.append(actor_target)
            self.critics.append(critic)
            self.critic_targets.append(critic_target)
            self.actor_optimizers.append(actor_optimizer)
            self.critic_optimizers.append(critic_optimizer)
        
        buffer_size = config.get("buffer_size", 100000)
        self.replay_buffers = [ReplayBuffer(buffer_size) for _ in range(num_agents)]
        
        # âœ… Exploration noise
        self.noise_scale = 0.1
        self.noise_decay = 0.9995
        self.min_noise = 0.01
        
        print("ğŸ“Œ SimplerArchVariant: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©â€ŒØªØ± (hidden=256)")
    
    def select_action(self, agent_id, obs, add_noise=True):
        """Ø§Ù†ØªØ®Ø§Ø¨ action"""
        if isinstance(agent_id, str):
            agent_id = int(agent_id.split('_')[-1])
        
        obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action = self.actors[agent_id](obs_tensor)
        
        action = action.cpu().numpy()[0]
        
        if add_noise:
            noise = np.random.normal(0, self.noise_scale, size=action.shape)
            action = action + noise
            self.noise_scale = max(self.min_noise, self.noise_scale * self.noise_decay)
        
        # âœ… Clipping Ø¯Ù‚ÛŒÙ‚
        action = np.clip(action, 0.0, 1.0)
        action = np.round(action, decimals=6)
        
        epsilon = 1e-7
        action = np.where(action < epsilon, epsilon, action)
        action = np.where(action > (1.0 - epsilon), 1.0 - epsilon, action)
        
        return action.astype(np.float32)
    
    def store_transition(self, agent_id, state, action, reward, next_state, done):
        """Ø°Ø®ÛŒØ±Ù‡ ØªØ¬Ø±Ø¨Ù‡"""
        if isinstance(agent_id, str):
            agent_id = int(agent_id.split('_')[-1])
        
        self.replay_buffers[agent_id].push(state, action, reward, next_state, done)
    
    def update(self):
        """Ø¢Ù¾Ø¯ÛŒØª Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ - Ø¨Ø§ Gradient Clipping"""
        for agent_id in range(self.num_agents):
            if len(self.replay_buffers[agent_id]) < self.batch_size:
                continue
            
            states, actions, rewards, next_states, dones = \
                self.replay_buffers[agent_id].sample(self.batch_size)
            
            states = torch.FloatTensor(states).to(self.device)
            actions = torch.FloatTensor(actions).to(self.device)
            rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)
            next_states = torch.FloatTensor(next_states).to(self.device)
            dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)
            
            all_states = states.repeat(1, self.num_agents)
            all_actions = actions.repeat(1, self.num_agents)
            all_next_states = next_states.repeat(1, self.num_agents)
            
            # ========== Update Critic ==========
            with torch.no_grad():
                next_actions = self.actor_targets[agent_id](next_states)
                all_next_actions = next_actions.repeat(1, self.num_agents)
                
                target_q = self.critic_targets[agent_id](all_next_states, all_next_actions)
                target_q = rewards + (1 - dones) * self.gamma * target_q
            
            current_q = self.critics[agent_id](all_states, all_actions)
            critic_loss = F.mse_loss(current_q, target_q)
            
            self.critic_optimizers[agent_id].zero_grad()
            critic_loss.backward()
            
            # âœ… Gradient clipping Ø¨Ø±Ø§ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø³Ø§Ø¯Ù‡â€ŒØªØ±
            torch.nn.utils.clip_grad_norm_(
                self.critics[agent_id].parameters(), 
                max_norm=0.5
            )
            
            self.critic_optimizers[agent_id].step()
            
            # ========== Update Actor ==========
            predicted_actions = self.actors[agent_id](states)
            all_predicted_actions = predicted_actions.repeat(1, self.num_agents)
            
            actor_loss = -self.critics[agent_id](all_states, all_predicted_actions).mean()
            
            self.actor_optimizers[agent_id].zero_grad()
            actor_loss.backward()
            
            # âœ… Gradient clipping
            torch.nn.utils.clip_grad_norm_(
                self.actors[agent_id].parameters(), 
                max_norm=0.5
            )
            
            self.actor_optimizers[agent_id].step()
            
            # ========== Soft Update ==========
            self._soft_update(self.actors[agent_id], self.actor_targets[agent_id])
            self._soft_update(self.critics[agent_id], self.critic_targets[agent_id])
    
    def _soft_update(self, source, target):
        """Soft update target network"""
        for target_param, source_param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(
                self.tau * source_param.data + (1.0 - self.tau) * target_param.data
            )
    
    def save(self, path):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„"""
        checkpoint = {
            'actors': [actor.state_dict() for actor in self.actors],
            'critics': [critic.state_dict() for critic in self.critics],
            'noise_scale': self.noise_scale
        }
        torch.save(checkpoint, path)
    
    def load(self, path):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„"""
        checkpoint = torch.load(path, map_location=self.device)
        for i in range(self.num_agents):
            self.actors[i].load_state_dict(checkpoint['actors'][i])
            self.critics[i].load_state_dict(checkpoint['critics'][i])
        
        if 'noise_scale' in checkpoint:
            self.noise_scale = checkpoint['noise_scale']


# ============================================================================
# Factory Function
# ============================================================================

def create_ablation_variant(variant_name, obs_dim, action_dim, num_agents, **config):
    """
    Ø³Ø§Ø®Øª variant Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±
    
    Args:
        variant_name: Ù†Ø§Ù… variant ('full_model', 'no_gat', ...)
        obs_dim: Ø§Ø¨Ø¹Ø§Ø¯ observation
        action_dim: Ø§Ø¨Ø¹Ø§Ø¯ action
        num_agents: ØªØ¹Ø¯Ø§Ø¯ agents
        **config: ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø¶Ø§ÙÛŒ
    
    Returns:
        instance Ø§Ø² variant Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±
    """
    
    variants = {
        'full_model': FullMADDPGVariant,
        'no_gat': NoGATVariant,
        'no_temporal': NoTemporalVariant,
        'decentralized': DecentralizedVariant,
        'simpler_arch': SimplerArchVariant
    }
    
    if variant_name not in variants:
        raise ValueError(f"Unknown variant: {variant_name}. "
                        f"Available: {list(variants.keys())}")
    
    print(f"\nğŸš€ Creating {variant_name} variant...")
    return variants[variant_name](obs_dim, action_dim, num_agents, **config)
