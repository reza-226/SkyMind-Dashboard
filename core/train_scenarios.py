"""
Training Script for Different Obstacle Scenarios
================================================
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø±Ø§ÛŒ Ø³Ù‡ Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ Ù…Ø®ØªÙ„Ù Ù…ÙˆØ§Ù†Ø¹
"""

import numpy as np
import torch
import time
from pathlib import Path
from typing import Dict, List
import json

from core.env_multi import MultiUAVEnv
from agents.agent_maddpg_multi import MADDPGAgent


class ScenarioTrainer:
    """Ú©Ù„Ø§Ø³ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø±Ø§ÛŒ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
    
    def __init__(
        self,
        scenario_name: str,
        obstacle_mode: str,
        n_episodes: int = 1000,
        max_steps: int = 500,
        save_dir: str = "results/scenarios"
    ):
        self.scenario_name = scenario_name
        self.obstacle_mode = obstacle_mode
        self.n_episodes = n_episodes
        self.max_steps = max_steps
        self.save_dir = Path(save_dir) / scenario_name
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ÛŒØ·
        self.env = MultiUAVEnv(
            n_uavs=3,
            map_size=100.0,
            obstacle_mode=obstacle_mode,
            max_steps=max_steps,
            seed=42
        )
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø¹Ø§Ù…Ù„ MADDPG
        obs_dim = list(self.env.observation_space.values())[0].shape[0]
        act_dim = list(self.env.action_space.values())[0].shape[0]
        
        self.agent = MADDPGAgent(
            n_agents=3,
            obs_dim=obs_dim,
            act_dim=act_dim,
            hidden_dim=128,
            lr_actor=1e-4,
            lr_critic=1e-3,
            gamma=0.99,
            tau=0.01
        )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
        self.metrics = {
            'episode_rewards': [],
            'episode_lengths': [],
            'collisions': [],
            'tasks_completed': [],
            'energy_consumed': [],
            'actor_losses': [],
            'critic_losses': [],
            'collision_risks': []
        }
        
        print(f"âœ… ScenarioTrainer Ø¨Ø±Ø§ÛŒ '{scenario_name}' Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯")
        print(f"   Ø­Ø§Ù„Øª Ù…ÙˆØ§Ù†Ø¹: {obstacle_mode}")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ Ø§Ù¾ÛŒØ²ÙˆØ¯Ù‡Ø§: {n_episodes}")
    
    def train(self):
        """Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„"""
        print(f"\n{'='*70}")
        print(f"ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ø³Ù†Ø§Ø±ÛŒÙˆ: {self.scenario_name}")
        print(f"{'='*70}\n")
        
        start_time = time.time()
        best_reward = -np.inf
        
        for episode in range(self.n_episodes):
            episode_reward, episode_metrics = self._run_episode(episode)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
            self.metrics['episode_rewards'].append(episode_reward)
            self.metrics['episode_lengths'].append(episode_metrics['steps'])
            self.metrics['collisions'].append(episode_metrics['collisions'])
            self.metrics['tasks_completed'].append(episode_metrics['tasks'])
            self.metrics['energy_consumed'].append(episode_metrics['energy'])
            
            if episode_metrics['actor_loss'] is not None:
                self.metrics['actor_losses'].append(episode_metrics['actor_loss'])
                self.metrics['critic_losses'].append(episode_metrics['critic_loss'])
            
            self.metrics['collision_risks'].append(episode_metrics['avg_risk'])
            
            # Ù„Ø§Ú¯ Ù¾ÛŒØ´Ø±ÙØª
            if (episode + 1) % 50 == 0:
                avg_reward = np.mean(self.metrics['episode_rewards'][-50:])
                avg_collisions = np.mean(self.metrics['collisions'][-50:])
                avg_tasks = np.mean(self.metrics['tasks_completed'][-50:])
                
                elapsed = time.time() - start_time
                eta = (elapsed / (episode + 1)) * (self.n_episodes - episode - 1)
                
                print(f"ğŸ“Š Episode {episode + 1}/{self.n_episodes}")
                print(f"   â”œâ”€ Avg Reward (50): {avg_reward:.2f}")
                print(f"   â”œâ”€ Avg Collisions: {avg_collisions:.2f}")
                print(f"   â”œâ”€ Avg Tasks: {avg_tasks:.2f}")
                print(f"   â”œâ”€ Actor Loss: {episode_metrics['actor_loss']:.4f}" if episode_metrics['actor_loss'] else "")
                print(f"   â”œâ”€ Elapsed: {elapsed/60:.1f}m | ETA: {eta/60:.1f}m")
                print(f"   â””â”€ {'â”€'*50}")
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
                if avg_reward > best_reward:
                    best_reward = avg_reward
                    self._save_checkpoint('best_model.pt')
                    print(f"   ğŸ’¾ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯! (Reward: {best_reward:.2f})")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
        self._save_checkpoint('final_model.pt')
        self._save_metrics()
        self._generate_plots()
        
        total_time = time.time() - start_time
        print(f"\n{'='*70}")
        print(f"âœ… Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
        print(f"   Ø²Ù…Ø§Ù† Ú©Ù„: {total_time/60:.1f} Ø¯Ù‚ÛŒÙ‚Ù‡")
        print(f"   Ø¨Ù‡ØªØ±ÛŒÙ† Reward: {best_reward:.2f}")
        print(f"   ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¯Ø±: {self.save_dir}")
        print(f"{'='*70}\n")
    
    def _run_episode(self, episode: int) -> tuple:
        """Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ© Ø§Ù¾ÛŒØ²ÙˆØ¯"""
        obs, info = self.env.reset(seed=42 + episode)
        
        episode_reward = 0
        total_collisions = 0
        total_tasks = 0
        total_energy = 0
        total_risk = 0
        steps = 0
        
        actor_losses = []
        critic_losses = []
        
        for step in range(self.max_steps):
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø¹Ù…Ù„
            actions = {}
            for i in range(3):
                obs_i = obs[f'agent_{i}']
                action = self.agent.select_action(i, obs_i, add_noise=True)
                actions[f'agent_{i}'] = action
            
            # Ø§Ø¬Ø±Ø§ÛŒ Ø¹Ù…Ù„
            next_obs, rewards, dones, infos = self.env.step(actions)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± replay buffer
            for i in range(3):
                self.agent.store_transition(
                    obs[f'agent_{i}'],
                    actions[f'agent_{i}'],
                    rewards[f'agent_{i}'],
                    next_obs[f'agent_{i}'],
                    dones[f'agent_{i}']
                )
            
            # Ø¢Ù…ÙˆØ²Ø´ Ø¹Ø§Ù…Ù„
            if self.agent.can_update():
                losses = self.agent.update()
                if losses:
                    actor_losses.append(losses['actor_loss'])
                    critic_losses.append(losses['critic_loss'])
            
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
            episode_reward += sum(rewards.values())
            for i in range(3):
                total_collisions += 1 if infos[f'agent_{i}']['collision'] else 0
                total_tasks += infos[f'agent_{i}']['tasks_completed']
                total_energy += infos[f'agent_{i}']['energy_consumed']
                total_risk += infos[f'agent_{i}']['collision_risk']
            
            obs = next_obs
            steps += 1
            
            if all(dones.values()):
                break
        
        return episode_reward, {
            'steps': steps,
            'collisions': total_collisions,
            'tasks': total_tasks,
            'energy': total_energy,
            'avg_risk': total_risk / (steps * 3),
            'actor_loss': np.mean(actor_losses) if actor_losses else None,
            'critic_loss': np.mean(critic_losses) if critic_losses else None
        }
    
    def _save_checkpoint(self, filename: str):
        """Ø°Ø®ÛŒØ±Ù‡ checkpoint"""
        checkpoint = {
            'scenario_name': self.scenario_name,
            'obstacle_mode': self.obstacle_mode,
            'episode': len(self.metrics['episode_rewards']),
            'agent_state': self.agent.state_dict(),
            'metrics': self.metrics
        }
        
        torch.save(checkpoint, self.save_dir / filename)
    
    def _save_metrics(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§"""
        # NPZ format
        np.savez(
            self.save_dir / 'training_metrics.npz',
            episode_rewards=np.array(self.metrics['episode_rewards']),
            episode_lengths=np.array(self.metrics['episode_lengths']),
            collisions=np.array(self.metrics['collisions']),
            tasks_completed=np.array(self.metrics['tasks_completed']),
            energy_consumed=np.array(self.metrics['energy_consumed']),
            actor_losses=np.array(self.metrics['actor_losses']),
            critic_losses=np.array(self.metrics['critic_losses']),
            collision_risks=np.array(self.metrics['collision_risks'])
        )
        
        # JSON format Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ
        summary = {
            'scenario': self.scenario_name,
            'obstacle_mode': self.obstacle_mode,
            'total_episodes': self.n_episodes,
            'final_avg_reward': float(np.mean(self.metrics['episode_rewards'][-100:])),
            'best_reward': float(np.max(self.metrics['episode_rewards'])),
            'avg_collisions': float(np.mean(self.metrics['collisions'])),
            'avg_tasks_completed': float(np.mean(self.metrics['tasks_completed'])),
            'total_energy_consumed': float(np.sum(self.metrics['energy_consumed']))
        }
        
        with open(self.save_dir / 'summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯: {self.save_dir}")
    
    def _generate_plots(self):
        """ØªÙˆÙ„ÛŒØ¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§"""
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle(f'Ù†ØªØ§ÛŒØ¬ Ø¢Ù…ÙˆØ²Ø´ - {self.scenario_name}', 
                    fontsize=16, weight='bold', y=0.995)
        
        # 1. Episode Rewards
        ax = axes[0, 0]
        rewards = self.metrics['episode_rewards']
        ax.plot(rewards, alpha=0.3, color='blue', label='Raw')
        if len(rewards) > 50:
            window = 50
            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')
            ax.plot(range(window-1, len(rewards)), smoothed, 
                   color='red', linewidth=2, label=f'MA({window})')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Total Reward')
        ax.set_title('Episode Rewards')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. Collisions
        ax = axes[0, 1]
        collisions = self.metrics['collisions']
        ax.plot(collisions, alpha=0.4, color='orange')
        if len(collisions) > 50:
            smoothed = np.convolve(collisions, np.ones(50)/50, mode='valid')
            ax.plot(range(49, len(collisions)), smoothed, 
                   color='red', linewidth=2)
        ax.set_xlabel('Episode')
        ax.set_ylabel('Number of Collisions')
        ax.set_title('Ø¨Ø±Ø®ÙˆØ±Ø¯Ù‡Ø§ Ø¯Ø± Ø·ÙˆÙ„ Ø¢Ù…ÙˆØ²Ø´')
        ax.grid(True, alpha=0.3)
        
        # 3. Tasks Completed
        ax = axes[0, 2]
        tasks = self.metrics['tasks_completed']
        ax.plot(tasks, alpha=0.4, color='green')
        if len(tasks) > 50:
            smoothed = np.convolve(tasks, np.ones(50)/50, mode='valid')
            ax.plot(range(49, len(tasks)), smoothed, 
                   color='darkgreen', linewidth=2)
        ax.set_xlabel('Episode')
        ax.set_ylabel('Tasks')
        ax.set_title('ÙˆØ¸Ø§ÛŒÙ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù‡')
        ax.grid(True, alpha=0.3)
        
        # 4. Energy Consumption
        ax = axes[1, 0]
        energy = self.metrics['energy_consumed']
        ax.plot(energy, alpha=0.4, color='purple')
        if len(energy) > 50:
            smoothed = np.convolve(energy, np.ones(50)/50, mode='valid')
            ax.plot(range(49, len(energy)), smoothed, 
                   color='darkviolet', linewidth=2)
        ax.set_xlabel('Episode')
        ax.set_ylabel('Energy (J)')
        ax.set_title('Ù…ØµØ±Ù Ø§Ù†Ø±Ú˜ÛŒ')
        ax.grid(True, alpha=0.3)
        
        # 5. Actor Loss
        ax = axes[1, 1]
        if self.metrics['actor_losses']:
            losses = self.metrics['actor_losses']
            ax.plot(losses, alpha=0.4, color='red')
            if len(losses) > 50:
                smoothed = np.convolve(losses, np.ones(50)/50, mode='valid')
                ax.plot(range(49, len(losses)), smoothed, 
                       color='darkred', linewidth=2)
        ax.set_xlabel('Update Step')
        ax.set_ylabel('Loss')
        ax.set_title('Actor Loss')
        ax.grid(True, alpha=0.3)
        
        # 6. Collision Risk
        ax = axes[1, 2]
        risks = self.metrics['collision_risks']
        ax.plot(risks, alpha=0.4, color='brown')
        if len(risks) > 50:
            smoothed = np.convolve(risks, np.ones(50)/50, mode='valid')
            ax.plot(range(49, len(risks)), smoothed, 
                   color='maroon', linewidth=2)
        ax.set_xlabel('Episode')
        ax.set_ylabel('Average Risk')
        ax.set_title('Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø±ÛŒØ³Ú© Ø¨Ø±Ø®ÙˆØ±Ø¯')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / 'training_curves.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯: {self.save_dir / 'training_curves.png'}")


def main():
    """Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³Ù‡ Ø³Ù†Ø§Ø±ÛŒÙˆ"""
    
    scenarios = [
        {
            'name': 'scenario_none',
            'obstacle_mode': 'none',
            'description': 'Ø¨Ø¯ÙˆÙ† Ù…Ø§Ù†Ø¹ (Baseline)'
        },
        {
            'name': 'scenario_moderate',
            'obstacle_mode': 'moderate',
            'description': 'Ù…ÙˆØ§Ù†Ø¹ Ù…ØªÙˆØ³Ø· (3-5 Ø«Ø§Ø¨Øª)'
        },
        {
            'name': 'scenario_complex',
            'obstacle_mode': 'complex',
            'description': 'Ù…ÙˆØ§Ù†Ø¹ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ (8-10 Ø«Ø§Ø¨Øª + 2-3 Ù…ØªØ­Ø±Ú©)'
        }
    ]
    
    print("\n" + "="*70)
    print("ğŸ¯ Ø¢Ù…ÙˆØ²Ø´ Ø³ÛŒØ³ØªÙ… SkyMind Ø¨Ø§ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù…ÙˆØ§Ù†Ø¹")
    print("="*70 + "\n")
    
    for i, scenario in enumerate(scenarios, 1):
        print(f"\n{'ğŸ”·'*35}")
        print(f"Ø³Ù†Ø§Ø±ÛŒÙˆ {i}/3: {scenario['name']}")
        print(f"ØªÙˆØ¶ÛŒØ­: {scenario['description']}")
        print(f"{'ğŸ”·'*35}\n")
        
        trainer = ScenarioTrainer(
            scenario_name=scenario['name'],
            obstacle_mode=scenario['obstacle_mode'],
            n_episodes=1000,
            max_steps=500
        )
        
        trainer.train()
        
        print(f"\nâœ… Ø³Ù†Ø§Ø±ÛŒÙˆ {scenario['name']} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!\n")
    
    print("\n" + "="*70)
    print("ğŸ‰ ØªÙ…Ø§Ù… Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯!")
    print("ğŸ“ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± Ù¾ÙˆØ´Ù‡ 'results/scenarios/' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
