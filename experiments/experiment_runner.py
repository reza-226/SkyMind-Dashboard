"""
Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ø³Ø·Ø­ÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
"""

import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List
from .scenario_loader import ScenarioLoader, Scenario


class ExperimentRunner:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ÛŒ Multi-Tier"""
    
    def __init__(self, config_path: str = "experiments/scenarios_config.yaml"):
        self.loader = ScenarioLoader(config_path)
        self.loader.load()
        self.results_dir = Path("results/multi_tier_evaluation")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
    def run_all_scenarios(self):
        """Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§"""
        print("\n" + "="*60)
        print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ø³Ø·Ø­ÛŒ")
        print("="*60)
        
        all_results = []
        
        for idx, scenario in enumerate(self.loader.scenarios, 1):
            print(f"\nğŸ“ [{idx}/{len(self.loader.scenarios)}] Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù†Ø§Ø±ÛŒÙˆ: {scenario.id}")
            print(f"   Tier: {scenario.tier} | Complexity: {scenario.complexity}")
            
            result = self._run_single_scenario(scenario)
            all_results.append(result)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
            self._save_scenario_result(scenario, result)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ú©Ù„ÛŒ
        self._save_final_results(all_results)
        
        print("\n" + "="*60)
        print("âœ… ØªÙ…Ø§Ù… Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
        print(f"ğŸ“ Ù†ØªØ§ÛŒØ¬ Ø¯Ø±: {self.results_dir}")
        print("="*60)
        
    def _run_single_scenario(self, scenario: Scenario) -> Dict:
        """Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ© Ø³Ù†Ø§Ø±ÛŒÙˆ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØªØ§ÛŒØ¬"""
        
        # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¢Ù…ÙˆØ²Ø´ (Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
        # Ø¯Ø± Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ù‚Ø¹ÛŒØŒ Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ø¨Ø§ MADDPG Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒØ´ÙˆØ¯
        num_episodes = 4000
        simulated_rewards = self._simulate_training(scenario, num_episodes)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯
        metrics = self._calculate_metrics(scenario, simulated_rewards)
        
        result = {
            "scenario_id": scenario.id,
            "tier": scenario.tier,
            "complexity": scenario.complexity,
            "config": {
                "num_tasks": scenario.complexity_specs.num_tasks,
                "num_uavs": scenario.complexity_specs.num_uavs,
                "processing_capacity": scenario.tier_specs.processing_capacity,
                "communication_delay": scenario.tier_specs.communication_delay,
                "energy_per_flop": scenario.tier_specs.energy_per_flop,
                "reliability": scenario.tier_specs.reliability
            },
            "training_results": {
                "total_episodes": num_episodes,
                "final_reward": simulated_rewards[-1],
                "avg_reward_last_100": np.mean(simulated_rewards[-100:]),
                "convergence_episode": self._find_convergence(simulated_rewards),
                "reward_history": simulated_rewards.tolist()
            },
            "metrics": metrics,
            "timestamp": datetime.now().isoformat()
        }
        
        return result
    
    def _simulate_training(self, scenario: Scenario, num_episodes: int) -> np.ndarray:
        """Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¢Ù…ÙˆØ²Ø´ (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÙˆÙ‚Øª MADDPG)"""
        
        # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Tier Ùˆ Complexity
        tier_factors = {
            "ground": 0.6,
            "edge": 0.75,
            "fog": 0.85,
            "cloud": 0.95
        }
        
        complexity_factors = {
            "easy": 0.9,
            "medium": 0.75,
            "hard": 0.6
        }
        
        base_reward = -100
        improvement_rate = 0.002
        tier_factor = tier_factors[scenario.tier]
        complexity_factor = complexity_factors[scenario.complexity]
        
        rewards = []
        for ep in range(num_episodes):
            progress = min(1.0, ep / (num_episodes * 0.7))
            reward = base_reward * (1 - progress * tier_factor * complexity_factor)
            noise = np.random.normal(0, 5)
            rewards.append(reward + noise)
        
        return np.array(rewards)
    
    def _find_convergence(self, rewards: np.ndarray, window: int = 100, threshold: float = 5.0) -> int:
        """ØªØ´Ø®ÛŒØµ Ù†Ù‚Ø·Ù‡ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ"""
        if len(rewards) < window:
            return len(rewards)
        
        for i in range(window, len(rewards)):
            recent_std = np.std(rewards[i-window:i])
            if recent_std < threshold:
                return i
        
        return len(rewards)
    
    def _calculate_metrics(self, scenario: Scenario, rewards: np.ndarray) -> Dict:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ÛŒ"""
        
        specs = scenario.tier_specs
        complexity = scenario.complexity_specs
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Latency (ms)
        latency = (
            specs.communication_delay * 1000 +  # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡
            (complexity.num_tasks * 100) / specs.processing_capacity
        )
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Energy Consumption (Joule)
        avg_task_size = np.mean(complexity.task_size_range)  # MB
        flops_per_task = avg_task_size * 1e6  # ØªØ®Ù…ÛŒÙ† FLOP
        energy = (
            complexity.num_tasks * flops_per_task * specs.energy_per_flop +
            specs.transmission_power * specs.communication_delay * complexity.num_uavs
        )
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Scalability Score (0-1)
        max_tasks = 50  # Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Task Ø¯Ø± Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Hard
        scalability = 1 - (complexity.num_tasks / max_tasks) * (1 - specs.reliability)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Success Rate
        success_rate = min(1.0, specs.reliability * (1 + np.mean(rewards[-100:]) / 100))
        
        # â­ Ù…Ø­Ø§Ø³Ø¨Ù‡ Throughput (tasks/sec)
        # ÙØ±Ù…ÙˆÙ„: ØªØ¹Ø¯Ø§Ø¯ ØªØ³Ú©â€ŒÙ‡Ø§ / (Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´ + ØªØ§Ø®ÛŒØ± Ø§Ø±ØªØ¨Ø§Ø·ÛŒ)
        processing_time = (complexity.num_tasks * 100) / specs.processing_capacity  # Ø«Ø§Ù†ÛŒÙ‡
        total_time = processing_time + specs.communication_delay  # Ø«Ø§Ù†ÛŒÙ‡
        throughput = complexity.num_tasks / max(total_time, 0.001)  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± ØµÙØ±
        
        return {
            "latency_ms": round(latency, 2),
            "energy_joules": round(energy, 4),
            "scalability_score": round(scalability, 4),
            "success_rate": round(max(0.0, success_rate), 4),
            "throughput": round(throughput, 2)  # â­ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
        }
    
    def _save_scenario_result(self, scenario: Scenario, result: Dict):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ ÛŒÚ© Ø³Ù†Ø§Ø±ÛŒÙˆ"""
        scenario_dir = self.results_dir / scenario.id
        scenario_dir.mkdir(exist_ok=True)
        
        output_file = scenario_dir / "result.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"   ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_file}")
    
    def _save_final_results(self, all_results: List[Dict]):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ Ú©Ù„ÛŒ"""
        output_file = self.results_dir / "final_results.json"
        
        summary = {
            "metadata": {
                "total_scenarios": len(all_results),
                "timestamp": datetime.now().isoformat(),
                "description": "Multi-Tier Evaluation Results"
            },
            "results": all_results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_file}")


if __name__ == "__main__":
    runner = ExperimentRunner()
    runner.run_all_scenarios()
