"""
run_maddpg_experiment.py
Ø§Ø¬Ø±Ø§ÛŒ MADDPG Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±
"""

import sys
import numpy as np
import json
import torch
from pathlib import Path

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ù¾Ø±ÙˆÚ˜Ù‡
sys.path.append(str(Path(__file__).parent))

from core.env_multi import MultiUAVEnv
from agents.agent_maddpg_multi import MADDPG_Agent


class MADDPGTester:
    """ØªØ³Øª Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ MADDPG"""
    
    def __init__(self, model_path: str, env_config: dict):
        self.env = MultiUAVEnv(**env_config)
        self.n_agents = env_config.get('n_agents', 3)
        
        # Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„
        print(f"ğŸ”„ Loading MADDPG model from {model_path}...")
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø¨Ø¹Ø§Ø¯ state Ùˆ action
        state = self.env.reset()
        state_dim = self._get_state_dim(state)
        action_dim = 4  # [v, theta, f, o] - velocity, angle, frequency, offload
        
        print(f"   State dimension: {state_dim}")
        print(f"   Action dimension: {action_dim}")
        print(f"   Number of agents: {self.n_agents}")
        
        # âœ… Ø³Ø§Ø®Øª agent Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ØµØ­ÛŒØ­
        self.agent = MADDPG_Agent(
            state_dim=state_dim,      # âœ… state_dim (Ù†Ù‡ obs_dim)
            action_dim=action_dim,    # âœ… action_dim (Ù†Ù‡ act_dim)
            n_agents=self.n_agents,
            lr=1e-4,                  # âœ… lr (Ù†Ù‡ lr_actor/lr_critic)
            gamma=0.95
        )
        
        # Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† ÙˆØ²Ù†â€ŒÙ‡Ø§
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                self.agent.load_state_dict(checkpoint['model_state_dict'])
            else:
                # Ø§Ú¯Ø± checkpoint Ø®ÙˆØ¯ state_dict Ø¨Ø§Ø´Ø¯
                try:
                    self.agent.load_state_dict(checkpoint)
                except:
                    print("   âš ï¸  Checkpoint format not compatible, trying individual actors...")
                    self._load_individual_actors(Path(model_path).parent)
            print("âœ… Model loaded successfully!")
        except Exception as e:
            print(f"âš ï¸  Warning: Could not load checkpoint: {e}")
            print("   Trying to load individual actor models...")
            self._load_individual_actors(Path(model_path).parent)
    
    def _load_individual_actors(self, model_dir: Path):
        """Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ actor Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡"""
        loaded = False
        if hasattr(self.agent, 'actors'):
            for i in range(self.n_agents):
                actor_path = model_dir / f'actor_agent{i}.pt'
                if actor_path.exists():
                    try:
                        self.agent.actors[i].load_state_dict(
                            torch.load(actor_path, map_location='cpu')
                        )
                        print(f"   âœ… Loaded actor for agent {i}")
                        loaded = True
                    except Exception as e:
                        print(f"   âš ï¸  Failed to load actor {i}: {e}")
                else:
                    print(f"   âš ï¸  Actor model not found: {actor_path}")
        
        if not loaded:
            print("   âš ï¸  No models loaded - using random initialization")
    
    def _get_state_dim(self, state):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø¨Ø¹Ø§Ø¯ state"""
        if isinstance(state, dict):
            total_dim = 0
            for key, value in state.items():
                if isinstance(value, np.ndarray):
                    total_dim += value.size
                elif isinstance(value, (list, tuple)):
                    total_dim += len(value)
                else:
                    total_dim += 1
            return total_dim
        elif isinstance(state, np.ndarray):
            return state.size
        return len(state)
    
    def _state_to_vector(self, state):
        """ØªØ¨Ø¯ÛŒÙ„ state Ø¨Ù‡ vector Ø¨Ø±Ø§ÛŒ MADDPG"""
        if isinstance(state, dict):
            vectors = []
            for key in sorted(state.keys()):
                value = state[key]
                if isinstance(value, np.ndarray):
                    vectors.append(value.flatten())
                elif isinstance(value, (list, tuple)):
                    vectors.append(np.array(value).flatten())
                elif isinstance(value, (int, float)):
                    vectors.append(np.array([value]))
            return np.concatenate(vectors)
        elif isinstance(state, np.ndarray):
            return state.flatten()
        return np.array(state)
    
    def run_episode(self, episode_num: int, max_steps: int = 200):
        """Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ© Ø§Ù¾ÛŒØ²ÙˆØ¯"""
        state = self.env.reset()
        done = False
        episode_reward = 0
        episode_energy = 0
        episode_delays = []
        steps = 0
        
        while not done and steps < max_steps:
            # ØªØ¨Ø¯ÛŒÙ„ state Ø¨Ù‡ vector
            state_vector = self._state_to_vector(state)
            
            # Ø§Ù†ØªØ®Ø§Ø¨ action
            try:
                # ÙØ±Ø¶: agent.act() ÛŒÚ© Ù…ØªØ¯ Ø¯Ø§Ø±Ø¯
                if hasattr(self.agent, 'act'):
                    actions = self.agent.act(state_vector)
                else:
                    # Ø§Ú¯Ø± Ù…ØªØ¯ Ø¯ÛŒÚ¯Ø±ÛŒ Ø¯Ø§Ø±Ø¯ØŒ Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
                    actions = np.random.uniform(-1, 1, (self.n_agents, 4))
                
                # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ numpy Ø§Ú¯Ø± tensor Ø§Ø³Øª
                if isinstance(actions, torch.Tensor):
                    actions = actions.detach().cpu().numpy()
                
                # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² shape ØµØ­ÛŒØ­
                if actions.ndim == 1:
                    actions = actions.reshape(self.n_agents, -1)
                
            except Exception as e:
                print(f"âš ï¸  Error in action selection: {e}")
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² action ØªØµØ§Ø¯ÙÛŒ Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
                actions = np.random.uniform(-1, 1, (self.n_agents, 4))
            
            # ØªØ¨Ø¯ÛŒÙ„ actions Ø¨Ù‡ Ù„ÛŒØ³Øª Ø¨Ø±Ø§ÛŒ Ù…Ø­ÛŒØ·
            actions_list = [actions[i] for i in range(self.n_agents)]
            
            # Ø§Ø¬Ø±Ø§ÛŒ action Ø¯Ø± Ù…Ø­ÛŒØ·
            try:
                result = self.env.step(actions_list)
            except Exception as e:
                print(f"âš ï¸  Error in step {steps}: {e}")
                break
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø±ÙˆØ¬ÛŒ step
            if len(result) == 4:
                next_state, reward, done, info = result
            elif len(result) == 5:
                next_state, reward, done, truncated, info = result
                done = done or truncated
            else:
                print(f"âš ï¸  Unexpected step output length: {len(result)}")
                break
            
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
            if isinstance(reward, (list, tuple, np.ndarray)):
                episode_reward += sum(reward)
            else:
                episode_reward += reward
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Energy
            if isinstance(next_state, dict) and 'energy' in next_state:
                energy = next_state['energy']
                if isinstance(energy, np.ndarray):
                    episode_energy += np.sum(energy)
                elif isinstance(energy, (list, tuple)):
                    episode_energy += sum(energy)
                else:
                    episode_energy += energy
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Delay
            if isinstance(next_state, dict):
                if 'distances' in next_state and 'uav_velocities' in next_state:
                    distances = np.array(next_state['distances'])
                    velocities = np.array(next_state['uav_velocities'])
                    velocities = np.where(velocities > 0, velocities, 1e-6)
                    delays = distances / velocities
                    episode_delays.append(np.mean(delays))
            
            state = next_state
            steps += 1
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Delay
        avg_delay = np.mean(episode_delays) if episode_delays else 0.0
        
        return {
            'reward': episode_reward,
            'energy': episode_energy,
            'delay': avg_delay,
            'steps': steps
        }
    
    def run_experiments(self, n_episodes: int = 50, max_steps: int = 200):
        """Ø§Ø¬Ø±Ø§ÛŒ n Ø§Ù¾ÛŒØ²ÙˆØ¯ Ùˆ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù†ØªØ§ÛŒØ¬"""
        print(f"\nğŸš€ Starting MADDPG experiments ({n_episodes} episodes)...")
        print(f"   Environment: {self.n_agents} UAVs")
        print(f"   Max steps per episode: {max_steps}")
        print("-" * 60)
        
        results = {
            'rewards': [],
            'energies': [],
            'delays': [],
            'steps': []
        }
        
        for ep in range(n_episodes):
            try:
                ep_result = self.run_episode(ep, max_steps)
                
                results['rewards'].append(ep_result['reward'])
                results['energies'].append(ep_result['energy'])
                results['delays'].append(ep_result['delay'])
                results['steps'].append(ep_result['steps'])
                
                if (ep + 1) % 10 == 0:
                    print(f"  Episode {ep+1}/{n_episodes}: "
                          f"R={ep_result['reward']:.2e}, "
                          f"E={ep_result['energy']:.2e}J, "
                          f"D={ep_result['delay']:.2f}s, "
                          f"Steps={ep_result['steps']}")
            except Exception as e:
                print(f"âš ï¸  Error in episode {ep+1}: {e}")
                continue
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø±
        stats = {
            'policy': 'MADDPG',
            'n_episodes': len(results['rewards']),
            'reward': {
                'mean': float(np.mean(results['rewards'])),
                'std': float(np.std(results['rewards'])),
                'min': float(np.min(results['rewards'])),
                'max': float(np.max(results['rewards']))
            },
            'energy': {
                'mean': float(np.mean(results['energies'])),
                'std': float(np.std(results['energies'])),
                'min': float(np.min(results['energies'])),
                'max': float(np.max(results['energies']))
            },
            'delay': {
                'mean': float(np.mean(results['delays'])),
                'std': float(np.std(results['delays'])),
                'min': float(np.min(results['delays'])),
                'max': float(np.max(results['delays']))
            },
            'steps': {
                'mean': float(np.mean(results['steps'])),
                'std': float(np.std(results['steps']))
            }
        }
        
        print("\n" + "=" * 60)
        print("âœ… MADDPG Experiments Completed!")
        print("=" * 60)
        print(f"  Average Reward: {stats['reward']['mean']:.2e} Â± {stats['reward']['std']:.2e}")
        print(f"  Average Energy: {stats['energy']['mean']:.2e} Â± {stats['energy']['std']:.2e} J")
        print(f"  Average Delay:  {stats['delay']['mean']:.2f} Â± {stats['delay']['std']:.2f} s")
        print(f"  Average Steps:  {stats['steps']['mean']:.1f} Â± {stats['steps']['std']:.1f}")
        print("=" * 60)
        
        return stats, results


def main():
    """Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ"""
    print("\n" + "=" * 60)
    print("MADDPG EVALUATION EXPERIMENT")
    print("=" * 60)
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø­ÛŒØ·
    env_config = {
        'n_agents': 3,
        'n_users': 10,
        'area_size': 1000.0,
        'dt': 1.0,
        'alpha_delay': 1.0,
        'beta_energy': 1e-6,
        'gamma_eff': 1000.0
    }
    
    # Ù…Ø³ÛŒØ± Ù…Ø¯Ù„
    possible_paths = [
        'models/maddpg_sky_env_1.pth',
        'models/actor_agent0.pt',
    ]
    
    model_path = None
    for path in possible_paths:
        if Path(path).exists():
            model_path = path
            break
    
    if model_path is None:
        print("âŒ No MADDPG model found!")
        print("   Please train MADDPG first or provide correct model path.")
        return
    
    print(f"ğŸ“‚ Using model: {model_path}")
    
    try:
        # Ø³Ø§Ø®Øª tester
        tester = MADDPGTester(model_path, env_config)
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§
        stats, raw_results = tester.run_experiments(
            n_episodes=50,
            max_steps=200
        )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        output_dir = Path('results')
        output_dir.mkdir(exist_ok=True)
        
        with open(output_dir / 'maddpg_stats.json', 'w') as f:
            json.dump(stats, f, indent=2)
        
        with open(output_dir / 'maddpg_raw_results.json', 'w') as f:
            json.dump(raw_results, f, indent=2)
        
        print(f"\nğŸ’¾ Results saved to {output_dir}/")
        print("   âœ… maddpg_stats.json")
        print("   âœ… maddpg_raw_results.json")
        
    except Exception as e:
        print(f"\nâŒ Error during execution: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
