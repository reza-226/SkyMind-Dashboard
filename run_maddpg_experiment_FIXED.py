"""
run_maddpg_experiment_FIXED.py
Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ MADDPG Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ØµØ­ÛŒØ­ - Version 2
"""

import numpy as np
import sys
from pathlib import Path
import torch

sys.path.append(str(Path(__file__).parent))

from core.env_multi import MultiUAVEnv
from agents.agent_maddpg_multi import MADDPG_Agent

print("=" * 70)
print("ğŸš€ MADDPG Multi-Agent Experiment (Random Policy)")
print("=" * 70)

# ============================================================================
# Multi-Agent Wrapper
# ============================================================================
class MultiAgentMADDPG:
    """Wrapper Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ú†Ù†Ø¯ agent Ù…Ø³ØªÙ‚Ù„"""
    
    def __init__(self, state_dim, action_dim, n_agents):
        self.n_agents = n_agents
        self.action_dim = action_dim
        
        # Ø³Ø§Ø®Øª ÛŒÚ© agent Ø¨Ø±Ø§ÛŒ Ù‡Ø± UAV
        self.agents = [
            MADDPG_Agent(
                state_dim=state_dim,
                action_dim=action_dim,
                n_agents=n_agents,
                lr=1e-4,
                gamma=0.95
            )
            for _ in range(n_agents)
        ]
        
        print(f"âœ… Created {n_agents} independent agents")
    
    def act(self, state, noise_scale=0.0):
        """
        Ù‡Ø± agent Ø¨Ù‡ state Ú©Ø§Ù…Ù„ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¯Ø§Ø±Ù‡ (centralized training)
        ÙˆÙ„ÛŒ action Ù…Ø³ØªÙ‚Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
        """
        actions = []
        for agent in self.agents:
            action = agent.act(state, noise_scale)
            actions.append(action)
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ (n_agents, action_dim)
        return np.array(actions)

# ============================================================================
# Ù…Ø­ÛŒØ·
# ============================================================================
env_config = {
    'n_agents': 3,
    'n_users': 10,
    'dt': 1.0,
    'area_size': 1000.0,
    'c1': 9.26e-4,
    'c2': 2250.0,
    'bandwidth': 1e6,
    'noise_power': 1e-10,
    'alpha_delay': 1.0,
    'beta_energy': 1e-6,
    'gamma_eff': 1e3
}

env = MultiUAVEnv(**env_config)
print(f"\nğŸ“‹ Environment created successfully!")
print(f"   n_agents: {env_config['n_agents']}")
print(f"   n_users : {env_config['n_users']}")
print(f"   area_size: {env_config['area_size']}m")

# ============================================================================
# Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø®ØªØ§Ø± State
# ============================================================================
print("\n" + "=" * 70)
print("ğŸ” Inspecting State Structure")
print("=" * 70)

state = env.reset()
print(f"\nState type: {type(state)}")

if isinstance(state, dict):
    print(f"State keys: {state.keys()}")
    for key, value in state.items():
        if isinstance(value, np.ndarray):
            print(f"   {key}: shape={value.shape}, dtype={value.dtype}")
        else:
            print(f"   {key}: type={type(value)}, value={value}")
    
    # ØªØ¨Ø¯ÛŒÙ„ dict Ø¨Ù‡ vector Ù…Ø³Ø·Ø­
    state_vector = []
    for key in sorted(state.keys()):
        val = state[key]
        if isinstance(val, np.ndarray):
            state_vector.append(val.flatten())
        else:
            state_vector.append(np.array([val]).flatten())
    
    state_flat = np.concatenate(state_vector)
    state_dim = len(state_flat)
    
    print(f"\nâœ… Flattened state dimension: {state_dim}")
    
elif isinstance(state, np.ndarray):
    state_dim = state.shape[0] if len(state.shape) == 1 else np.prod(state.shape)
    state_flat = state.flatten()
    print(f"\nâœ… Array state dimension: {state_dim}")
    
else:
    print(f"âš ï¸  Unknown state type: {type(state)}")
    sys.exit(1)

# ============================================================================
# Agent
# ============================================================================
multi_agent = MultiAgentMADDPG(
    state_dim=state_dim,
    action_dim=4,
    n_agents=3
)

print(f"\nğŸ“‹ Agent Configuration:")
print(f"   State dim: {state_dim}")
print(f"   Action dim: 4")
print(f"   N agents: 3")
print(f"\nâš ï¸  Using RANDOM policy (no pre-trained models)")

# ============================================================================
# Helper function Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ state
# ============================================================================
def state_to_vector(state):
    """ØªØ¨Ø¯ÛŒÙ„ state (dict ÛŒØ§ array) Ø¨Ù‡ vector"""
    if isinstance(state, dict):
        state_vector = []
        for key in sorted(state.keys()):
            val = state[key]
            if isinstance(val, np.ndarray):
                state_vector.append(val.flatten())
            else:
                state_vector.append(np.array([val]).flatten())
        return np.concatenate(state_vector)
    elif isinstance(state, np.ndarray):
        return state.flatten()
    else:
        return state

# Helper Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ scalar
def to_scalar(value):
    """ØªØ¨Ø¯ÛŒÙ„ Ù‡Ø± Ù†ÙˆØ¹ value Ø¨Ù‡ ÛŒÚ© Ø¹Ø¯Ø¯ scalar"""
    if isinstance(value, np.ndarray):
        return float(np.sum(value))  # ÛŒØ§ np.mean(value)
    elif isinstance(value, (list, tuple)):
        return float(np.sum(value))
    else:
        return float(value)

# ============================================================================
# Ø§Ø¬Ø±Ø§ÛŒ Episode
# ============================================================================
print("\n" + "=" * 70)
print("ğŸ® Running Episodes")
print("=" * 70)

n_episodes = 10
results = {
    'rewards': [],
    'delays': [],
    'energies': []
}

for ep in range(n_episodes):
    state = env.reset()
    state_vec = state_to_vector(state)
    
    episode_reward = 0.0
    episode_delay = 0.0
    episode_energy = 0.0
    done = False
    step = 0
    
    print(f"\nğŸ“ Episode {ep + 1}/{n_episodes}")
    
    while not done and step < 100:
        # Ú¯Ø±ÙØªÙ† action Ø§Ø² multi-agent
        actions = multi_agent.act(state_vec, noise_scale=0.0)
        
        # Ø§Ø¬Ø±Ø§ Ø¯Ø± Ù…Ø­ÛŒØ·
        step_result = env.step(actions)
        
        # Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§
        if len(step_result) == 5:
            next_state, reward, done, truncated, info = step_result
        elif len(step_result) == 4:
            next_state, reward, done, info = step_result
            truncated = False
        else:
            print(f"âš ï¸  Unexpected step output length: {len(step_result)}")
            break
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ (ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ scalar)
        reward_scalar = to_scalar(reward)
        episode_reward += reward_scalar
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ delay Ùˆ energy Ø§Ø² info
        if isinstance(info, dict):
            if 'delay' in info:
                episode_delay += to_scalar(info['delay'])
            if 'energy' in info:
                episode_energy += to_scalar(info['energy'])
        
        state = next_state
        state_vec = state_to_vector(state)
        step += 1
        
        if done or truncated:
            break
    
    results['rewards'].append(episode_reward)
    results['delays'].append(episode_delay)
    results['energies'].append(episode_energy)
    
    print(f"   Steps: {step}")
    print(f"   Total Reward: {episode_reward:.2f}")
    print(f"   Total Delay: {episode_delay:.2f}s")
    print(f"   Total Energy: {episode_energy:.2e}J")

# ============================================================================
# Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
# ============================================================================
print("\n" + "=" * 70)
print("ğŸ“Š Results Summary")
print("=" * 70)

for metric_name, values in results.items():
    mean_val = np.mean(values)
    std_val = np.std(values)
    print(f"\n{metric_name.upper()}:")
    print(f"   Mean: {mean_val:.2e}")
    print(f"   Std:  {std_val:.2e}")
    print(f"   Min:  {np.min(values):.2e}")
    print(f"   Max:  {np.max(values):.2e}")

print("\nâœ… Experiment completed!")
print("\nğŸ’¡ Note: Results are based on RANDOM policy.")
print("   Train the model with correct state_dim=38 for better results.")
