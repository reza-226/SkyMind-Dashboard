"""
run_obstacle_experiments_FINAL_FIXED.py
=======================================
Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµØ­ÛŒØ­ Energy Ùˆ Delay
"""

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from core.env_multi import MultiUAVEnv
import numpy as np
import argparse
import time
from pathlib import Path
import json

class BasePolicy:
    def __init__(self, env):
        self.env = env
        self.n_agents = env.n_agents
    
    def select_action(self, state, step_count, avg_delay, avg_energy):
        raise NotImplementedError

class RandomPolicy(BasePolicy):
    def select_action(self, state, step_count=0, avg_delay=0, avg_energy=0):
        actions = []
        for i in range(self.n_agents):
            v = np.random.uniform(5, 30)
            theta = np.random.uniform(0, 2*np.pi)
            f = np.random.uniform(1e9, 3e9)
            o = np.random.uniform(0.3, 1.0)
            actions.append(np.array([v, theta, f, o], dtype=np.float32))
        return actions

class GreedyPolicy(BasePolicy):
    def select_action(self, state, step_count=0, avg_delay=0, avg_energy=0):
        uav_positions = state['uav_positions']
        user_positions = state['user_positions']
        
        actions = []
        for i in range(self.n_agents):
            uav_pos = uav_positions[i]
            distances = np.linalg.norm(user_positions - uav_pos, axis=1)
            closest_user_idx = np.argmin(distances)
            target = user_positions[closest_user_idx]
            
            delta = target - uav_pos
            theta = np.arctan2(delta[1], delta[0])
            v = 25.0
            f = 2.5e9
            o = 0.8
            
            actions.append(np.array([v, theta, f, o], dtype=np.float32))
        return actions

class ObstacleAwarePolicy(BasePolicy):
    def select_action(self, state, step_count=0, avg_delay=0, avg_energy=0):
        uav_positions = state['uav_positions']
        user_positions = state['user_positions']
        
        actions = []
        for i in range(self.n_agents):
            uav_pos = uav_positions[i]
            
            # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ø±Ø®ÙˆØ±Ø¯ Ø¨Ø§ UAVÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±
            safe_angle = 0
            min_safe_distance = 50
            for j in range(self.n_agents):
                if i != j:
                    other_pos = uav_positions[j]
                    dist = np.linalg.norm(other_pos - uav_pos)
                    if dist < min_safe_distance:
                        repel_vector = uav_pos - other_pos
                        safe_angle += np.arctan2(repel_vector[1], repel_vector[0])
            
            # Ø­Ø±Ú©Øª Ø¨Ù‡ Ø³Ù…Øª Ú©Ø§Ø±Ø¨Ø±Ø§Ù†
            distances = np.linalg.norm(user_positions - uav_pos, axis=1)
            closest_user_idx = np.argmin(distances)
            target = user_positions[closest_user_idx]
            delta = target - uav_pos
            target_angle = np.arctan2(delta[1], delta[0])
            
            # ØªØ±Ú©ÛŒØ¨ Ø²Ø§ÙˆÛŒÙ‡ Ù‡Ø¯Ù Ùˆ Ø²Ø§ÙˆÛŒÙ‡ Ø§Ù…Ù†
            theta = (target_angle + safe_angle) / 2
            v = 20.0
            f = 2e9
            o = 0.9
            
            actions.append(np.array([v, theta, f, o], dtype=np.float32))
        return actions

class HybridPolicy(BasePolicy):
    def select_action(self, state, step_count=0, avg_delay=0, avg_energy=0):
        uav_positions = state['uav_positions']
        user_positions = state['user_positions']
        
        # ØªÙ†Ø¸ÛŒÙ… Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ
        if avg_delay > 5.0:  # Delay Ø²ÛŒØ§Ø¯
            v_base, f_base, o_base = 28.0, 2.8e9, 0.7
        elif avg_energy > 50000:  # Ù…ØµØ±Ù Ø§Ù†Ø±Ú˜ÛŒ Ø²ÛŒØ§Ø¯
            v_base, f_base, o_base = 15.0, 1.8e9, 0.85
        else:  # Ø­Ø§Ù„Øª Ù…ØªØ¹Ø§Ø¯Ù„
            v_base, f_base, o_base = 22.0, 2.3e9, 0.8
        
        actions = []
        for i in range(self.n_agents):
            uav_pos = uav_positions[i]
            
            # Obstacle avoidance
            safe_angle = 0
            min_safe_distance = 60
            for j in range(self.n_agents):
                if i != j:
                    other_pos = uav_positions[j]
                    dist = np.linalg.norm(other_pos - uav_pos)
                    if dist < min_safe_distance:
                        repel_vector = uav_pos - other_pos
                        safe_angle += np.arctan2(repel_vector[1], repel_vector[0]) * 0.3
            
            # Target selection
            distances = np.linalg.norm(user_positions - uav_pos, axis=1)
            closest_user_idx = np.argmin(distances)
            target = user_positions[closest_user_idx]
            delta = target - uav_pos
            target_angle = np.arctan2(delta[1], delta[0])
            
            theta = target_angle + safe_angle
            
            # ØªÙ†Ø¸ÛŒÙ… Ø¯Ù‚ÛŒÙ‚ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ§ØµÙ„Ù‡
            dist_to_target = distances[closest_user_idx]
            if dist_to_target < 100:
                v = v_base * 0.7
                f = f_base * 1.1
            else:
                v = v_base
                f = f_base
            
            actions.append(np.array([v, theta, f, o_base], dtype=np.float32))
        return actions

def run_single_experiment(policy_name, policy, env, n_episodes=50):
    """Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ© Ø¢Ø²Ù…Ø§ÛŒØ´ Ø¨Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµØ­ÛŒØ­ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§"""
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ø²Ù…Ø§ÛŒØ´: {policy_name}")
    print(f"{'='*70}")
    
    episode_rewards = []
    episode_delays = []
    episode_energies = []
    
    for ep in range(n_episodes):
        state = env.reset()
        done = False
        step_count = 0
        total_reward = 0
        
        # Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¯Ø± Ø§Ù¾ÛŒØ²ÙˆØ¯
        step_delays = []
        step_energies = []
        
        while not done and step_count < 100:
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª Ø¨Ø±Ø§ÛŒ Hybrid
            avg_delay = np.mean(step_delays) if step_delays else 0
            avg_energy = np.mean(step_energies) if step_energies else 0
            
            actions = policy.select_action(state, step_count, avg_delay, avg_energy)
            
            step_result = env.step(actions)
            
            # Unpack Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§
            if len(step_result) == 4:
                next_state, rewards, done, _ = step_result
            elif len(step_result) == 5:
                next_state, rewards, done, _, info = step_result
            else:
                raise ValueError(f"Ø®Ø±ÙˆØ¬ÛŒ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø² step(): {len(step_result)} Ù…Ù‚Ø¯Ø§Ø±")
            
            # âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµØ­ÛŒØ­ Energy Ø§Ø² state
            current_energies = next_state.get('energy', np.zeros(env.n_agents))
            total_energy_step = np.sum(current_energies)
            
            # âœ… Ù…Ø­Ø§Ø³Ø¨Ù‡ Delay Ø§Ø² distances (ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± Ø³Ø±Ø¹Øª Ù…ØªÙˆØ³Ø·)
            current_distances = next_state.get('distances', np.zeros(env.n_agents))
            current_velocities = next_state.get('uav_velocities', np.ones(env.n_agents) * 20)
            delays = current_distances / (current_velocities + 1e-6)  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± ØµÙØ±
            mean_delay_step = np.mean(delays)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§
            step_delays.append(mean_delay_step)
            step_energies.append(total_energy_step)
            
            total_reward += np.sum(rewards)
            state = next_state
            step_count += 1
        
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù¾ÛŒØ²ÙˆØ¯
        episode_rewards.append(total_reward)
        episode_delays.append(np.mean(step_delays))
        episode_energies.append(np.mean(step_energies))
        
        if (ep + 1) % 10 == 0:
            print(f"  Episode {ep+1}/{n_episodes} | "
                  f"Reward: {total_reward:.2e} | "
                  f"Delay: {episode_delays[-1]:.2f}s | "
                  f"Energy: {episode_energies[-1]:.2e}J")
    
    results = {
        'policy': policy_name,
        'mean_reward': float(np.mean(episode_rewards)),
        'std_reward': float(np.std(episode_rewards)),
        'mean_delay': float(np.mean(episode_delays)),
        'std_delay': float(np.std(episode_delays)),
        'mean_energy': float(np.mean(episode_energies)),
        'std_energy': float(np.std(episode_energies)),
        'all_rewards': [float(r) for r in episode_rewards],
        'all_delays': [float(d) for d in episode_delays],
        'all_energies': [float(e) for e in episode_energies]
    }
    
    print(f"\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬ {policy_name}:")
    print(f"  Reward : {results['mean_reward']:.2e} Â± {results['std_reward']:.2e}")
    print(f"  Delay  : {results['mean_delay']:.2f}s Â± {results['std_delay']:.2f}s")
    print(f"  Energy : {results['mean_energy']:.2e} Â± {results['std_energy']:.2e}J")
    
    return results

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--episodes', type=int, default=50)
    parser.add_argument('--n_agents', type=int, default=3)
    parser.add_argument('--output', type=str, default='results/obstacle_experiments_fixed.json')
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ğŸ”¬ Ø¢Ø²Ù…Ø§ÛŒØ´ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµØ­ÛŒØ­ Energy Ùˆ Delay")
    print("="*70)
    print(f"ØªØ¹Ø¯Ø§Ø¯ Ø§Ù¾ÛŒØ²ÙˆØ¯Ù‡Ø§: {args.episodes}")
    print(f"ØªØ¹Ø¯Ø§Ø¯ UAVÙ‡Ø§: {args.n_agents}")
    
    env = MultiUAVEnv(n_agents=args.n_agents)
    
    policies = {
        'Random': RandomPolicy(env),
        'Greedy': GreedyPolicy(env),
        'Obstacle-Aware': ObstacleAwarePolicy(env),
        'Hybrid': HybridPolicy(env)
    }
    
    all_results = {}
    
    for name, policy in policies.items():
        start_time = time.time()
        results = run_single_experiment(name, policy, env, args.episodes)
        elapsed = time.time() - start_time
        results['execution_time'] = elapsed
        all_results[name] = results
        print(f"â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {elapsed:.2f}s\n")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… Ù†ØªØ§ÛŒØ¬ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {output_path}")
    
    # Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡
    print("\n" + "="*70)
    print("ğŸ“Š Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†Ù‡Ø§ÛŒÛŒ")
    print("="*70)
    print(f"{'Policy':<20} {'Reward':<15} {'Delay (s)':<15} {'Energy (J)':<15}")
    print("-"*70)
    for name, res in all_results.items():
        print(f"{name:<20} {res['mean_reward']:<15.2e} "
              f"{res['mean_delay']:<15.2f} {res['mean_energy']:<15.2e}")

if __name__ == "__main__":
    main()
