"""
run_obstacle_experiments_hybrid.py
===================================
Ø¢Ø²Ù…Ø§ÛŒØ´ Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø´Ø§Ù…Ù„ Hybrid Policy
"""

import numpy as np
import sys
import os

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from core.env_multi import MultiUAVEnv

# ==================== POLICIES ====================

class RandomPolicy:
    """Ø³ÛŒØ§Ø³Øª ØªØµØ§Ø¯ÙÛŒ - Baseline"""
    def __init__(self, n_agents=3):
        self.n_agents = n_agents
    
    def select_action(self, state):
        actions = []
        for _ in range(self.n_agents):
            v = np.random.uniform(10.0, 25.0)      # Ø³Ø±Ø¹Øª
            theta = np.random.uniform(0, 2*np.pi)  # Ø²Ø§ÙˆÛŒÙ‡
            f = np.random.uniform(1e9, 3e9)        # ÙØ±Ú©Ø§Ù†Ø³ CPU
            o = np.random.uniform(0.3, 0.9)        # Ù†Ø³Ø¨Øª offload
            actions.append(np.array([v, theta, f, o], dtype=np.float32))
        return actions


class GreedyPolicy:
    """Ø³ÛŒØ§Ø³Øª Ø­Ø±ÛŒØµØ§Ù†Ù‡ - Ø­Ø±Ú©Øª Ø¨Ù‡ Ø³Ù…Øª Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ú©Ø§Ø±Ø¨Ø±"""
    def __init__(self, n_agents=3):
        self.n_agents = n_agents
    
    def select_action(self, state):
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§
        uav_positions = state['uav_positions']  # shape: (n_agents, 2)
        user_positions = state.get('user_positions', np.array([[50, 50]]))
        
        actions = []
        for i in range(self.n_agents):
            uav_pos = uav_positions[i]
            
            # ÛŒØ§ÙØªÙ† Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ú©Ø§Ø±Ø¨Ø±
            distances = np.linalg.norm(user_positions - uav_pos, axis=1)
            nearest_user = user_positions[np.argmin(distances)]
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ø§ÙˆÛŒÙ‡ Ø­Ø±Ú©Øª
            direction = nearest_user - uav_pos
            theta = np.arctan2(direction[1], direction[0])
            
            # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø«Ø§Ø¨Øª Ø¨Ù‡ÛŒÙ†Ù‡
            v = 20.0      # Ø³Ø±Ø¹Øª Ù…ØªÙˆØ³Ø·
            f = 2.0e9     # ÙØ±Ú©Ø§Ù†Ø³ CPU Ù…ØªÙˆØ³Ø·
            o = 0.7       # 70% offload
            
            actions.append(np.array([v, theta, f, o], dtype=np.float32))
        
        return actions


class ObstacleAwarePolicy:
    """Ø³ÛŒØ§Ø³Øª Ø¢Ú¯Ø§Ù‡ Ø§Ø² Ù…ÙˆØ§Ù†Ø¹ - Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ"""
    def __init__(self, n_agents=3):
        self.n_agents = n_agents
    
    def select_action(self, state):
        uav_positions = state['uav_positions']
        user_positions = state.get('user_positions', np.array([[50, 50]]))
        
        actions = []
        for i in range(self.n_agents):
            uav_pos = uav_positions[i]
            
            # ÛŒØ§ÙØªÙ† Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ú©Ø§Ø±Ø¨Ø±
            distances = np.linalg.norm(user_positions - uav_pos, axis=1)
            nearest_user = user_positions[np.argmin(distances)]
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ø§ÙˆÛŒÙ‡
            direction = nearest_user - uav_pos
            theta = np.arctan2(direction[1], direction[0])
            
            # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø­Ø§ÙØ¸Ù‡â€ŒÚ©Ø§Ø±Ø§Ù†Ù‡ (Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ delay)
            v = 15.0      # Ø³Ø±Ø¹Øª Ú©Ù…ØªØ± Ø¨Ø±Ø§ÛŒ Ù…Ø§Ù†ÙˆØ± Ø¨Ù‡ØªØ±
            f = 2.2e9     # ÙØ±Ú©Ø§Ù†Ø³ Ú©Ù…ÛŒ Ø¨Ø§Ù„Ø§ØªØ±
            o = 0.5       # ØªÙˆØ§Ø²Ù† 50-50
            
            actions.append(np.array([v, theta, f, o], dtype=np.float32))
        
        return actions


class HybridPolicy:
    """
    ğŸŒŸ Ø³ÛŒØ§Ø³Øª ØªØ±Ú©ÛŒØ¨ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯
    ========================
    ØªØ±Ú©ÛŒØ¨ Ø¨Ù‡ØªØ±ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Greedy (reward Ø¨Ø§Ù„Ø§) Ùˆ Obstacle-Aware (delay Ú©Ù…)
    
    Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ:
    - Ø§Ø² Ù†Ø§ÙˆØ¨Ø±ÛŒ Greedy Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ø­Ø±Ú©Øª Ø¨Ù‡ Ø³Ù…Øª Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ú©Ø§Ø±Ø¨Ø±)
    - Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ¶Ø¹ÛŒØª ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    - Ù‡Ø¯Ù: Ø¨Ø§Ù„Ø§Ù†Ø³ Ø¨ÛŒÙ† RewardØŒ Delay Ùˆ Energy
    """
    
    def __init__(self, n_agents=3):
        self.n_agents = n_agents
        self.greedy_policy = GreedyPolicy(n_agents)
        
        # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…
        self.base_velocity = 17.5      # Ø¨ÛŒÙ† Greedy (20) Ùˆ Obstacle-Aware (15)
        self.base_frequency = 2.3e9    # Ú©Ù…ÛŒ Ø¨Ø§Ù„Ø§ØªØ± Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ delay
        self.base_offload = 0.65       # Ø¨ÛŒÙ† 0.7 Ùˆ 0.5
    
    def select_action(self, state):
        # Ø§Ø¨ØªØ¯Ø§ Ø§Ø² Greedy Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¬Ù‡Øª Ø­Ø±Ú©Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        base_actions = self.greedy_policy.select_action(state)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯
        mean_delay = state.get('mean_delay', 3.0)
        energy_total = state.get('energy_total', 2e4)
        
        # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø±Ø§ÛŒØ·
        actions = []
        for i, base_action in enumerate(base_actions):
            theta = base_action[1]  # Ø­ÙØ¸ Ø²Ø§ÙˆÛŒÙ‡ Ø§Ø² Greedy
            
            # ğŸ¯ ØªÙ†Ø¸ÛŒÙ… Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ø³Ø±Ø¹Øª
            if mean_delay > 4.0:
                v = 15.0  # Ø³Ø±Ø¹Øª Ú©Ù… Ø¨Ø±Ø§ÛŒ Ù…Ø§Ù†ÙˆØ± Ø¨Ù‡ØªØ±
            elif mean_delay < 2.5:
                v = 20.0  # Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§ Ø§Ú¯Ø± delay Ø®ÛŒÙ„ÛŒ Ú©Ù… Ø§Ø³Øª
            else:
                v = self.base_velocity
            
            # âš¡ ØªÙ†Ø¸ÛŒÙ… Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© ÙØ±Ú©Ø§Ù†Ø³
            if energy_total > 3e4:
                f = 2.0e9  # Ú©Ø§Ù‡Ø´ Ø¨Ø±Ø§ÛŒ ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ Ø§Ù†Ø±Ú˜ÛŒ
            elif energy_total < 1.8e4:
                f = 2.5e9  # Ø§ÙØ²Ø§ÛŒØ´ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ delay Ø¨ÛŒØ´ØªØ±
            else:
                f = self.base_frequency
            
            # ğŸ”„ ØªÙ†Ø¸ÛŒÙ… Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© offload
            if mean_delay > 4.0:
                o = 0.5   # Ú©Ø§Ù‡Ø´ offload
            elif mean_delay < 2.5:
                o = 0.75  # Ø§ÙØ²Ø§ÛŒØ´ offload
            else:
                o = self.base_offload
            
            actions.append(np.array([v, theta, f, o], dtype=np.float32))
        
        return actions


# ==================== EXPERIMENT RUNNER ====================

def run_single_experiment(env, policy, n_episodes=50, max_steps=50):
    """Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ© Ø¢Ø²Ù…Ø§ÛŒØ´ Ø¨Ø§ ÛŒÚ© Ø³ÛŒØ§Ø³Øª"""
    
    total_rewards = []
    total_delays = []
    total_energies = []
    
    for ep in range(n_episodes):
        # ğŸ”§ FIX: reset() ÙÙ‚Ø· ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
        state = env.reset()
        ep_reward = 0
        ep_steps = 0
        
        for step in range(max_steps):
            # Ø§Ù†ØªØ®Ø§Ø¨ action
            actions = policy.select_action(state)
            
            # Ø§Ø¬Ø±Ø§ÛŒ action
            step_result = env.step(actions)
            
            # ğŸ”§ FIX: Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ø±Ú¯Ø´ØªÛŒ
            if len(step_result) == 5:
                next_state, rewards, dones, truncated, info = step_result
            elif len(step_result) == 4:
                next_state, rewards, dones, info = step_result
                truncated = False
            else:
                raise ValueError(f"Unexpected step() return: {len(step_result)} values")
            
            # Ø¬Ù…Ø¹ reward
            if isinstance(rewards, dict):
                ep_reward += sum(rewards.values())
            elif isinstance(rewards, (list, np.ndarray)):
                ep_reward += sum(rewards)
            else:
                ep_reward += rewards
            
            state = next_state
            ep_steps += 1
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø§ÛŒØ§Ù† episode
            if isinstance(dones, dict):
                if all(dones.values()):
                    break
            elif isinstance(dones, (list, np.ndarray)):
                if all(dones):
                    break
            elif dones:
                break
        
        # Ø«Ø¨Øª Ù†ØªØ§ÛŒØ¬
        total_rewards.append(ep_reward)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ delay Ùˆ energy Ø§Ø² state Ù†Ù‡Ø§ÛŒÛŒ
        delay = state.get('mean_delay', 0)
        energy = state.get('energy_total', 0)
        
        total_delays.append(delay)
        total_energies.append(energy)
        
        if (ep + 1) % 10 == 0:
            print(f"  Episode {ep+1}/{n_episodes} - "
                  f"Reward: {ep_reward:.2e}, "
                  f"Delay: {delay:.2f}, "
                  f"Energy: {energy:.2e}")
    
    return {
        'rewards': total_rewards,
        'delays': total_delays,
        'energies': total_energies,
        'mean_reward': np.mean(total_rewards),
        'std_reward': np.std(total_rewards),
        'mean_delay': np.mean(total_delays),
        'std_delay': np.std(total_delays),
        'mean_energy': np.mean(total_energies),
        'std_energy': np.std(total_energies)
    }


def main():
    print("="*70)
    print("ğŸš€ Ø¢Ø²Ù…Ø§ÛŒØ´ Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø´Ø§Ù…Ù„ Hybrid Policy")
    print("="*70)
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ø²Ù…Ø§ÛŒØ´
    n_uavs = 3
    n_episodes = 50
    max_steps = 50
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ÛŒØ·
    print(f"\nğŸ“¦ Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ÛŒØ· Ø¨Ø§ {n_uavs} UAV...")
    env = MultiUAVEnv(n_agents=n_uavs)
    
    # ØªØ¹Ø±ÛŒÙ Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§
    policies = {
        'Random': RandomPolicy(n_uavs),
        'Greedy': GreedyPolicy(n_uavs),
        'Obstacle-Aware': ObstacleAwarePolicy(n_uavs),
        'Hybrid': HybridPolicy(n_uavs)  # ğŸŒŸ Ø³ÛŒØ§Ø³Øª Ø¬Ø¯ÛŒØ¯!
    }
    
    # Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§
    results = []
    
    for name, policy in policies.items():
        print(f"\n{'='*70}")
        print(f"ğŸ§ª Ø¢Ø²Ù…Ø§ÛŒØ´ Ø³ÛŒØ§Ø³Øª: {name}")
        print(f"{'='*70}")
        
        result = run_single_experiment(env, policy, n_episodes, max_steps)
        result['policy'] = name
        results.append(result)
        
        print(f"\nğŸ“Š Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ {name}:")
        print(f"  Reward:  {result['mean_reward']:.2e} Â± {result['std_reward']:.2e}")
        print(f"  Delay:   {result['mean_delay']:.2f} Â± {result['std_delay']:.2f}")
        print(f"  Energy:  {result['mean_energy']:.2e} Â± {result['std_energy']:.2e}")
    
    # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
    print("\n" + "="*70)
    print("ğŸ“ˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ù‡Ù…Ù‡ Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§")
    print("="*70)
    print(f"{'Policy':<20} {'Reward':>15} {'Delay':>10} {'Energy':>15}")
    print("-"*70)
    
    for r in results:
        print(f"{r['policy']:<20} {r['mean_reward']:>15.2e} "
              f"{r['mean_delay']:>10.2f} {r['mean_energy']:>15.2e}")
    
    # ÛŒØ§ÙØªÙ† Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§
    print("\nğŸ† Ø¨Ø±Ù†Ø¯Ú¯Ø§Ù† Ø¯Ø± Ù‡Ø± Ù…Ø¹ÛŒØ§Ø±:")
    best_reward = max(results, key=lambda x: x['mean_reward'])
    best_delay = min(results, key=lambda x: x['mean_delay'])
    best_energy = min(results, key=lambda x: x['mean_energy'])
    
    print(f"  Reward:  {best_reward['policy']} ({best_reward['mean_reward']:.2e})")
    print(f"  Delay:   {best_delay['policy']} ({best_delay['mean_delay']:.2f})")
    print(f"  Energy:  {best_energy['policy']} ({best_energy['mean_energy']:.2e})")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Hybrid
    hybrid_result = next(r for r in results if r['policy'] == 'Hybrid')
    greedy_result = next(r for r in results if r['policy'] == 'Greedy')
    obstacle_result = next(r for r in results if r['policy'] == 'Obstacle-Aware')
    
    print("\nğŸŒŸ ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯ Hybrid:")
    
    reward_improvement = ((hybrid_result['mean_reward'] - greedy_result['mean_reward']) 
                         / greedy_result['mean_reward'] * 100)
    delay_improvement = ((obstacle_result['mean_delay'] - hybrid_result['mean_delay']) 
                        / obstacle_result['mean_delay'] * 100)
    energy_vs_greedy = ((hybrid_result['mean_energy'] - greedy_result['mean_energy']) 
                       / greedy_result['mean_energy'] * 100)
    
    print(f"  Reward vs Greedy:        {reward_improvement:+.1f}%")
    print(f"  Delay vs Obstacle-Aware: {delay_improvement:+.1f}%")
    print(f"  Energy vs Greedy:        {energy_vs_greedy:+.1f}%")
    
    return results


if __name__ == "__main__":
    results = main()
    print("\nâœ… Ø¢Ø²Ù…Ø§ÛŒØ´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø§ÛŒØ§Ù† ÛŒØ§ÙØª!")
