"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ablation
Ù…Ø³ÛŒØ±: scripts/run_ablation_study.py
"""

import sys
import os
from pathlib import Path

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ù¾Ø±ÙˆÚ˜Ù‡
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import numpy as np
import json
import time
from datetime import datetime
import torch

# Import Ù…Ø­ÛŒØ·
from pettingzoo.mpe import simple_tag_v3

# Import variants
from core.evaluation.ablation_variants import (
    FullMADDPGVariant,
    NoGATVariant,
    NoTemporalVariant,
    DecentralizedVariant,
    SimplerArchVariant
)


class AblationStudyRunner:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ablation"""
    
    def __init__(self, results_dir="results/ablation"):
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # Ù…Ø­ÛŒØ·
        self.env = None
        self.num_agents = 3
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´
        self.train_config = {
            "num_episodes": 500,
            "max_steps_per_episode": 100,
            "eval_interval": 50,
            "eval_episodes": 20,
            "gamma": 0.95,
            "tau": 0.001,
            "batch_size": 64,
            "actor_lr": 1e-4,
            "critic_lr": 1e-3,
            "buffer_size": 100000
        }
        
        # Variants Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´
        self.variants = {
            "full_model": FullMADDPGVariant,
            "no_gat": NoGATVariant,
            "no_temporal": NoTemporalVariant,
            "decentralized": DecentralizedVariant,
            "simpler_arch": SimplerArchVariant
        }
        
        # Ù†ØªØ§ÛŒØ¬
        self.results = {}
    
    def create_env(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ÛŒØ·"""
        env = simple_tag_v3.parallel_env(
            num_good=1,
            num_adversaries=2,
            num_obstacles=2,
            max_cycles=self.train_config["max_steps_per_episode"],
            continuous_actions=True
        )
        return env
    
    def get_obs_action_dims(self):
        """âœ… Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚ÛŒÙ‚ Ø§Ø¨Ø¹Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Agents"""
        env = self.create_env()
        obs_dict, _ = env.reset()
        
        print("\n" + "="*60)
        print("ğŸ” Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø§Ø¨Ø¹Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Agents...")
        print("="*60)
        
        # Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø¨Ø¹Ø§Ø¯ Ù‡Ø± Agent
        obs_dims = {}
        action_dims = {}
        
        # âœ… Ø§Ø¬Ø±Ø§ÛŒ Ú†Ù†Ø¯ step Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… observations
        all_obs_samples = {agent: [] for agent in env.agents}
        
        for step_idx in range(10):
            actions = {agent: env.action_space(agent).sample() 
                      for agent in env.agents}
            obs_dict, _, terminations, truncations, _ = env.step(actions)
            
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§
            for agent in env.agents:
                if agent in obs_dict:
                    obs = obs_dict[agent]
                    obs_dim = obs.shape[0] if hasattr(obs, 'shape') else len(obs)
                    all_obs_samples[agent].append(obs_dim)
            
            # Ø§Ú¯Ø± Ù‡Ù…Ù‡ agents ØªÙ…Ø§Ù… Ø´Ø¯Ù†Ø¯ØŒ Ù…Ø­ÛŒØ· Ø±Ø§ reset Ú©Ù†
            if not env.agents:
                env.reset()
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø¨Ø¹Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Agent
        for agent in all_obs_samples:
            if all_obs_samples[agent]:
                obs_dims[agent] = max(all_obs_samples[agent])
                action_space = env.action_space(agent)
                action_dims[agent] = action_space.shape[0]
                
                print(f"\n   Agent: {agent}")
                print(f"      Obs dims Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø´Ø¯Ù‡: {all_obs_samples[agent]}")
                print(f"      Max Obs dim: {obs_dims[agent]}")
                print(f"      Action dim: {action_dims[agent]}")
                print(f"      Action range: [{action_space.low[0]:.2f}, {action_space.high[0]:.2f}]")
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡
        obs_dim = max(obs_dims.values()) if obs_dims else 14
        action_dim = max(action_dims.values()) if action_dims else 5
        
        env.close()
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Ø§Ø¨Ø¹Ø§Ø¯ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡:")
        print(f"   - Max Observation dim: {obs_dim}")
        print(f"   - Max Action dim: {action_dim}")
        print(f"   - Number of agents: {len(obs_dims)}")
        print(f"{'='*60}\n")
        
        return obs_dim, action_dim
    
    def normalize_observation(self, obs, target_dim):
        """âœ… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ ØªÙ†Ø¸ÛŒÙ… Ø§Ø¨Ø¹Ø§Ø¯ observation"""
        current_dim = len(obs)
        
        if current_dim < target_dim:
            # Pad Ø¨Ø§ ØµÙØ±
            obs = np.pad(obs, (0, target_dim - current_dim), mode='constant', constant_values=0)
        elif current_dim > target_dim:
            # Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù†
            obs = obs[:target_dim]
        
        return obs
    
    def normalize_action(self, action):
        """âœ… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ù‚ÛŒÙ‚ action Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ [0, 1]"""
        # Clip Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ [0, 1]
        action = np.clip(action, 0.0, 1.0)
        
        # Ú¯Ø±Ø¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ 6 Ø±Ù‚Ù… Ø§Ø¹Ø´Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ±
        action = np.round(action, decimals=6)
        
        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø­Ø¯ÛŒ Ø±Ø§ Ú©Ù…ÛŒ Ø¬Ø§Ø¨Ø¬Ø§ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        epsilon = 1e-6
        action = np.where(action < epsilon, epsilon, action)
        action = np.where(action > (1.0 - epsilon), 1.0 - epsilon, action)
        
        return action
    
    def train_variant(self, variant_name, variant_class, obs_dim, action_dim):
        """Ø¢Ù…ÙˆØ²Ø´ ÛŒÚ© variant"""
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´: {variant_name}")
        print(f"{'='*60}\n")
        
        start_time = time.time()
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„
        model = variant_class(
            obs_dim=obs_dim,
            action_dim=action_dim,
            num_agents=self.num_agents,
            **self.train_config
        )
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ÛŒØ·
        env = self.create_env()
        
        # Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
        variant_dir = self.results_dir / variant_name
        variant_dir.mkdir(exist_ok=True)
        
        # Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
        episode_rewards = []
        eval_rewards = []
        best_eval_reward = -float('inf')
        
        # Ø´Ù…Ø§Ø±Ø´ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ clipping
        clipping_warnings = 0
        
        try:
            for episode in range(self.train_config["num_episodes"]):
                # Reset Ù…Ø­ÛŒØ·
                obs_dict, info = env.reset()
                
                episode_reward = 0
                step = 0
                
                while env.agents:
                    # Ø§Ù†ØªØ®Ø§Ø¨ actions Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ agents
                    actions = {}
                    for agent_id in env.agents:
                        obs = obs_dict[agent_id]
                        
                        # âœ… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ observation
                        obs = self.normalize_observation(obs, obs_dim)
                        
                        # Ø¯Ø±ÛŒØ§ÙØª action Ø§Ø² Ù…Ø¯Ù„
                        action = model.select_action(agent_id, obs, add_noise=True)
                        
                        # âœ… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ù‚ÛŒÙ‚ action
                        action = self.normalize_action(action)
                        
                        actions[agent_id] = action
                    
                    # Ø§Ø¬Ø±Ø§ÛŒ step
                    next_obs_dict, rewards, terminations, truncations, infos = env.step(actions)
                    
                    # Ø°Ø®ÛŒØ±Ù‡ transitions
                    for agent_id in env.agents:
                        if agent_id in obs_dict and agent_id in next_obs_dict:
                            obs = obs_dict[agent_id]
                            next_obs = next_obs_dict[agent_id]
                            
                            # âœ… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ observations
                            obs = self.normalize_observation(obs, obs_dim)
                            next_obs = self.normalize_observation(next_obs, obs_dim)
                            
                            model.store_transition(
                                agent_id=agent_id,
                                state=obs,
                                action=actions[agent_id],
                                reward=rewards[agent_id],
                                next_state=next_obs,
                                done=terminations[agent_id] or truncations[agent_id]
                            )
                    
                    # Ø¢Ù¾Ø¯ÛŒØª Ù¾Ø§Ø¯Ø§Ø´
                    episode_reward += sum(rewards.values())
                    
                    # Ø¢Ù¾Ø¯ÛŒØª Ù…Ø¯Ù„
                    model.update()
                    
                    # Ø¢Ù¾Ø¯ÛŒØª state
                    obs_dict = next_obs_dict
                    step += 1
                
                episode_rewards.append(episode_reward)
                
                # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª
                if (episode + 1) % 10 == 0:
                    avg_reward = np.mean(episode_rewards[-10:])
                    print(f"Episode {episode+1}/{self.train_config['num_episodes']} | "
                          f"Avg Reward: {avg_reward:.2f}")
                
                # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
                if (episode + 1) % self.train_config["eval_interval"] == 0:
                    eval_reward = self.evaluate_variant(model, env, obs_dim)
                    eval_rewards.append(eval_reward)
                    
                    print(f"ğŸ“Š Evaluation at episode {episode+1}: {eval_reward:.2f}")
                    
                    # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
                    if eval_reward > best_eval_reward:
                        best_eval_reward = eval_reward
                        model.save(variant_dir / "best_model.pt")
                        print(f"âœ… New best model saved: {eval_reward:.2f}")
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ù…Ø§Ù†
            training_time = (time.time() - start_time) / 60
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
            results = {
                "variant": variant_name,
                "best_eval_reward": float(best_eval_reward),
                "final_avg_reward": float(np.mean(episode_rewards[-100:])),
                "training_time_minutes": float(training_time),
                "episode_rewards": [float(r) for r in episode_rewards],
                "eval_rewards": [float(r) for r in eval_rewards],
                "config": {
                    "obs_dim": obs_dim,
                    "action_dim": action_dim,
                    "num_agents": self.num_agents
                }
            }
            
            with open(variant_dir / "training_results.json", "w") as f:
                json.dump(results, f, indent=2)
            
            self.results[variant_name] = results
            
            print(f"\nâœ… Ø¢Ù…ÙˆØ²Ø´ {variant_name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
            print(f"   Best Reward: {best_eval_reward:.2f}")
            print(f"   Final Avg: {results['final_avg_reward']:.2f}")
            print(f"   Time: {training_time:.1f} min")
            
        except Exception as e:
            print(f"\nâŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ {variant_name}: {str(e)}")
            import traceback
            traceback.print_exc()
            
            self.results[variant_name] = {
                "variant": variant_name,
                "error": str(e),
                "status": "failed"
            }
        
        finally:
            env.close()
    
    def evaluate_variant(self, model, env, obs_dim):
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„"""
        eval_rewards = []
        
        for _ in range(self.train_config["eval_episodes"]):
            obs_dict, _ = env.reset()
            episode_reward = 0
            
            while env.agents:
                actions = {}
                for agent_id in env.agents:
                    obs = obs_dict[agent_id]
                    
                    # âœ… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ observation
                    obs = self.normalize_observation(obs, obs_dim)
                    
                    # Ø¯Ø±ÛŒØ§ÙØª action Ø¨Ø¯ÙˆÙ† Ù†ÙˆÛŒØ²
                    action = model.select_action(agent_id, obs, add_noise=False)
                    
                    # âœ… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ù‚ÛŒÙ‚ action
                    action = self.normalize_action(action)
                    
                    actions[agent_id] = action
                
                next_obs_dict, rewards, terminations, truncations, _ = env.step(actions)
                
                episode_reward += sum(rewards.values())
                obs_dict = next_obs_dict
            
            eval_rewards.append(episode_reward)
        
        return np.mean(eval_rewards)
    
    def run_all_variants(self):
        """Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù… variants"""
        
        print("\n" + "="*60)
        print("ğŸ”¬ Ø´Ø±ÙˆØ¹ Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ablation")
        print("="*60)
        
        # âœ… Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø¨Ø¹Ø§Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² Ù…Ø­ÛŒØ·
        obs_dim, action_dim = self.get_obs_action_dims()
        
        print(f"\nğŸ“Š ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´:")
        print(f"   Observation dim: {obs_dim}")
        print(f"   Action dim: {action_dim}")
        print(f"   Number of agents: {self.num_agents}")
        print(f"   Episodes: {self.train_config['num_episodes']}")
        print(f"   Eval interval: {self.train_config['eval_interval']}")
        
        # Ø¢Ù…ÙˆØ²Ø´ Ù‡Ø± variant
        for variant_name, variant_class in self.variants.items():
            self.train_variant(variant_name, variant_class, obs_dim, action_dim)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø®Ù„Ø§ØµÙ‡
        self.save_summary()
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
        self.print_summary()
    
    def save_summary(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬"""
        summary = {
            "timestamp": datetime.now().isoformat(),
            "config": self.train_config,
            "results": self.results
        }
        
        with open(self.results_dir / "ablation_summary.json", "w") as f:
            json.dump(summary, f, indent=2)
    
    def print_summary(self):
        """Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬"""
        print("\n" + "="*60)
        print("âœ… ØªÙ…Ø§Ù… Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ÛŒ Ablation ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")
        print("="*60)
        
        print("\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬ Ablation:\n")
        print(f"{'Variant':<20} {'Best Reward':<15} {'Final Avg':<15} {'Time (min)':<12}")
        print("-" * 62)
        
        for variant_name, result in self.results.items():
            if "error" in result:
                print(f"{variant_name:<20} {'FAILED':<15} {'-':<15} {'-':<12}")
            else:
                print(f"{variant_name:<20} "
                      f"{result['best_eval_reward']:<15.2f} "
                      f"{result['final_avg_reward']:<15.2f} "
                      f"{result['training_time_minutes']:<12.1f}")
        
        print("\nğŸ’¾ Ù†ØªØ§ÛŒØ¬ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¯Ø±:", self.results_dir)


def main():
    """Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ"""
    runner = AblationStudyRunner(results_dir="results/ablation")
    runner.run_all_variants()


if __name__ == "__main__":
    main()
