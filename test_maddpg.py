# test_maddpg.py

import torch
import numpy as np
from models.actor_critic.maddpg_agent import MADDPGAgent

def test_maddpg_agent():
    print("\n" + "="*80)
    print("üß™ Testing MADDPG Agent")
    print("="*80)
    
    # Initialize agent
    agent = MADDPGAgent(
        state_dim=114,
        offload_dim=4,
        continuous_dim=7,
        action_dim=11
    )
    
    print(f"\n‚úÖ Agent initialized on device: {agent.device}")
    
    # Test action selection
    print("\n1Ô∏è‚É£ Testing action selection...")
    state = np.random.randn(114)
    action = agent.select_action(state, add_noise=True)
    
    print(f"  ‚úÖ Action keys: {action.keys()}")
    print(f"  ‚úÖ Offload layer: {action['offload']}")
    print(f"  ‚úÖ Bandwidth: {action['bandwidth']} (sum={action['bandwidth'].sum():.4f})")
    print(f"  ‚úÖ CPU: {action['cpu']:.4f}")
    print(f"  ‚úÖ Movement: {action['move']}")
    
    # Test replay buffer
    print("\n2Ô∏è‚É£ Testing replay buffer...")
    for i in range(100):
        state = np.random.randn(114)
        action = agent.select_action(state, add_noise=False)
        reward = np.random.randn()
        next_state = np.random.randn(114)
        done = i % 20 == 0
        
        agent.replay_buffer.push(state, action, reward, next_state, done)
    
    print(f"  ‚úÖ Buffer size: {len(agent.replay_buffer)}")
    
    # Test network update
    print("\n3Ô∏è‚É£ Testing network update...")
    losses = agent.update(batch_size=32)
    
    if losses:
        print(f"  ‚úÖ Critic loss: {losses['critic_loss']:.4f}")
        print(f"  ‚úÖ Actor loss: {losses['actor_loss']:.4f}")
        print(f"  ‚úÖ Q-value: {losses['q_value']:.4f}")
    
    # Test save/load
    print("\n4Ô∏è‚É£ Testing save/load...")
    agent.save('test_agent.pth')
    agent.load('test_agent.pth')
    
    print("\n" + "="*80)
    print("‚úÖ All MADDPG Agent Tests Passed!")
    print("="*80 + "\n")

if __name__ == "__main__":
    test_maddpg_agent()
