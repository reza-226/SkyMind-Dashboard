"""
test_maddpg.py
==============
Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ MADDPG Ø¨Ø§ ØªØ¨Ø¯ÛŒÙ„ state_dict
"""

import torch
import numpy as np
from core.env_multi import MultiUAVEnv
from agents.agent_maddpg_multi import MADDPG_Agent
from tqdm import tqdm
import json
import os

def convert_old_state_dict(old_state_dict):
    """
    ØªØ¨Ø¯ÛŒÙ„ state_dict Ù‚Ø¯ÛŒÙ…ÛŒ (fc1, fc2, fc3) Ø¨Ù‡ ÙØ±Ù…Øª Ø¬Ø¯ÛŒØ¯ (net.0, net.2, net.4)
    
    Ù‚Ø¯ÛŒÙ…ÛŒ:
        fc1.weight, fc1.bias
        fc2.weight, fc2.bias
        fc3.weight, fc3.bias
    
    Ø¬Ø¯ÛŒØ¯:
        net.0.weight, net.0.bias
        net.2.weight, net.2.bias
        net.4.weight, net.4.bias
    """
    new_state_dict = {}
    mapping = {
        'fc1': 'net.0',
        'fc2': 'net.2',
        'fc3': 'net.4'
    }
    
    for old_key, value in old_state_dict.items():
        # Ù…Ø«Ù„Ø§Ù‹: fc1.weight -> net.0.weight
        for old_name, new_name in mapping.items():
            if old_key.startswith(old_name):
                new_key = old_key.replace(old_name, new_name)
                new_state_dict[new_key] = value
                break
    
    return new_state_dict


def test_trained_model(num_episodes=100, save_results=True):
    """
    Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ Ø¨Ø¯ÙˆÙ† exploration
    """
    print("="*70)
    print("ğŸ§ª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ MADDPG Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡")
    print("="*70)
    
    # 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ÛŒØ·
    n_agents = 3
    n_users = 5
    env = MultiUAVEnv(n_agents=n_agents, n_users=n_users)
    print(f"âœ“ Ù…Ø­ÛŒØ· Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ (Agents={n_agents}, Users={n_users})")
    
    # 2. Ø³Ø§Ø®Øª Agent
    state_dim = 6
    action_dim = 4
    
    agent = MADDPG_Agent(
        state_dim=state_dim,
        action_dim=action_dim,
        n_agents=n_agents,
        lr=1e-4,
        gamma=0.99,
        tau=0.01,
        device='cpu'
    )
    print("âœ“ Agent Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯")
    
    # 3. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ø§ ØªØ¨Ø¯ÛŒÙ„ state_dict
    device = torch.device("cpu")
    actor_path = 'models/actor_agent0.pt'
    
    if not os.path.exists(actor_path):
        print(f"âŒ Ø®Ø·Ø§: ÙØ§ÛŒÙ„ {actor_path} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")
        return None
    
    try:
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ state_dict Ù‚Ø¯ÛŒÙ…ÛŒ
        old_state_dict = torch.load(actor_path, map_location=device)
        
        print("ğŸ“‹ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡:")
        for key in old_state_dict.keys():
            print(f"  - {key}")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ Ù…Ø¹Ù…Ø§Ø±ÛŒ
        if 'fc1.weight' in old_state_dict:
            print("\nğŸ”§ ØªØ¨Ø¯ÛŒÙ„ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ø¨Ù‡ Ø¬Ø¯ÛŒØ¯...")
            new_state_dict = convert_old_state_dict(old_state_dict)
            agent.actor.load_state_dict(new_state_dict)
        else:
            # Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø¬Ø¯ÛŒØ¯ Ø§Ø³Øª
            agent.actor.load_state_dict(old_state_dict)
        
        agent.actor.eval()
        print(f"âœ“ Actor Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ Ø§Ø² {actor_path}")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Actor: {e}")
        return None
    
    print("\nğŸ¯ Ø´Ø±ÙˆØ¹ ØªØ³Øª...")
    print("-"*70)
    
    # 4. Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬
    test_rewards = []
    test_energies = []
    test_delays = []
    episode_details = []
    
    # 5. Ø­Ù„Ù‚Ù‡ ØªØ³Øª
    for ep in tqdm(range(num_episodes), desc="Testing"):
        state = env.reset()
        episode_reward = 0
        episode_energy = 0
        episode_delay = 0
        step_count = 0
        
        for step in range(200):
            # Ø¯Ø±ÛŒØ§ÙØª Ø§Ú©Ø´Ù† Ø¨Ø¯ÙˆÙ† noise
            with torch.no_grad():
                actions = []
                for i in range(n_agents):
                    action = agent.act(state[i], noise_scale=0.0)
                    actions.append(action)
                actions = np.array(actions)
            
            # Ø§Ø¬Ø±Ø§ÛŒ Ø§Ú©Ø´Ù†
            next_state, reward, done, info = env.step(actions)
            
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ metrics
            episode_reward += reward
            episode_energy += info.get('energy_total', 0)
            episode_delay += info.get('mean_delay', 0)
            
            state = next_state
            step_count = step + 1
            
            if done:
                break
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        test_rewards.append(episode_reward)
        test_energies.append(episode_energy)
        test_delays.append(episode_delay)
        
        episode_details.append({
            'episode': ep + 1,
            'reward': float(episode_reward),
            'energy': float(episode_energy),
            'delay': float(episode_delay),
            'steps': step_count
        })
        
        # Ù†Ù…Ø§ÛŒØ´ Ù‡Ø± 10 Ø§Ù¾ÛŒØ²ÙˆØ¯
        if (ep + 1) % 10 == 0:
            print(f"\nEpisode {ep+1:3d}/{num_episodes} | "
                  f"Reward: {episode_reward:7.2f} | "
                  f"Energy: {episode_energy:.2e} | "
                  f"Delay: {episode_delay:.6f}")
    
    # 6. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§
    results = {
        'rewards': test_rewards,
        'energies': test_energies,
        'delays': test_delays,
        'episode_details': episode_details,
        'statistics': {
            'reward_mean': float(np.mean(test_rewards)),
            'reward_std': float(np.std(test_rewards)),
            'reward_max': float(np.max(test_rewards)),
            'reward_min': float(np.min(test_rewards)),
            'energy_mean': float(np.mean(test_energies)),
            'energy_std': float(np.std(test_energies)),
            'delay_mean': float(np.mean(test_delays)),
            'delay_std': float(np.std(test_delays))
        }
    }
    
    # 7. Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
    print("\n" + "="*70)
    print("ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ")
    print("="*70)
    print(f"{'Metric':<20} {'Mean':<15} {'Std':<15} {'Min/Max':<20}")
    print("-"*70)
    print(f"{'Reward':<20} {results['statistics']['reward_mean']:>10.2f}    "
          f"{results['statistics']['reward_std']:>10.2f}    "
          f"{results['statistics']['reward_min']:>7.2f} / {results['statistics']['reward_max']:<7.2f}")
    print(f"{'Energy':<20} {results['statistics']['energy_mean']:>10.2e}    "
          f"{results['statistics']['energy_std']:>10.2e}")
    print(f"{'Delay':<20} {results['statistics']['delay_mean']:>10.6f}    "
          f"{results['statistics']['delay_std']:>10.6f}")
    print("="*70)
    
    # 8. Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    if save_results:
        os.makedirs('results', exist_ok=True)
        
        np.savez('results/test_results.npz',
                 rewards=test_rewards,
                 energies=test_energies,
                 delays=test_delays)
        print("âœ… ÙØ§ÛŒÙ„ NumPy Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: results/test_results.npz")
        
        with open('results/test_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print("âœ… ÙØ§ÛŒÙ„ JSON Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: results/test_results.json")
    
    return results


if __name__ == "__main__":
    results = test_trained_model(num_episodes=100)
    
    if results is not None:
        print("\nâœ… ØªØ³Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
        print(f"ğŸ“ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± Ù¾ÙˆØ´Ù‡ results/ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")
    else:
        print("\nâŒ ØªØ³Øª Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯!")
