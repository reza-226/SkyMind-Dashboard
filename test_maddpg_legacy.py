"""
test_maddpg_legacy.py (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ - Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯!)
=====================================================
Fix: Ø§Ø³ØªØ®Ø±Ø§Ø¬ state Ø§Ø² Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¹Ø§Ù…Ù„
"""

import torch
import torch.nn as nn
import numpy as np
from core.env_multi import MultiUAVEnv
from tqdm import tqdm
import json
import os


class ActorLegacy(nn.Module):
    """Ù…Ø¹Ù…Ø§Ø±ÛŒ Actor Legacy"""
    def __init__(self, state_dim, action_dim, n_agents, hidden_dim=128):
        super(ActorLegacy, self).__init__()
        input_dim = state_dim * 2
        self.fc1 = nn.Linear(input_dim, 256)
        self.fc2 = nn.Linear(256, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        self.relu = nn.ReLU()
        self.tanh = nn.Tanh()
    
    def forward(self, state):
        x = self.relu(self.fc1(state))
        x = self.relu(self.fc2(x))
        action = self.tanh(self.fc3(x))
        return action


def extract_agent_state(state_dict, agent_idx, state_dim=6):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ state Ø¹Ø§Ù…Ù„ Ø®Ø§Øµ Ø§Ø² Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù…Ø­ÛŒØ·
    
    state_dim = 6 Ø´Ø§Ù…Ù„:
    [pos_x, pos_y, velocity, angle, energy, distance]
    """
    uav_pos = state_dict['uav_positions'][agent_idx]  # (2,)
    uav_vel = state_dict['uav_velocities'][agent_idx]  # scalar
    uav_angle = state_dict['uav_angles'][agent_idx]    # scalar
    energy = state_dict['energy'][agent_idx]           # scalar
    distance = state_dict['distances'][agent_idx]      # scalar
    
    # ØªØ±Ú©ÛŒØ¨ Ø¨Ù‡ ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± 6 Ø¨Ø¹Ø¯ÛŒ
    agent_state = np.concatenate([
        uav_pos,                    # [0:2]
        [uav_vel],                  # [2]
        [uav_angle],                # [3]
        [energy],                   # [4]
        [distance]                  # [5]
    ])
    
    return agent_state


def prepare_legacy_state(state, state_dim=6):
    """ØªØ¨Ø¯ÛŒÙ„ state Ø¨Ù‡ ÙØ±Ù…Øª (1 x 12)"""
    if isinstance(state, np.ndarray):
        state = torch.FloatTensor(state)
    
    if state.dim() == 1:
        state = state.unsqueeze(0)
    
    # Ø¯Ùˆ Ø¨Ø±Ø§Ø¨Ø± Ú©Ø±Ø¯Ù†: (1 x 6) -> (1 x 12)
    state_doubled = torch.cat([state, state], dim=-1)
    return state_doubled


def test_trained_model_legacy(num_episodes=100, save_results=True):
    """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø¨Ø§ Ù…Ø¹Ù…Ø§Ø±ÛŒ Legacy"""
    print("="*70)
    print("ğŸ§ª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ MADDPG (Legacy Architecture)")
    print("="*70)
    
    # 1. Ù…Ø­ÛŒØ·
    n_agents = 3
    n_users = 5
    env = MultiUAVEnv(n_agents=n_agents, n_users=n_users)
    print(f"âœ“ Ù…Ø­ÛŒØ· Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ (Agents={n_agents}, Users={n_users})")
    
    # 2. Actor
    state_dim = 6
    action_dim = 4
    hidden_dim = 128
    
    actor = ActorLegacy(state_dim, action_dim, n_agents, hidden_dim)
    print("âœ“ Actor Legacy Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯")
    
    # 3. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§
    device = torch.device("cpu")
    actor_path = 'models/actor_agent0.pt'
    
    if not os.path.exists(actor_path):
        print(f"âŒ Ø®Ø·Ø§: ÙØ§ÛŒÙ„ {actor_path} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")
        return None
    
    try:
        state_dict = torch.load(actor_path, map_location=device)
        actor.load_state_dict(state_dict)
        actor.eval()
        print(f"âœ… Actor Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ Ø§Ø² {actor_path}")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ: {e}")
        return None
    
    print("\nğŸ¯ Ø´Ø±ÙˆØ¹ ØªØ³Øª...")
    print("-"*70)
    
    # 4. Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    test_rewards = []
    test_energies = []
    test_delays = []
    episode_details = []
    
    # 5. Ø­Ù„Ù‚Ù‡ ØªØ³Øª
    for ep in tqdm(range(num_episodes), desc="Testing"):
        state_dict = env.reset()  # Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø³Øª!
        
        episode_reward = 0
        episode_energy = 0
        episode_delay = 0
        step_count = 0
        
        for step in range(200):
            with torch.no_grad():
                actions = []
                
                # ğŸ”§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ state Ø§Ø² Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¹Ø§Ù…Ù„
                for i in range(n_agents):
                    agent_state = extract_agent_state(state_dict, i, state_dim)
                    state_legacy = prepare_legacy_state(agent_state, state_dim)
                    action = actor(state_legacy).cpu().numpy()
                    
                    if action.ndim > 1:
                        action = action[0]
                    
                    actions.append(action)
                
                actions = np.array(actions)
            
            # Ø§Ø¬Ø±Ø§ÛŒ Ø§Ú©Ø´Ù†
            next_state_dict, reward, done, info = env.step(actions)
            
            # ğŸ”§ reward Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¢Ø±Ø§ÛŒÙ‡ Ø¨Ø§Ø´Ø¯
            if isinstance(reward, np.ndarray):
                reward = reward.sum()
            
            episode_reward += reward
            episode_energy += info.get('energy_total', 0)
            episode_delay += info.get('mean_delay', 0)
            
            state_dict = next_state_dict
            step_count = step + 1
            
            if done:
                break
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        test_rewards.append(episode_reward)
        test_energies.append(episode_energy)
        test_delays.append(episode_delay)
        
        episode_details.append({
            'episode': ep + 1,
            'reward': float(episode_reward),
            'energy': float(episode_energy),
            'delay': float(episode_delay),
            'steps': step_count
        })
    
    # 6. Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
    print("\n" + "="*70)
    print("ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ")
    print("="*70)
    
    mean_reward = np.mean(test_rewards)
    mean_energy = np.mean(test_energies)
    mean_delay = np.mean(test_delays)
    
    print(f"ğŸ“ˆ Mean Reward:  {mean_reward:>12.4f}  (Â±{np.std(test_rewards):.4f})")
    print(f"âš¡ Mean Energy:  {mean_energy:>12.4f}  (Â±{np.std(test_energies):.4f})")
    print(f"â±ï¸  Mean Delay:   {mean_delay:>12.4f}  (Â±{np.std(test_delays):.4f})")
    
    # 7. Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    if save_results:
        results = {
            'summary': {
                'num_episodes': num_episodes,
                'mean_reward': float(mean_reward),
                'std_reward': float(np.std(test_rewards)),
                'mean_energy': float(mean_energy),
                'std_energy': float(np.std(test_energies)),
                'mean_delay': float(mean_delay),
                'std_delay': float(np.std(test_delays))
            },
            'episodes': episode_details
        }
        
        # JSON
        json_path = 'results/test_results_legacy.json'
        os.makedirs('results', exist_ok=True)
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ Ù†ØªØ§ÛŒØ¬ JSON Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {json_path}")
        
        # NPZ
        npz_path = 'results/test_results_legacy.npz'
        np.savez(
            npz_path,
            rewards=test_rewards,
            energies=test_energies,
            delays=test_delays
        )
        print(f"ğŸ’¾ Ù†ØªØ§ÛŒØ¬ NPZ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {npz_path}")
    
    print("\nâœ… ØªØ³Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!")
    return results


if __name__ == "__main__":
    results = test_trained_model_legacy(num_episodes=100, save_results=True)
