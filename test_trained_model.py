# test_trained_model.py (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ - Robust Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ config)
"""
ØªØ³Øª Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø¨Ø§ MADDPG
âœ… Ø³Ø§Ø²Ú¯Ø§Ø± Ú©Ø§Ù…Ù„ Ø¨Ø§ train_4layer_3level.py
âœ… Ù…Ù‚Ø§ÙˆÙ… Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù config
"""
import torch
import numpy as np
import sys
import os
from pathlib import Path
import json

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ±Ù‡Ø§
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from environments.uav_mec_env import UAVMECEnvironment
from models.actor_critic.maddpg_agent import MADDPGAgent

def test_model(model_path, num_episodes=10, render=False):
    """
    ØªØ³Øª Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡
    
    Args:
        model_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ù…Ø¯Ù„
        num_episodes: ØªØ¹Ø¯Ø§Ø¯ Ø§Ù¾ÛŒØ²ÙˆØ¯Ù‡Ø§ÛŒ ØªØ³Øª
        render: Ù†Ù…Ø§ÛŒØ´ Ù…Ø­ÛŒØ· (Ø§Ú¯Ø± Ù…Ù…Ú©Ù† Ø¨Ø§Ø´Ø¯)
    """
    print(f"\n{'='*70}")
    print(f"ðŸ§ª Testing Trained MADDPG Model")
    print(f"{'='*70}")
    print(f"ðŸ“ Model: {model_path}")
    print(f"ðŸŽ² Episodes: {num_episodes}")
    print(f"{'='*70}\n")
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ config Ø§Ø² Ù¾ÙˆØ´Ù‡ Ù…Ø¯Ù„
    model_dir = Path(model_path).parent
    config_path = model_dir / 'config.json'
    
    env_config = None
    state_dim = None
    action_dim = None
    
    if config_path.exists():
        try:
            with open(config_path, 'r') as f:
                config = json.load(f)
            print(f"âœ… Loaded config from: {config_path}")
            print(f"ðŸ“‹ Config keys: {list(config.keys())}\n")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ env_config (ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù)
            if 'env_config' in config:
                env_config = config['env_config']
            elif 'config' in config:
                env_config = config['config']
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ dimensions (ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù)
            if 'dimensions' in config:
                state_dim = config['dimensions'].get('state_dim')
                action_dim = config['dimensions'].get('action_dim')
            elif 'state_dim' in config:
                state_dim = config['state_dim']
                action_dim = config.get('action_dim', 7)
            
            if state_dim:
                print(f"ðŸ“ Config Dimensions:")
                print(f"   State:  {state_dim}")
                print(f"   Action: {action_dim}\n")
            
        except Exception as e:
            print(f"âš ï¸  Could not parse config: {e}")
            config_path = None
    
    # Ø§Ú¯Ø± config Ù†Ø¨ÙˆØ¯ ÛŒØ§ Ù†Ø§Ù‚Øµ Ø¨ÙˆØ¯ØŒ Ø§Ø² defaults Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
    if env_config is None:
        print(f"âš ï¸  Using default environment config")
        env_config = {
            'num_uavs': 3,
            'num_devices': 5,
            'num_edge_servers': 2,
            'grid_size': 500.0,
            'max_steps': 50,
        }
    
    if action_dim is None:
        action_dim = 7
    
    # Ø³Ø§Ø®Øª Ù…Ø­ÛŒØ·
    print(f"ðŸŒ Creating environment with config:")
    for k, v in env_config.items():
        print(f"   {k}: {v}")
    
    env = UAVMECEnvironment(**env_config)
    
    # ØªØ´Ø®ÛŒØµ Ø§Ø¨Ø¹Ø§Ø¯ Ø§Ø² Ù…Ø­ÛŒØ·
    print(f"\nðŸ” Detecting environment dimensions...")
    dummy_state = env.reset()
    if isinstance(dummy_state, tuple):
        dummy_state = dummy_state[0]
    
    detected_state_dim = len(dummy_state) if isinstance(dummy_state, np.ndarray) else dummy_state.shape[0]
    
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø¨Ø¹Ø§Ø¯ config ÛŒØ§ detected
    if state_dim is None:
        state_dim = detected_state_dim
        print(f"   Using detected state_dim: {state_dim}")
    else:
        if state_dim != detected_state_dim:
            print(f"   âš ï¸  WARNING: Config state_dim ({state_dim}) != detected ({detected_state_dim})")
            print(f"   â“ Which one to use?")
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² config Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡
            print(f"   â†’ Using CONFIG value: {state_dim} (model was trained with this)")
        else:
            print(f"   âœ… Dimensions match: {state_dim}")
    
    print(f"\n{'='*60}")
    print(f"ðŸ“ Final Dimensions:")
    print(f"   State:  {state_dim}")
    print(f"   Action: {action_dim}")
    print(f"{'='*60}\n")
    
    # Ø³Ø§Ø®Øª Agent (Ø¨Ø§ Ù‡Ù…Ø§Ù† Ø³Ø§Ø®ØªØ§Ø± train_4layer_3level.py)
    print(f"ðŸ¤– Creating MADDPGAgent...")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"   Device: {device}")
    
    try:
        agent = MADDPGAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=512,
            lr_actor=1e-4,
            lr_critic=1e-3,
            gamma=0.99,
            tau=0.01
        )
        print(f"   âœ… Agent created successfully")
    except Exception as e:
        print(f"   âŒ Error creating agent: {e}")
        return None
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§
    print(f"\nðŸ“¦ Loading model weights...")
    try:
        if not Path(model_path).exists():
            raise FileNotFoundError(f"Model file not found: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=device)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø®ØªØ§Ø± checkpoint
        print(f"   Keys in checkpoint: {list(checkpoint.keys())}")
        
        # Load state_dicts
        loaded = False
        
        # Ø±ÙˆØ´ 1: actor_state_dict & critic_state_dict
        if 'actor_state_dict' in checkpoint:
            agent.actor.load_state_dict(checkpoint['actor_state_dict'])
            print(f"   âœ… Loaded actor_state_dict")
            if 'critic_state_dict' in checkpoint:
                agent.critic.load_state_dict(checkpoint['critic_state_dict'])
                print(f"   âœ… Loaded critic_state_dict")
            loaded = True
        
        # Ø±ÙˆØ´ 2: actor & critic
        elif 'actor' in checkpoint and 'critic' in checkpoint:
            agent.actor.load_state_dict(checkpoint['actor'])
            agent.critic.load_state_dict(checkpoint['critic'])
            print(f"   âœ… Loaded actor & critic")
            loaded = True
        
        # Ø±ÙˆØ´ 3: model_state_dict
        elif 'model_state_dict' in checkpoint:
            agent.actor.load_state_dict(checkpoint['model_state_dict'])
            print(f"   âœ… Loaded model_state_dict")
            loaded = True
        
        # Ø±ÙˆØ´ 4: Ù…Ø³ØªÙ‚ÛŒÙ… (ÙÙ‚Ø· actor)
        else:
            try:
                agent.actor.load_state_dict(checkpoint)
                print(f"   âœ… Loaded actor (direct)")
                loaded = True
            except:
                pass
        
        if not loaded:
            raise ValueError("Could not load model weights - unknown checkpoint format")
        
        print(f"âœ… Model loaded successfully!\n")
        
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        print(f"\nðŸ” Checkpoint structure:")
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            if isinstance(checkpoint, dict):
                for k, v in list(checkpoint.items())[:10]:
                    if isinstance(v, dict):
                        print(f"   {k}: dict with {len(v)} keys")
                    elif hasattr(v, 'shape'):
                        print(f"   {k}: tensor {v.shape}")
                    else:
                        print(f"   {k}: {type(v)}")
        except:
            pass
        return None
    
    # Set to eval mode
    agent.actor.eval()
    
    # ØªØ³Øª episodes
    print(f"{'='*70}")
    print(f"ðŸŽ® Running Test Episodes")
    print(f"{'='*70}\n")
    
    test_rewards = []
    test_lengths = []
    
    for ep in range(num_episodes):
        state = env.reset()
        if isinstance(state, tuple):
            state = state[0]
        
        # Ú†Ú© Ø§Ø¨Ø¹Ø§Ø¯ state
        current_state_dim = len(state)
        if current_state_dim != state_dim:
            print(f"âš ï¸  Episode {ep+1}: State dimension mismatch!")
            print(f"   Expected: {state_dim}, Got: {current_state_dim}")
            print(f"   Attempting to adapt...")
            
            # Ø³Ø¹ÛŒ Ø¯Ø± ØªØ·Ø¨ÛŒÙ‚ Ø§Ø¨Ø¹Ø§Ø¯
            if current_state_dim > state_dim:
                state = state[:state_dim]
                print(f"   â†’ Truncated to {state_dim}")
            else:
                state = np.pad(state, (0, state_dim - current_state_dim))
                print(f"   â†’ Padded to {state_dim}")
        
        episode_reward = 0
        step_count = 0
        done = False
        
        while not done:
            # Select action (greedy, no exploration)
            try:
                with torch.no_grad():
                    action = agent.select_action(state, noise=0.0)
            except Exception as e:
                print(f"   âŒ Error selecting action: {e}")
                break
            
            # Execute in environment
            try:
                result = env.step(action)
            except Exception as e:
                print(f"   âŒ Error in env.step: {e}")
                break
            
            if len(result) == 4:
                next_state, reward, done, info = result
            else:
                print(f"   âš ï¸  Unexpected step result format: {len(result)} items")
                break
            
            if isinstance(next_state, tuple):
                next_state = next_state[0]
            
            # ØªØ·Ø¨ÛŒÙ‚ Ø§Ø¨Ø¹Ø§Ø¯ next_state
            if len(next_state) != state_dim:
                if len(next_state) > state_dim:
                    next_state = next_state[:state_dim]
                else:
                    next_state = np.pad(next_state, (0, state_dim - len(next_state)))
            
            episode_reward += reward
            step_count += 1
            state = next_state
            
            # Safety limit
            if step_count > 1000:
                print(f"   âš ï¸  Episode {ep+1} too long (>1000 steps), breaking...")
                break
        
        test_rewards.append(episode_reward)
        test_lengths.append(step_count)
        
        print(f"Episode {ep+1:2d}/{num_episodes} â”‚ Steps: {step_count:3d} â”‚ Reward: {episode_reward:9.2f}")
    
    # Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
    print(f"\n{'='*70}")
    print(f"ðŸ“Š Test Results Summary")
    print(f"{'='*70}")
    print(f"  Episodes:      {num_episodes}")
    print(f"  Mean Reward:   {np.mean(test_rewards):9.2f} Â± {np.std(test_rewards):.2f}")
    print(f"  Min Reward:    {np.min(test_rewards):9.2f}")
    print(f"  Max Reward:    {np.max(test_rewards):9.2f}")
    print(f"  Mean Length:   {np.mean(test_lengths):.1f} steps")
    print(f"{'='*70}\n")
    
    return test_rewards, test_lengths


def compare_with_random(env_config, num_episodes=10):
    """
    Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ random policy
    """
    print(f"\n{'='*70}")
    print(f"ðŸŽ² Testing Random Policy (Baseline)")
    print(f"{'='*70}\n")
    
    env = UAVMECEnvironment(**env_config)
    
    random_rewards = []
    random_lengths = []
    
    for ep in range(num_episodes):
        state = env.reset()
        if isinstance(state, tuple):
            state = state[0]
        
        episode_reward = 0
        step_count = 0
        done = False
        
        while not done:
            # Random action
            action = np.random.uniform(-1, 1, size=7)
            
            result = env.step(action)
            if len(result) == 4:
                next_state, reward, done, info = result
            else:
                break
            
            if isinstance(next_state, tuple):
                next_state = next_state[0]
            
            episode_reward += reward
            step_count += 1
            state = next_state
            
            if step_count > 1000:
                break
        
        random_rewards.append(episode_reward)
        random_lengths.append(step_count)
        
        print(f"Episode {ep+1:2d}/{num_episodes} â”‚ Steps: {step_count:3d} â”‚ Reward: {episode_reward:9.2f}")
    
    print(f"\n{'='*70}")
    print(f"ðŸ“Š Random Policy Results")
    print(f"{'='*70}")
    print(f"  Mean Reward:   {np.mean(random_rewards):9.2f} Â± {np.std(random_rewards):.2f}")
    print(f"  Min Reward:    {np.min(random_rewards):9.2f}")
    print(f"  Max Reward:    {np.max(random_rewards):9.2f}")
    print(f"  Mean Length:   {np.mean(random_lengths):.1f} steps")
    print(f"{'='*70}\n")
    
    return random_rewards, random_lengths


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Test trained MADDPG model')
    parser.add_argument('--model', type=str,
                       default='results/4layer_3level/level_1/best_model.pth',
                       help='Path to trained model checkpoint')
    parser.add_argument('--episodes', type=int, default=10,
                       help='Number of test episodes')
    parser.add_argument('--compare-random', action='store_true',
                       help='Also run random policy for comparison')
    args = parser.parse_args()
    
    # Test trained model
    results = test_model(args.model, num_episodes=args.episodes)
    
    if results is None:
        print("âŒ Model testing failed!")
        exit(1)
    
    trained_rewards, trained_lengths = results
    
    # Compare with random if requested
    if args.compare_random:
        # Load env config
        model_dir = Path(args.model).parent
        config_path = model_dir / 'config.json'
        
        env_config = None
        if config_path.exists():
            try:
                with open(config_path, 'r') as f:
                    config = json.load(f)
                env_config = config.get('env_config') or config.get('config')
            except:
                pass
        
        if env_config is None:
            env_config = {
                'num_uavs': 3,
                'num_devices': 5,
                'num_edge_servers': 2,
                'grid_size': 500.0,
                'max_steps': 50,
            }
        
        random_rewards, random_lengths = compare_with_random(env_config, num_episodes=args.episodes)
        
        # Final comparison
        improvement = np.mean(trained_rewards) - np.mean(random_rewards)
        improvement_pct = (improvement / abs(np.mean(random_rewards))) * 100 if np.mean(random_rewards) != 0 else 0
        
        print(f"\n{'='*70}")
        print(f"ðŸ“ˆ Performance Comparison")
        print(f"{'='*70}")
        print(f"  Trained Mean:     {np.mean(trained_rewards):9.2f}")
        print(f"  Random Mean:      {np.mean(random_rewards):9.2f}")
        print(f"  Improvement:      {improvement:9.2f} ({improvement_pct:+.1f}%)")
        print(f"{'='*70}")
        
        if improvement > 0:
            print(f"âœ… Trained model is BETTER than random policy!")
        elif improvement < -100:
            print(f"âŒ Trained model is significantly WORSE than random!")
        else:
            print(f"âš ï¸  Trained model is similar to random (needs more training)")
        
        print(f"{'='*70}\n")
    
    print("âœ… Testing complete!")
