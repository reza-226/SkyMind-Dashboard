"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¢Ù…ÙˆØ²Ø´ MADDPG Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø²:
- Ù…Ø­ÛŒØ·â€ŒÙ‡Ø§ÛŒ Dictionary-based (agent_0, agent_1, ...)
- Ø³Ø§Ø®ØªØ§Ø± ÙØ§ÛŒÙ„ ÙˆØ§Ù‚Ø¹ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ (agents/maddpg_wrapper.py)
- Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ 3D CollisionChecker
- Ù…Ø¯ÛŒØ±ÛŒØª Ø®ÙˆØ¯Ú©Ø§Ø± Discrete/Continuous action spaces
- Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
- Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ TensorBoard
"""

import os
import sys
import numpy as np
import torch
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import deque
import matplotlib.pyplot as plt
from datetime import datetime

# ğŸ”§ Patch CollisionChecker Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ 3D
sys.path.insert(0, str(Path(__file__).parent / "core"))
from collision_checker_patch import patch_collision_checker
patch_collision_checker()  # Ø§Ø¹Ù…Ø§Ù„ Ù¾Ú† Ù‚Ø¨Ù„ Ø§Ø² Ø³Ø§Ø®Øª Ù…Ø­ÛŒØ·

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ù¾Ø±ÙˆÚ˜Ù‡
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'core'))
sys.path.insert(0, str(project_root / 'agents'))

# Import Ù…Ø­ÛŒØ·
from core.env_multi import MultiUAVEnv

# Import Agent
from agents.maddpg_wrapper import MADDPGAgent

# Import ÛŒÙˆØªÛŒÙ„ÛŒØªÛŒâ€ŒÙ‡Ø§
try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False
    print("âš ï¸ TensorBoard not available. Install with: pip install tensorboard")


class ReplayBuffer:
    """
    Ø¨Ø§ÙØ± ØªØ¬Ø±Ø¨Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ ØªØ¬Ø±Ø¨ÛŒØ§Øª
    """
    def __init__(self, capacity: int = 100000):
        self.buffer = deque(maxlen=capacity)
    
    def add(
        self, 
        states: Dict[str, np.ndarray], 
        actions: Dict[str, np.ndarray],
        rewards: Dict[str, float],
        next_states: Dict[str, np.ndarray],
        dones: Dict[str, bool]
    ):
        """Ø§ÙØ²ÙˆØ¯Ù† ÛŒÚ© ØªØ¬Ø±Ø¨Ù‡"""
        self.buffer.append((states, actions, rewards, next_states, dones))
    
    def sample(self, batch_size: int) -> Tuple:
        """Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ ØªØµØ§Ø¯ÙÛŒ"""
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        
        batch_states = []
        batch_actions = []
        batch_rewards = []
        batch_next_states = []
        batch_dones = []
        
        for idx in indices:
            states, actions, rewards, next_states, dones = self.buffer[idx]
            batch_states.append(states)
            batch_actions.append(actions)
            batch_rewards.append(rewards)
            batch_next_states.append(next_states)
            batch_dones.append(dones)
        
        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones
    
    def __len__(self):
        return len(self.buffer)


def create_env(config: Optional[Dict] = None) -> MultiUAVEnv:
    """
    Ø³Ø§Ø®Øª Ù…Ø­ÛŒØ· Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ØµØ­ÛŒØ­
    """
    default_config = {
        'num_uavs': 3,           # Ù¾Ø§Ø±Ø§Ù…ØªØ± ØµØ­ÛŒØ­ (Ù†Ù‡ n_uavs)
        'map_size': 100,
        'num_obstacles': 10,
        'max_steps': 500,
        'render_mode': None
    }
    
    if config:
        default_config.update(config)
    
    print(f"ğŸ—ï¸ Creating environment with config: {default_config}")
    
    try:
        env = MultiUAVEnv(**default_config)
        
        # Ø§ÙØ²ÙˆØ¯Ù† alias Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ
        if hasattr(env, 'num_uavs') and not hasattr(env, 'n_agents'):
            env.n_agents = env.num_uavs
        
        print(f"âœ… Environment created: {env.num_uavs} UAVs")
        return env
        
    except Exception as e:
        print(f"âŒ Error creating environment: {e}")
        raise


def get_state_action_dims(env: MultiUAVEnv) -> Tuple[int, int]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø¨Ø¹Ø§Ø¯ state Ùˆ action Ø§Ø² Ù…Ø­ÛŒØ·
    """
    # Ø¯Ø±ÛŒØ§ÙØª ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ state
    states, _ = env.reset()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ state_dim Ø§Ø² Ø§ÙˆÙ„ÛŒÙ† agent
    first_agent_key = list(states.keys())[0]
    sample_state = states[first_agent_key]
    state_dim = sample_state.flatten().shape[0]
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ action_dim
    sample_action_space = env.action_space
    
    if hasattr(sample_action_space, 'n'):
        # Discrete action space
        act_dim = sample_action_space.n
        print(f"ğŸ“Š Discrete action space detected: {act_dim} actions")
    elif hasattr(sample_action_space, 'shape'):
        # Continuous action space
        act_dim = sample_action_space.shape[0] if len(sample_action_space.shape) > 0 else 2
        print(f"ğŸ“Š Continuous action space detected: {act_dim}D")
    else:
        # Fallback: ÙØ±Ø¶ Ú©Ù† 2D continuous
        act_dim = 2
        print(f"âš ï¸ Unknown action space, assuming 2D continuous")
    
    print(f"ğŸ“ State dim: {state_dim}, Action dim: {act_dim}")
    
    return state_dim, act_dim


def train_maddpg(
    env: MultiUAVEnv,
    n_episodes: int = 1000,
    max_steps: int = 500,
    batch_size: int = 64,
    buffer_capacity: int = 100000,
    update_freq: int = 100,
    save_freq: int = 100,
    log_dir: str = "runs",
    model_dir: str = "models"
):
    """
    Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø¢Ù…ÙˆØ²Ø´ MADDPG
    """
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒâ€ŒÙ‡Ø§
    Path(log_dir).mkdir(exist_ok=True)
    Path(model_dir).mkdir(exist_ok=True)
    
    # TensorBoard
    writer = None
    if TENSORBOARD_AVAILABLE:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        writer = SummaryWriter(f"{log_dir}/maddpg_{timestamp}")
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø¨Ø¹Ø§Ø¯
    state_dim, act_dim = get_state_action_dims(env)
    n_agents = env.num_uavs
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ Starting MADDPG Training")
    print(f"{'='*70}")
    print(f"Agents: {n_agents}")
    print(f"State dim: {state_dim}, Action dim: {act_dim}")
    print(f"Episodes: {n_episodes}, Max steps: {max_steps}")
    print(f"Batch size: {batch_size}, Buffer: {buffer_capacity}")
    print(f"{'='*70}\n")
    
    # Ø³Ø§Ø®Øª Agents
    agents = {}
    for i in range(n_agents):
        agent_id = f"agent_{i}"
        agents[agent_id] = MADDPGAgent(
            state_dim=state_dim,
            action_dim=act_dim,
            n_agents=n_agents,
            agent_id=i,
            hidden_dim=256,
            lr_actor=1e-4,
            lr_critic=1e-3,
            gamma=0.99,
            tau=0.01
        )
    
    # Replay Buffer
    replay_buffer = ReplayBuffer(capacity=buffer_capacity)
    
    # Ø¢Ù…Ø§Ø± Ø¢Ù…ÙˆØ²Ø´
    episode_rewards = []
    episode_losses = []
    best_avg_reward = -float('inf')
    
    # Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø¢Ù…ÙˆØ²Ø´
    for episode in range(n_episodes):
        states, _ = env.reset()
        episode_reward = {agent_id: 0.0 for agent_id in agents.keys()}
        episode_loss = {agent_id: [] for agent_id in agents.keys()}
        
        for step in range(max_steps):
            # Ø§Ù†ØªØ®Ø§Ø¨ actions
            actions = {}
            for agent_id, agent in agents.items():
                state = states[agent_id].flatten()
                
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†ÙˆÛŒØ² Ø¨Ø±Ø§ÛŒ Ø§Ú©ØªØ´Ø§Ù
                action = agent.select_action(state, add_noise=True)
                actions[agent_id] = action
            
            # Ø§Ø¬Ø±Ø§ÛŒ action Ø¯Ø± Ù…Ø­ÛŒØ·
            next_states, rewards, dones, truncated, _ = env.step(actions)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¨Ø§ÙØ±
            replay_buffer.add(states, actions, rewards, next_states, dones)
            
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ rewards
            for agent_id in agents.keys():
                episode_reward[agent_id] += rewards[agent_id]
            
            # Ø¢Ù…ÙˆØ²Ø´ agents (Ø§Ú¯Ø± Ø¨Ø§ÙØ± Ú©Ø§ÙÛŒ Ø¨Ø§Ø´Ø¯)
            if len(replay_buffer) > batch_size:
                # Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ø² Ø¨Ø§ÙØ±
                batch = replay_buffer.sample(batch_size)
                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = batch
                
                # Ø¢Ù…ÙˆØ²Ø´ Ù‡Ø± agent
                for agent_id, agent in agents.items():
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ† agent
                    agent_states = np.array([s[agent_id].flatten() for s in batch_states])
                    agent_actions = np.array([a[agent_id] for a in batch_actions])
                    agent_rewards = np.array([r[agent_id] for r in batch_rewards])
                    agent_next_states = np.array([s[agent_id].flatten() for s in batch_next_states])
                    agent_dones = np.array([d[agent_id] for d in batch_dones])
                    
                    # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø§ÛŒØ± agents
                    all_states = np.array([[s[aid].flatten() for aid in agents.keys()] 
                                          for s in batch_states])
                    all_actions = np.array([[a[aid] for aid in agents.keys()] 
                                           for a in batch_actions])
                    all_next_states = np.array([[s[aid].flatten() for aid in agents.keys()] 
                                                for s in batch_next_states])
                    
                    # Ø¢Ù…ÙˆØ²Ø´
                    critic_loss, actor_loss = agent.update(
                        agent_states,
                        agent_actions,
                        agent_rewards,
                        agent_next_states,
                        agent_dones,
                        all_states,
                        all_actions,
                        all_next_states
                    )
                    
                    episode_loss[agent_id].append((critic_loss, actor_loss))
            
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ state
            states = next_states
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø§ÛŒØ§Ù†
            if all(dones.values()) or all(truncated.values()):
                break
        
        # Ø¢Ù…Ø§Ø± episode
        avg_reward = np.mean(list(episode_reward.values()))
        episode_rewards.append(avg_reward)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† loss
        avg_losses = {}
        for agent_id in agents.keys():
            if episode_loss[agent_id]:
                critic_losses, actor_losses = zip(*episode_loss[agent_id])
                avg_losses[agent_id] = {
                    'critic': np.mean(critic_losses),
                    'actor': np.mean(actor_losses)
                }
        
        # Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
        if writer:
            writer.add_scalar('Train/AverageReward', avg_reward, episode)
            for agent_id, losses in avg_losses.items():
                writer.add_scalar(f'Train/{agent_id}/CriticLoss', losses['critic'], episode)
                writer.add_scalar(f'Train/{agent_id}/ActorLoss', losses['actor'], episode)
        
        # Ú†Ø§Ù¾ Ù¾ÛŒØ´Ø±ÙØª
        if (episode + 1) % 10 == 0:
            recent_avg = np.mean(episode_rewards[-100:])
            print(f"Episode {episode+1}/{n_episodes} | "
                  f"Avg Reward: {avg_reward:.2f} | "
                  f"Recent 100: {recent_avg:.2f} | "
                  f"Buffer: {len(replay_buffer)}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
        if (episode + 1) % save_freq == 0:
            recent_avg = np.mean(episode_rewards[-100:])
            if recent_avg > best_avg_reward:
                best_avg_reward = recent_avg
                for agent_id, agent in agents.items():
                    agent.save(f"{model_dir}/{agent_id}_best.pth")
                print(f"ğŸ’¾ Best model saved! Avg reward: {best_avg_reward:.2f}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª
        if (episode + 1) % save_freq == 0:
            for agent_id, agent in agents.items():
                agent.save(f"{model_dir}/{agent_id}_ep{episode+1}.pth")
    
    # Ø¨Ø³ØªÙ† writer
    if writer:
        writer.close()
    
    # Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± rewards
    plt.figure(figsize=(10, 6))
    plt.plot(episode_rewards, alpha=0.6, label='Episode Reward')
    
    # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú©
    window = 100
    if len(episode_rewards) >= window:
        moving_avg = np.convolve(episode_rewards, 
                                np.ones(window)/window, 
                                mode='valid')
        plt.plot(range(window-1, len(episode_rewards)), 
                moving_avg, 
                'r-', 
                linewidth=2, 
                label=f'{window}-Episode Moving Average')
    
    plt.xlabel('Episode')
    plt.ylabel('Average Reward')
    plt.title('MADDPG Training Progress')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(f"{log_dir}/training_rewards.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"\n{'='*70}")
    print(f"âœ… Training completed!")
    print(f"Best average reward: {best_avg_reward:.2f}")
    print(f"Final average reward (last 100 eps): {np.mean(episode_rewards[-100:]):.2f}")
    print(f"{'='*70}\n")
    
    return agents, episode_rewards


def main():
    """
    ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ
    """
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø­ÛŒØ·
    env_config = {
        'num_uavs': 3,
        'map_size': 100,
        'num_obstacles': 10,
        'max_steps': 500,
        'render_mode': None
    }
    
    # Ø³Ø§Ø®Øª Ù…Ø­ÛŒØ·
    env = create_env(env_config)
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´
    train_config = {
        'n_episodes': 1000,
        'max_steps': 500,
        'batch_size': 64,
        'buffer_capacity': 100000,
        'update_freq': 100,
        'save_freq': 100,
        'log_dir': 'runs',
        'model_dir': 'models'
    }
    
    # Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´
    try:
        agents, rewards = train_maddpg(env, **train_config)
        print("ğŸ‰ Training finished successfully!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Training interrupted by user")
        
    except Exception as e:
        print(f"\nâŒ Training failed with error: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        env.close()
        print("ğŸ”’ Environment closed")


if __name__ == "__main__":
    main()
