"""
train_maddpg_final.py (Ù†Ø³Ø®Ù‡ v3 - Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ Ù…Ø­ÛŒØ· Ø³ÙØ§Ø±Ø´ÛŒ)
"""

import os
import sys
import json
import numpy as np
from datetime import datetime
from tqdm import tqdm
import matplotlib.pyplot as plt

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.env_multi import MultiUAVEnv
from agents.agent_maddpg_multi import MADDPG_Agent

# Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
EPISODES = 5000
MAX_STEPS = 200
BATCH_SIZE = 256
SAVE_INTERVAL = 100
EARLY_STOP_PATIENCE = 200

os.makedirs('results/maddpg_training', exist_ok=True)
os.makedirs('models/checkpoints', exist_ok=True)

def get_env_dimensions(env):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø¨Ø¹Ø§Ø¯ Ø§Ø² Ù…Ø­ÛŒØ· Ø³ÙØ§Ø±Ø´ÛŒ"""
    
    # ØªØ¹Ø¯Ø§Ø¯ Ø¹Ø§Ù…Ù„â€ŒÙ‡Ø§
    num_agents = None
    for attr in ['num_uavs', 'n_uavs', 'num_agents', 'n_agents']:
        if hasattr(env, attr):
            num_agents = getattr(env, attr)
            print(f"   âœ“ ØªØ¹Ø¯Ø§Ø¯ Ø¹Ø§Ù…Ù„â€ŒÙ‡Ø§ ({attr}): {num_agents}")
            break
    
    if num_agents is None:
        num_agents = 3  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        print(f"   âš ï¸ ØªØ¹Ø¯Ø§Ø¯ Ø¹Ø§Ù…Ù„â€ŒÙ‡Ø§: {num_agents} (Ù¾ÛŒØ´â€ŒÙØ±Ø¶)")
    
    # Ø¨Ø¹Ø¯ Ø­Ø§Ù„Øª - ØªØ³Øª Ø¨Ø§ reset
    state, _ = env.reset()
    
    if isinstance(state, np.ndarray):
        if state.ndim == 1:
            state_dim = len(state)
        elif state.ndim == 2:
            state_dim = state.shape[1]  # Ø¨Ø±Ø§ÛŒ multi-agent
        else:
            state_dim = np.prod(state.shape)
    else:
        # Ø§Ú¯Ø± state ÛŒÚ© Ù„ÛŒØ³Øª ÛŒØ§ tuple Ø§Ø³Øª
        state_dim = len(state) if hasattr(state, '__len__') else 10  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶
    
    print(f"   âœ“ Ø¨Ø¹Ø¯ Ø­Ø§Ù„Øª: {state_dim} (Ø§Ø² reset Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯)")
    
    # Ø¨Ø¹Ø¯ Ø¹Ù…Ù„ - ØªØ³Øª
    action_dim = None
    for attr in ['action_dim', 'action_space_dim']:
        if hasattr(env, attr):
            action_dim = getattr(env, attr)
            print(f"   âœ“ Ø¨Ø¹Ø¯ Ø¹Ù…Ù„ ({attr}): {action_dim}")
            break
    
    if action_dim is None:
        action_dim = 3  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶: [offload_ratio, cpu_freq, tx_power]
        print(f"   âš ï¸ Ø¨Ø¹Ø¯ Ø¹Ù…Ù„: {action_dim} (Ù¾ÛŒØ´â€ŒÙØ±Ø¶)")
    
    return num_agents, state_dim, action_dim

def train_maddpg():
    """Ø¢Ù…ÙˆØ²Ø´ MADDPG"""
    
    print("="*60)
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ MADDPG")
    print("="*60)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ÛŒØ·
    print("\nğŸ“Œ Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ÛŒØ·...")
    env = MultiUAVEnv()
    print("   âœ“ Ù…Ø­ÛŒØ· Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø¨Ø¹Ø§Ø¯
    print("\nğŸ“Œ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø¨Ø¹Ø§Ø¯ Ù…Ø­ÛŒØ·...")
    num_agents, state_dim, action_dim = get_env_dimensions(env)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Agent
    print("\nğŸ“Œ Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ MADDPG Agent...")
    agent = MADDPG_Agent(
        state_dim=state_dim,
        action_dim=action_dim,
        num_agents=num_agents,
        lr_actor=1e-4,
        lr_critic=1e-3,
        gamma=0.99,
        tau=0.01,
        buffer_size=1000000
    )
    print("   âœ“ Agent Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª")
    
    # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
    episode_rewards = []
    episode_delays = []
    episode_energies = []
    best_reward = float('-inf')
    no_improvement = 0
    
    # Ø­Ù„Ù‚Ù‡ Ø¢Ù…ÙˆØ²Ø´
    print("\n" + "="*60)
    print("ğŸ“ Ø´Ø±ÙˆØ¹ Ø­Ù„Ù‚Ù‡ Ø¢Ù…ÙˆØ²Ø´...")
    print("="*60 + "\n")
    
    pbar = tqdm(range(1, EPISODES + 1), desc="Training")
    
    for episode in pbar:
        state, _ = env.reset()
        
        # ØªØ¨Ø¯ÛŒÙ„ state Ø¨Ù‡ numpy array Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²
        if not isinstance(state, np.ndarray):
            state = np.array(state)
        
        # Flatten Ú©Ø±Ø¯Ù† state Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²
        if state.ndim > 1:
            state = state.flatten()
        
        episode_reward = 0
        episode_delay = 0
        episode_energy = 0
        
        for step in range(MAX_STEPS):
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø¹Ù…Ù„
            actions = agent.select_actions(state, add_noise=True)
            
            # Reshape Ø¨Ø±Ø§ÛŒ Ù…Ø­ÛŒØ· (Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ø§Ø´Ø¯)
            actions_for_env = actions.reshape(num_agents, action_dim)
            
            # Ø§Ø¬Ø±Ø§ Ø¯Ø± Ù…Ø­ÛŒØ·
            next_state, reward, done, truncated, info = env.step(actions_for_env)
            
            # ØªØ¨Ø¯ÛŒÙ„ next_state
            if not isinstance(next_state, np.ndarray):
                next_state = np.array(next_state)
            if next_state.ndim > 1:
                next_state = next_state.flatten()
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¨Ø§ÙØ±
            agent.store_transition(state, actions, reward, next_state, done)
            
            # Ø¢Ù…ÙˆØ²Ø´
            if len(agent.memory) > BATCH_SIZE:
                agent.update(BATCH_SIZE)
            
            # Ø¢Ù¾Ø¯ÛŒØª
            episode_reward += reward
            episode_delay += info.get('delay_total', info.get('delay', 0))
            episode_energy += info.get('energy_total', info.get('energy', 0))
            
            state = next_state
            
            if done or truncated:
                break
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
        episode_rewards.append(episode_reward)
        episode_delays.append(episode_delay)
        episode_energies.append(episode_energy)
        
        # Ø¢Ù¾Ø¯ÛŒØª progress bar
        avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)
        pbar.set_postfix({
            'Reward': f'{episode_reward:.2e}',
            'Avg100': f'{avg_reward:.2e}',
            'Delay': f'{episode_delay:.2f}s',
            'Energy': f'{episode_energy:.2e}J'
        })
        
        # Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
        if avg_reward > best_reward:
            best_reward = avg_reward
            no_improvement = 0
            agent.save_models('models/checkpoints/best_model')
            if episode % 50 == 0:  # Ù¾Ø±ÛŒÙ†Øª Ù‡Ø± 50 Ø§Ù¾ÛŒØ²ÙˆØ¯
                print(f"\nâœ¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ (Episode {episode}): {best_reward:.2e}")
        else:
            no_improvement += 1
        
        # Checkpoint
        if episode % SAVE_INTERVAL == 0:
            agent.save_models(f'models/checkpoints/episode_{episode}')
            print(f"\nğŸ’¾ Checkpoint Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: Episode {episode}")
        
        # Early stopping
        if no_improvement >= EARLY_STOP_PATIENCE:
            print(f"\nâš ï¸ Early Stopping: {EARLY_STOP_PATIENCE} Ø§Ù¾ÛŒØ²ÙˆØ¯ Ø¨Ø¯ÙˆÙ† Ø¨Ù‡Ø¨ÙˆØ¯")
            break
    
    pbar.close()
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    print("\n" + "="*60)
    print("ğŸ’¾ Ø¯Ø± Ø­Ø§Ù„ Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬...")
    print("="*60)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # JSON
    results = {
        'timestamp': timestamp,
        'episodes': len(episode_rewards),
        'best_reward': float(best_reward),
        'final_avg_reward': float(np.mean(episode_rewards[-100:])) if len(episode_rewards) >= 100 else float(np.mean(episode_rewards)),
        'rewards': [float(r) for r in episode_rewards],
        'delays': [float(d) for d in episode_delays],
        'energies': [float(e) for e in episode_energies]
    }
    
    json_path = f'results/maddpg_training/training_{timestamp}.json'
    with open(json_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"   âœ“ JSON: {json_path}")
    
    # Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    episodes_range = range(1, len(episode_rewards) + 1)
    
    # Reward
    axes[0, 0].plot(episodes_range, episode_rewards, alpha=0.3, label='Episode Reward')
    if len(episode_rewards) >= 100:
        moving_avg = np.convolve(episode_rewards, np.ones(100)/100, mode='valid')
        axes[0, 0].plot(range(100, len(episode_rewards) + 1), moving_avg, 
                        label='Moving Avg (100)', linewidth=2)
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Reward')
    axes[0, 0].set_title('Training Reward')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Delay
    axes[0, 1].plot(episodes_range, episode_delays, alpha=0.3)
    if len(episode_delays) >= 100:
        moving_avg = np.convolve(episode_delays, np.ones(100)/100, mode='valid')
        axes[0, 1].plot(range(100, len(episode_delays) + 1), moving_avg, linewidth=2)
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('Total Delay (s)')
    axes[0, 1].set_title('Episode Delay')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Energy
    axes[1, 0].plot(episodes_range, episode_energies, alpha=0.3)
    if len(episode_energies) >= 100:
        moving_avg = np.convolve(episode_energies, np.ones(100)/100, mode='valid')
        axes[1, 0].plot(range(100, len(episode_energies) + 1), moving_avg, linewidth=2)
    axes[1, 0].set_xlabel('Episode')
    axes[1, 0].set_ylabel('Total Energy (J)')
    axes[1, 0].set_title('Episode Energy')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Delay vs Energy
    axes[1, 1].scatter(episode_delays, episode_energies, alpha=0.5, s=10)
    axes[1, 1].set_xlabel('Delay (s)')
    axes[1, 1].set_ylabel('Energy (J)')
    axes[1, 1].set_title('Delay-Energy Trade-off')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    png_path = f'results/maddpg_training/training_{timestamp}.png'
    plt.savefig(png_path, dpi=300, bbox_inches='tight')
    print(f"   âœ“ PNG: {png_path}")
    plt.close()
    
    # TXT
    txt_path = f'results/maddpg_training/training_{timestamp}.txt'
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("Ù†ØªØ§ÛŒØ¬ Ø¢Ù…ÙˆØ²Ø´ MADDPG\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"ØªØ§Ø±ÛŒØ®: {timestamp}\n")
        f.write(f"ØªØ¹Ø¯Ø§Ø¯ Ø§Ù¾ÛŒØ²ÙˆØ¯Ù‡Ø§: {len(episode_rewards)}\n\n")
        f.write(f"Ø¨Ù‡ØªØ±ÛŒÙ† Reward: {best_reward:.2e}\n")
        last_100 = episode_rewards[-100:] if len(episode_rewards) >= 100 else episode_rewards
        f.write(f"Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¢Ø®Ø±:\n")
        f.write(f"  - Reward: {np.mean(last_100):.2e}\n")
        f.write(f"  - Delay: {np.mean(episode_delays[-len(last_100):]):.2f} s\n")
        f.write(f"  - Energy: {np.mean(episode_energies[-len(last_100):]):.2e} J\n")
    print(f"   âœ“ TXT: {txt_path}")
    
    # Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ
    agent.save_models('models/maddpg_final')
    print(f"   âœ“ Models: models/maddpg_final/")
    
    print("\n" + "="*60)
    print("âœ… Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!")
    print("="*60)

if __name__ == '__main__':
    train_maddpg()
