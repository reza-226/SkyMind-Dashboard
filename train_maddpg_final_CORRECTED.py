# train_maddpg_final_FIXED_V3.py
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¢Ù…ÙˆØ²Ø´ MADDPG - Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ ØªØ¨Ø¯ÛŒÙ„ Tensor
"""

import numpy as np
import torch
import matplotlib.pyplot as plt
from pathlib import Path
import time

from core.env_multi import MultiUAVEnv
from agents.agent_maddpg_multi import MADDPG_Agent

# =============================================================================
# 1. ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# =============================================================================
CONFIG = {
    # Ù…Ø­ÛŒØ·
    'n_agents': 3,
    'n_users': 5,
    'dt': 1.0,
    'area_size': 1000.0,
    
    # Ø¢Ù…ÙˆØ²Ø´
    'n_episodes': 1000,
    'max_steps': 200,
    'batch_size': 128,
    'buffer_size': 100000,
    
    # ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
    'lr_actor': 1e-4,
    'lr_critic': 1e-3,
    'gamma': 0.99,
    'tau': 0.01,
    
    # Ø§Ú©ØªØ´Ø§Ù
    'noise_scale_start': 1.0,
    'noise_scale_end': 0.1,
    'noise_decay': 0.995,
    
    # Ø°Ø®ÛŒØ±Ù‡
    'save_interval': 50,
    'log_interval': 10,
    
    # Early stopping
    'patience': 100,
    'min_improvement': 0.01,
    
    # Device
    'device': 'cuda' if torch.cuda.is_available() else 'cpu'
}

# =============================================================================
# 2. ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ
# =============================================================================

def flatten_state(state_dict):
    """ØªØ¨Ø¯ÛŒÙ„ dictionary state Ø¨Ù‡ vector 38 Ø¨Ø¹Ø¯ÛŒ"""
    parts = []
    
    # UAV positions (3 UAVs Ã— 2) = 6
    if 'uav_positions' in state_dict:
        parts.append(state_dict['uav_positions'].flatten())
    
    # User positions (5 users Ã— 2) = 10
    if 'user_positions' in state_dict:
        parts.append(state_dict['user_positions'].flatten())
    
    # UAV velocities (3) = 3
    if 'uav_velocities' in state_dict:
        parts.append(state_dict['uav_velocities'])
    
    # UAV headings (3) = 3
    if 'uav_headings' in state_dict:
        parts.append(state_dict['uav_headings'])
    
    # UAV energies (3) = 3
    if 'uav_energies' in state_dict:
        parts.append(state_dict['uav_energies'])
    
    # User data sizes (5) = 5
    if 'user_data_sizes' in state_dict:
        parts.append(state_dict['user_data_sizes'])
    
    # User deadlines (5) = 5
    if 'user_deadlines' in state_dict:
        parts.append(state_dict['user_deadlines'])
    
    # Time remaining (1) = 1
    if 'time_remaining' in state_dict:
        parts.append(np.array([state_dict['time_remaining']]))
    
    # Distances (2) = 2
    if 'distances' in state_dict:
        dist = state_dict['distances']
        if isinstance(dist, np.ndarray):
            parts.append(dist.flatten()[:2])
        else:
            parts.append(np.array([dist, dist]))
    
    # ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡
    result = np.concatenate(parts)
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² 38 Ø¨Ø¹Ø¯
    if len(result) < 38:
        result = np.pad(result, (0, 38 - len(result)), mode='constant')
    elif len(result) > 38:
        result = result[:38]
    
    return result


class ReplayBufferWrapper:
    """
    Wrapper Ø¨Ø±Ø§ÛŒ Replay Buffer Ú©Ù‡ Ø¨Ø§ Agent Ø³Ø§Ø²Ú¯Ø§Ø± Ø§Ø³Øª
    Ùˆ NumPy arrays Ø±Ø§ Ø¨Ù‡ Tensor ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    """
    
    def __init__(self, max_size, batch_size=128, device='cpu'):
        self.max_size = max_size
        self.batch_size = batch_size
        self.device = device  # Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Tensor
        self.buffer = []
        self.position = 0
    
    def push(self, state, action, reward, next_state, done):
        if len(self.buffer) < self.max_size:
            self.buffer.append(None)
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.max_size
    
    def sample(self, batch_size=None):
        """
        Sample Ú©Ù‡ Ø¨Ø§ Ù‡Ø± Ø¯Ùˆ API Ø³Ø§Ø²Ú¯Ø§Ø± Ø§Ø³Øª Ùˆ NumPy Ø±Ø§ Ø¨Ù‡ Tensor ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        """
        bs = batch_size if batch_size is not None else self.batch_size
        bs = min(bs, len(self.buffer))
        
        if bs == 0:
            return None, None, None, None, None
        
        indices = np.random.choice(len(self.buffer), bs, replace=False)
        states, actions, rewards, next_states, dones = [], [], [], [], []
        
        for i in indices:
            s, a, r, ns, d = self.buffer[i]
            states.append(s)
            actions.append(a)
            rewards.append(r)
            next_states.append(ns)
            dones.append(d)
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Tensor
        states = torch.FloatTensor(np.array(states)).to(self.device)
        actions = torch.FloatTensor(np.array(actions)).to(self.device)
        rewards = torch.FloatTensor(np.array(rewards)).to(self.device)
        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)
        dones = torch.FloatTensor(np.array(dones)).to(self.device)
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)


# =============================================================================
# 3. Ø¢Ù…ÙˆØ²Ø´
# =============================================================================

def train():
    print("="*70)
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ MADDPG")
    print("="*70)
    
    # ØªØ´Ø®ÛŒØµ device
    device = torch.device(CONFIG['device'])
    print(f"\nğŸ–¥ï¸  Device: {device}")
    
    # Ø³Ø§Ø®Øª Ù…Ø­ÛŒØ·
    env = MultiUAVEnv(
        n_agents=CONFIG['n_agents'],
        n_users=CONFIG['n_users'],
        dt=CONFIG['dt'],
        area_size=CONFIG['area_size']
    )
    
    # ØªØ¹ÛŒÛŒÙ† Ø§Ø¨Ø¹Ø§Ø¯
    state_dict = env.reset()
    state_sample = flatten_state(state_dict)
    state_dim = len(state_sample)
    action_dim = 4
    
    print(f"\nğŸ“Š ØªÙ†Ø¸ÛŒÙ…Ø§Øª:")
    print(f"   State dimension: {state_dim}")
    print(f"   Action dimension: {action_dim}")
    print(f"   Number of agents: {CONFIG['n_agents']}")
    print(f"   Number of users: {CONFIG['n_users']}")
    
    # Ø³Ø§Ø®Øª Agents
    agents = []
    for i in range(CONFIG['n_agents']):
        agent = MADDPG_Agent(
            state_dim=state_dim,
            action_dim=action_dim,
            n_agents=CONFIG['n_agents'],
            lr=CONFIG['lr_actor'],
            gamma=CONFIG['gamma']
        )
        # Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ device
        if hasattr(agent, 'actor'):
            agent.actor = agent.actor.to(device)
        if hasattr(agent, 'target_actor'):
            agent.target_actor = agent.target_actor.to(device)
        if hasattr(agent, 'critic'):
            agent.critic = agent.critic.to(device)
        if hasattr(agent, 'target_critic'):
            agent.target_critic = agent.target_critic.to(device)
        
        agents.append(agent)
    
    # Replay buffer Ø¨Ø§ device
    replay_buffer = ReplayBufferWrapper(
        max_size=CONFIG['buffer_size'],
        batch_size=CONFIG['batch_size'],
        device=device
    )
    
    # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ tracking
    episode_rewards = []
    moving_avg_rewards = []
    best_reward = -float('inf')
    patience_counter = 0
    noise_scale = CONFIG['noise_scale_start']
    
    # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡
    model_dir = Path('models')
    results_dir = Path('results')
    model_dir.mkdir(exist_ok=True)
    results_dir.mkdir(exist_ok=True)
    
    print(f"\nğŸ¯ Ø´Ø±ÙˆØ¹ {CONFIG['n_episodes']} Ø§Ù¾ÛŒØ²ÙˆØ¯...")
    start_time = time.time()
    
    # =============================================================================
    # Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø¢Ù…ÙˆØ²Ø´
    # =============================================================================
    for episode in range(CONFIG['n_episodes']):
        state_dict = env.reset()
        state = flatten_state(state_dict)
        episode_reward = 0
        
        for step in range(CONFIG['max_steps']):
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø§Ú©Ø´Ù† (Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Tensor Ù†ÛŒØ³ØªØŒ agent.act Ø®ÙˆØ¯Ø´ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯)
            actions = []
            for agent in agents:
                action = agent.act(state, noise_scale=noise_scale)
                actions.append(action)
            
            actions = np.array(actions)
            
            # Ø§Ø¬Ø±Ø§ Ø¯Ø± Ù…Ø­ÛŒØ·
            try:
                next_state_dict, rewards, done, info = env.step(actions)
                next_state = flatten_state(next_state_dict)
            except Exception as e:
                print(f"\nâš ï¸ Ø®Ø·Ø§ Ø¯Ø± env.step: {e}")
                print(f"   Actions shape: {actions.shape}")
                break
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± buffer (Ø¨Ù‡ ØµÙˆØ±Øª NumPy)
            replay_buffer.push(state, actions, rewards, next_state, done)
            
            # Ø¢Ù…ÙˆØ²Ø´
            if len(replay_buffer) > CONFIG['batch_size']:
                for i, agent in enumerate(agents):
                    other_agents = [a for j, a in enumerate(agents) if j != i]
                    
                    # Ø­Ø§Ù„Ø§ sample() Ø®Ø±ÙˆØ¬ÛŒ Tensor Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
                    agent.update(replay_buffer, other_agents)
            
            state = next_state
            episode_reward += np.mean(rewards) if isinstance(rewards, np.ndarray) else rewards
            
            if done:
                break
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ù¾ÛŒØ²ÙˆØ¯
        episode_rewards.append(episode_reward)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ moving average
        window = min(50, len(episode_rewards))
        moving_avg = np.mean(episode_rewards[-window:])
        moving_avg_rewards.append(moving_avg)
        
        # Ú©Ø§Ù‡Ø´ noise
        noise_scale = max(CONFIG['noise_scale_end'], 
                         noise_scale * CONFIG['noise_decay'])
        
        # Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
        if (episode + 1) % CONFIG['log_interval'] == 0:
            elapsed = time.time() - start_time
            print(f"\nğŸ“ˆ Episode {episode + 1}/{CONFIG['n_episodes']}")
            print(f"   Reward: {episode_reward:.2f}")
            print(f"   Moving Avg: {moving_avg:.2f}")
            print(f"   Noise: {noise_scale:.3f}")
            print(f"   Buffer: {len(replay_buffer)}")
            print(f"   Time: {elapsed:.1f}s")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
        if moving_avg > best_reward + CONFIG['min_improvement']:
            best_reward = moving_avg
            patience_counter = 0
            
            if (episode + 1) % CONFIG['log_interval'] == 0:
                print(f"   ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ (reward: {best_reward:.2f})")
            
            for i, agent in enumerate(agents):
                if hasattr(agent, 'actor'):
                    torch.save(agent.actor.state_dict(), 
                             model_dir / f'best_actor_agent{i}.pt')
                if hasattr(agent, 'critic'):
                    torch.save(agent.critic.state_dict(), 
                             model_dir / f'best_critic_agent{i}.pt')
        else:
            patience_counter += 1
        
        # Ø°Ø®ÛŒØ±Ù‡ checkpoint
        if (episode + 1) % CONFIG['save_interval'] == 0:
            print(f"   ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ checkpoint")
            for i, agent in enumerate(agents):
                if hasattr(agent, 'actor'):
                    torch.save(agent.actor.state_dict(), 
                             model_dir / f'checkpoint_actor_agent{i}_ep{episode+1}.pt')
        
        # Early stopping
        if patience_counter >= CONFIG['patience']:
            print(f"\nâš ï¸ Early stopping (no improvement for {CONFIG['patience']} episodes)")
            break
    
    # =============================================================================
    # Ù¾Ø§ÛŒØ§Ù† Ø¢Ù…ÙˆØ²Ø´
    # =============================================================================
    total_time = time.time() - start_time
    print("\n" + "="*70)
    print("âœ… Ø¢Ù…ÙˆØ²Ø´ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")
    print(f"   Ø²Ù…Ø§Ù† Ú©Ù„: {total_time:.1f}s ({total_time/60:.1f} Ø¯Ù‚ÛŒÙ‚Ù‡)")
    print(f"   Ø¨Ù‡ØªØ±ÛŒÙ† reward: {best_reward:.2f}")
    print("="*70)
    
    # Ø°Ø®ÛŒØ±Ù‡ metrics
    np.savez(results_dir / 'training_metrics.npz',
             episode_rewards=episode_rewards,
             moving_avg=moving_avg_rewards)
    
    # Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(episode_rewards, alpha=0.3, label='Episode Reward')
    plt.plot(moving_avg_rewards, label='Moving Average (50)', linewidth=2)
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title('Training Progress')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(moving_avg_rewards, linewidth=2, color='green')
    plt.xlabel('Episode')
    plt.ylabel('Moving Average Reward')
    plt.title('Smoothed Performance')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(results_dir / 'training_curves.png', dpi=150, bbox_inches='tight')
    print(f"\nğŸ“Š Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {results_dir / 'training_curves.png'}")
    
    return agents, episode_rewards, moving_avg_rewards


# =============================================================================
# Ø§Ø¬Ø±Ø§
# =============================================================================
if __name__ == "__main__":
    try:
        agents, rewards, moving_avg = train()
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ: {e}")
        import traceback
        traceback.print_exc()
