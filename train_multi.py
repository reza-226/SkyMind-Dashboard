"""
train_multi.py (Compatible Version)
=====================================
Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ train_maddpg_complete.py Ùˆ Dashboard
"""

import argparse
import numpy as np
import torch
import pandas as pd
from collections import deque
from core.env_multi import MultiUAVEnv
from agents.agent_maddpg_multi import MADDPG_Agent
from tqdm import tqdm
import os


def train_maddpg(n_episodes=2000, resume=False, checkpoint_path='models/'):
    """Ø¢Ù…ÙˆØ²Ø´ MADDPG Ø¨Ø§ Ø­ÙØ¸ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ"""
    
    print("[SkyMind-TPSG] Training Multi-Agent DRL Simulation started...\n")
    
    # Ù…Ø­ÛŒØ·
    n_agents = 3
    n_users = 5
    env = MultiUAVEnv(n_agents=n_agents, n_users=n_users)
    
    state_dim = 6
    action_dim = 4
    
    # Agents
    agents = []
    for i in range(n_agents):
        agent = MADDPG_Agent(
            state_dim=state_dim,
            action_dim=action_dim,
            n_agents=n_agents,
            agent_id=i,
            lr=1e-4
        )
        agents.append(agent)
    
    # Replay Buffer
    replay_buffer = deque(maxlen=100000)
    print("[SkyMind-TPSG] âœ“ Replay Buffer created")
    
    # Resume logic
    start_episode = 0
    if resume:
        print(f"\n[SkyMind-TPSG] ğŸ”„ Resume mode activated")
        
        # âœ… Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ train_maddpg_complete.py
        legacy_checkpoint = os.path.join(checkpoint_path, 'maddpg_sky_env_1.pth')
        
        if os.path.exists(legacy_checkpoint):
            print("  ğŸ“‚ Legacy checkpoint detected, loading...")
            checkpoint = torch.load(legacy_checkpoint)
            
            for i, agent in enumerate(agents):
                if f'actor_agent{i}' in checkpoint:
                    agent.actor.load_state_dict(checkpoint[f'actor_agent{i}'])
                    agent.critic.load_state_dict(checkpoint[f'critic_agent{i}'])
                    print(f"  âœ… Agent {i} loaded from legacy checkpoint")
        else:
            # ÙØ±Ù…Øª Ø¬Ø¯ÛŒØ¯
            for i, agent in enumerate(agents):
                actor_path = os.path.join(checkpoint_path, f'actor_agent{i}.pt')
                critic_path = os.path.join(checkpoint_path, f'critic_agent{i}.pt')
                
                if os.path.exists(actor_path):
                    agent.actor.load_state_dict(torch.load(actor_path))
                    agent.critic.load_state_dict(torch.load(critic_path))
                    print(f"  âœ… Agent {i} loaded")
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ù…Ø§Ø±Ù‡ episode
        episode_file = os.path.join(checkpoint_path, 'episode.txt')
        if os.path.exists(episode_file):
            with open(episode_file, 'r') as f:
                start_episode = int(f.read().strip())
    
    # Hyperparameters
    batch_size = 128
    epsilon = 0.3 if not resume else 0.1
    epsilon_decay = 0.999
    epsilon_min = 0.05
    
    # Metrics tracking
    rewards_history = []
    critic_losses_history = []
    actor_losses_history = []
    energy_history = []  # âœ… Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Dashboard
    delay_history = []   # âœ… Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Dashboard
    
    # âœ… CSV log file (Ù…Ø·Ø§Ø¨Ù‚ train_maddpg_complete.py)
    csv_path = 'data/episodes.csv'
    os.makedirs('data', exist_ok=True)
    
    if not os.path.exists(csv_path) or not resume:
        with open(csv_path, 'w') as f:
            f.write("episode,reward,energy,delay,critic_loss,actor_loss\n")
    
    # Ø­Ù„Ù‚Ù‡ Ø¢Ù…ÙˆØ²Ø´
    for episode in tqdm(range(start_episode, start_episode + n_episodes), 
                        desc="Training"):
        
        state = env.reset()
        episode_reward = 0
        episode_critic_loss = []
        episode_actor_loss = []
        episode_energy = 0
        episode_delay = 0
        
        for step in range(200):
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø§Ú©Ø´Ù†
            actions = []
            for i, agent in enumerate(agents):
                if isinstance(state, dict):
                    agent_state = extract_agent_state(state, i)
                else:
                    agent_state = state[i] if state.ndim > 1 else state
                
                if np.random.rand() < epsilon:
                    action = np.random.uniform(-1, 1, action_dim)
                else:
                    action = agent.select_action(agent_state)
                
                actions.append(action)
            
            actions = np.array(actions)
            
            # Ø§Ø¬Ø±Ø§ÛŒ Ø§Ú©Ø´Ù†
            next_state, reward, done, info = env.step(actions)
            
            if isinstance(reward, np.ndarray):
                reward = reward.sum()
            
            episode_reward += reward
            
            # âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ energy Ùˆ delay (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
            if 'energy_total' in info:
                episode_energy += info['energy_total']
            if 'mean_delay' in info:
                episode_delay = info['mean_delay']
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± buffer
            replay_buffer.append({
                'state': state,
                'actions': actions,
                'reward': reward,
                'next_state': next_state,
                'done': done
            })
            
            # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ
            if len(replay_buffer) >= batch_size:
                for agent in agents:
                    critic_loss, actor_loss = agent.update(
                        replay_buffer, 
                        agents, 
                        batch_size=batch_size
                    )
                    
                    if critic_loss is not None:
                        episode_critic_loss.append(critic_loss)
                    if actor_loss is not None:
                        episode_actor_loss.append(actor_loss)
            
            state = next_state
            
            if done:
                break
        
        epsilon = max(epsilon_min, epsilon * epsilon_decay)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
        rewards_history.append(episode_reward)
        energy_history.append(episode_energy)
        delay_history.append(episode_delay)
        
        avg_critic_loss = np.mean(episode_critic_loss) if episode_critic_loss else 0
        avg_actor_loss = np.mean(episode_actor_loss) if episode_actor_loss else 0
        
        critic_losses_history.append(avg_critic_loss)
        actor_losses_history.append(avg_actor_loss)
        
        # âœ… Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± CSV (Ù…Ø·Ø§Ø¨Ù‚ ÙØ±Ù…Øª Ù‚Ø¨Ù„ÛŒ)
        with open(csv_path, 'a') as f:
            f.write(f"{episode + 1},{episode_reward:.4f},{episode_energy:.4f},"
                    f"{episode_delay:.4f},{avg_critic_loss:.6f},{avg_actor_loss:.6f}\n")
        
        # Checkpoint
        if (episode + 1) % 100 == 0:
            os.makedirs(checkpoint_path, exist_ok=True)
            
            # âœ… Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯Ùˆ ÙØ±Ù…Øª (Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ)
            # ÙØ±Ù…Øª 1: ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ (Ø¬Ø¯ÛŒØ¯)
            for i, agent in enumerate(agents):
                torch.save(
                    agent.actor.state_dict(),
                    os.path.join(checkpoint_path, f'actor_agent{i}.pt')
                )
                torch.save(
                    agent.critic.state_dict(),
                    os.path.join(checkpoint_path, f'critic_agent{i}.pt')
                )
            
            # ÙØ±Ù…Øª 2: ÙØ§ÛŒÙ„ ÙˆØ§Ø­Ø¯ (Ù‚Ø¯ÛŒÙ…ÛŒ - Ø¨Ø±Ø§ÛŒ Dashboard)
            legacy_dict = {}
            for i, agent in enumerate(agents):
                legacy_dict[f'actor_agent{i}'] = agent.actor.state_dict()
                legacy_dict[f'critic_agent{i}'] = agent.critic.state_dict()
            
            torch.save(legacy_dict, 
                      os.path.join(checkpoint_path, 'maddpg_sky_env_1.pth'))
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø´Ù…Ø§Ø±Ù‡ episode
            with open(os.path.join(checkpoint_path, 'episode.txt'), 'w') as f:
                f.write(str(episode + 1))
            
            print(f"\nğŸ’¾ Checkpoint saved (Episode {episode + 1})")
            print(f"   Reward: {episode_reward:.4f}")
            print(f"   Energy: {episode_energy:.4f}")
            print(f"   Delay: {episode_delay:.4f}")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
    print("\n[SkyMind-TPSG] âœ… Training completed")
    
    # âœ… NPZ Ø¨Ø§ ÙØ±Ù…Øª Ú©Ø§Ù…Ù„ (Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ Dashboard)
    np.savez(
        'results/training_metrics.npz',
        rewards=rewards_history,
        critic_losses=critic_losses_history,  # âœ… Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚
        actor_losses=actor_losses_history,
        energy=energy_history,  # âœ… Ø¨Ø±Ø§ÛŒ Dashboard
        delay=delay_history      # âœ… Ø¨Ø±Ø§ÛŒ Dashboard
    )
    print("ğŸ’¾ Metrics saved: results/training_metrics.npz")
    
    return agents, rewards_history


def extract_agent_state(state_dict, agent_idx, state_dim=6):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ state Ø¹Ø§Ù…Ù„"""
    if isinstance(state_dict, dict):
        agent_state = np.concatenate([
            state_dict['uav_positions'][agent_idx],
            [state_dict['uav_velocities'][agent_idx]],
            [state_dict['uav_angles'][agent_idx]],
            [state_dict['energy'][agent_idx]],
            [state_dict['distances'][agent_idx]]
        ])
        return agent_state
    return state_dict


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--episodes', type=int, default=2000)
    parser.add_argument('--resume', action='store_true')
    parser.add_argument('--checkpoint', type=str, default='models/')
    
    args = parser.parse_args()
    
    train_maddpg(
        n_episodes=args.episodes,
        resume=args.resume,
        checkpoint_path=args.checkpoint
    )
