"""
Metrics Collector for GE-CL-MADDPG Training
Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
"""

import numpy as np
from typing import Dict, List, Optional
from collections import defaultdict, deque


class MetricsCollector:
    """
    Ø³ÛŒØ³ØªÙ… Ø¬Ø§Ù…Ø¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
    """
    
    def __init__(self, window_size: int = 100):
        """
        Args:
            window_size: Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù¾Ù†Ø¬Ø±Ù‡ Ø¨Ø±Ø§ÛŒ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú©
        """
        self.window_size = window_size
        
        # Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
        self.episode_rewards = []
        self.episode_lengths = []
        self.actor_losses = []
        self.critic_losses = []
        
        # Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ UAV-MEC
        self.energy_consumption = []
        self.task_latencies = []
        self.success_rates = []
        self.collision_counts = []
        self.gini_indices = []
        self.collaboration_scores = []
        
        # Ù¾Ù†Ø¬Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ­Ø±Ú©
        self.recent_rewards = deque(maxlen=window_size)
        self.recent_success = deque(maxlen=window_size)
        
        # Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ per-agent
        self.agent_metrics = defaultdict(lambda: {
            'rewards': [],
            'actions': [],
            'losses': []
        })
        
    def record_episode(self, 
                      episode: int,
                      total_reward: float,
                      episode_length: int,
                      actor_loss: float = 0.0,
                      critic_loss: float = 0.0,
                      **kwargs):
        """
        Ø«Ø¨Øª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ø§Ù¾ÛŒØ²ÙˆØ¯
        
        Args:
            episode: Ø´Ù…Ø§Ø±Ù‡ Ø§Ù¾ÛŒØ²ÙˆØ¯
            total_reward: Ù…Ø¬Ù…ÙˆØ¹ reward
            episode_length: Ø·ÙˆÙ„ Ø§Ù¾ÛŒØ²ÙˆØ¯
            actor_loss: loss Ø´Ø¨Ú©Ù‡ actor
            critic_loss: loss Ø´Ø¨Ú©Ù‡ critic
            **kwargs: Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
        """
        # Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
        self.episode_rewards.append(total_reward)
        self.episode_lengths.append(episode_length)
        self.actor_losses.append(actor_loss)
        self.critic_losses.append(critic_loss)
        
        # Ù¾Ù†Ø¬Ø±Ù‡ Ù…ØªØ­Ø±Ú©
        self.recent_rewards.append(total_reward)
        
        # Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ
        if 'energy' in kwargs:
            self.energy_consumption.append(kwargs['energy'])
        if 'latency' in kwargs:
            self.task_latencies.append(kwargs['latency'])
        if 'success_rate' in kwargs:
            self.success_rates.append(kwargs['success_rate'])
            self.recent_success.append(kwargs['success_rate'])
        if 'collisions' in kwargs:
            self.collision_counts.append(kwargs['collisions'])
        if 'gini_index' in kwargs:
            self.gini_indices.append(kwargs['gini_index'])
        if 'collaboration_score' in kwargs:
            self.collaboration_scores.append(kwargs['collaboration_score'])
    
    def record_agent_step(self, agent_id: int, reward: float, 
                         action: np.ndarray, loss: float = 0.0):
        """
        Ø«Ø¨Øª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ú¯Ø§Ù… Ø¨Ø±Ø§ÛŒ ÛŒÚ© agent
        
        Args:
            agent_id: Ø´Ù†Ø§Ø³Ù‡ agent
            reward: reward Ø¯Ø±ÛŒØ§ÙØªÛŒ
            action: Ø¹Ù…Ù„ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡
            loss: loss Ø´Ø¨Ú©Ù‡
        """
        self.agent_metrics[agent_id]['rewards'].append(reward)
        self.agent_metrics[agent_id]['actions'].append(action)
        self.agent_metrics[agent_id]['losses'].append(loss)
    
    def get_recent_avg_reward(self, window: Optional[int] = None) -> float:
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† reward Ø§Ø®ÛŒØ±
        
        Args:
            window: Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù¾Ù†Ø¬Ø±Ù‡ (Ø§Ú¯Ø± None Ø¨Ø§Ø´Ø¯ Ø§Ø² window_size Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
        
        Returns:
            Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† reward
        """
        if not self.episode_rewards:
            return 0.0
        
        if window is None:
            return np.mean(list(self.recent_rewards))
        
        window = min(window, len(self.episode_rewards))
        return np.mean(self.episode_rewards[-window:])
    
    def get_recent_success_rate(self, window: Optional[int] = None) -> float:
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø®ÛŒØ±
        
        Args:
            window: Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù¾Ù†Ø¬Ø±Ù‡
        
        Returns:
            Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª
        """
        if not self.success_rates:
            return 0.0
        
        if window is None:
            return np.mean(list(self.recent_success))
        
        window = min(window, len(self.success_rates))
        return np.mean(self.success_rates[-window:])
    
    def get_summary(self) -> Dict:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ
        
        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø´Ø§Ù…Ù„ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒØ¯ÛŒ
        """
        if not self.episode_rewards:
            return {}
        
        summary = {
            'total_episodes': len(self.episode_rewards),
            'avg_reward': np.mean(self.episode_rewards),
            'std_reward': np.std(self.episode_rewards),
            'best_reward': np.max(self.episode_rewards),
            'worst_reward': np.min(self.episode_rewards),
            'recent_avg_reward': self.get_recent_avg_reward(),
            'avg_episode_length': np.mean(self.episode_lengths),
        }
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ
        if self.energy_consumption:
            summary['avg_energy'] = np.mean(self.energy_consumption)
            summary['total_energy'] = np.sum(self.energy_consumption)
        
        if self.task_latencies:
            summary['avg_latency'] = np.mean(self.task_latencies)
            summary['min_latency'] = np.min(self.task_latencies)
        
        if self.success_rates:
            summary['avg_success_rate'] = np.mean(self.success_rates)
            summary['recent_success_rate'] = self.get_recent_success_rate()
        
        if self.collision_counts:
            summary['total_collisions'] = np.sum(self.collision_counts)
            summary['avg_collisions'] = np.mean(self.collision_counts)
        
        if self.gini_indices:
            summary['avg_gini_index'] = np.mean(self.gini_indices)
        
        if self.collaboration_scores:
            summary['avg_collaboration'] = np.mean(self.collaboration_scores)
        
        return summary
    
    def get_agent_summary(self, agent_id: int) -> Dict:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ÛŒÚ© agent Ø®Ø§Øµ
        
        Args:
            agent_id: Ø´Ù†Ø§Ø³Ù‡ agent
        
        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¢Ù…Ø§Ø± agent
        """
        if agent_id not in self.agent_metrics:
            return {}
        
        metrics = self.agent_metrics[agent_id]
        
        return {
            'total_steps': len(metrics['rewards']),
            'avg_reward': np.mean(metrics['rewards']) if metrics['rewards'] else 0.0,
            'avg_loss': np.mean(metrics['losses']) if metrics['losses'] else 0.0,
            'total_reward': np.sum(metrics['rewards']) if metrics['rewards'] else 0.0,
        }
    
    def print_summary(self, episode: Optional[int] = None):
        """
        Ú†Ø§Ù¾ Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ
        
        Args:
            episode: Ø´Ù…Ø§Ø±Ù‡ Ø§Ù¾ÛŒØ²ÙˆØ¯ ÙØ¹Ù„ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        """
        summary = self.get_summary()
        
        if not summary:
            print("âš ï¸ No metrics recorded yet")
            return
        
        print("\n" + "="*60)
        if episode is not None:
            print(f"ğŸ“Š METRICS SUMMARY (Episode {episode})")
        else:
            print("ğŸ“Š METRICS SUMMARY")
        print("="*60)
        
        print(f"Total Episodes: {summary['total_episodes']}")
        print(f"Average Reward: {summary['avg_reward']:.4f} Â± {summary['std_reward']:.4f}")
        print(f"Recent Avg (100): {summary['recent_avg_reward']:.4f}")
        print(f"Best Reward: {summary['best_reward']:.4f}")
        print(f"Avg Episode Length: {summary['avg_episode_length']:.2f}")
        
        if 'avg_success_rate' in summary:
            print(f"Success Rate: {summary['avg_success_rate']*100:.2f}%")
        
        if 'avg_energy' in summary:
            print(f"Avg Energy: {summary['avg_energy']:.2f} J")
        
        if 'avg_latency' in summary:
            print(f"Avg Latency: {summary['avg_latency']:.2f} ms")
        
        if 'total_collisions' in summary:
            print(f"Total Collisions: {summary['total_collisions']}")
        
        print("="*60 + "\n")
    
    def reset(self):
        """Ø±ÛŒØ³Øª Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§"""
        self.__init__(window_size=self.window_size)


# ========================================
# Environment Metrics Collector
# ========================================

class EnvironmentMetricsCollector:
    """
    Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù…Ø­ÛŒØ· Ø¯Ø± Ø·ÙˆÙ„ Ø§Ø¬Ø±Ø§
    """
    
    def __init__(self):
        self.step_metrics = []
        self.episode_start_time = None
        
    def record_step(self, step: int, **metrics):
        """Ø«Ø¨Øª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ú¯Ø§Ù…"""
        metrics['step'] = step
        self.step_metrics.append(metrics)
    
    def get_episode_metrics(self) -> Dict:
        """Ø¯Ø±ÛŒØ§ÙØª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ú©Ù„ Ø§Ù¾ÛŒØ²ÙˆØ¯"""
        if not self.step_metrics:
            return {}
        
        # ØªØ¬Ù…ÛŒØ¹ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
        aggregated = {
            'total_steps': len(self.step_metrics),
        }
        
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÚ¯ÛŒØ±ÛŒ
        keys = set().union(*[m.keys() for m in self.step_metrics])
        keys.discard('step')
        
        for key in keys:
            values = [m.get(key, 0) for m in self.step_metrics]
            aggregated[f'avg_{key}'] = np.mean(values)
            aggregated[f'sum_{key}'] = np.sum(values)
            aggregated[f'max_{key}'] = np.max(values)
        
        return aggregated
    
    def reset(self):
        """Ø±ÛŒØ³Øª Ø¨Ø±Ø§ÛŒ Ø§Ù¾ÛŒØ²ÙˆØ¯ Ø¬Ø¯ÛŒØ¯"""
        self.step_metrics = []
